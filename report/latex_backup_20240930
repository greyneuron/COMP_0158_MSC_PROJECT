\documentclass{report}
\usepackage{setspace}
%\usepackage{subfigure}

\pagestyle{plain}
\usepackage{amssymb,graphicx,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{float}

\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}

\newtheorem{theorem}{THEOREM}
\newtheorem{lemma}[theorem]{LEMMA}
\newtheorem{corollary}[theorem]{COROLLARY}
\newtheorem{proposition}[theorem]{PROPOSITION}
\newtheorem{remark}[theorem]{REMARK}
\newtheorem{definition}[theorem]{DEFINITION}
\newtheorem{fact}[theorem]{FACT}

\newtheorem{problem}[theorem]{PROBLEM}
\newtheorem{exercise}[theorem]{EXERCISE}
\def \set#1{\{#1\} }

\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}



\newcommand{\nats}{\mbox{\( \mathbb N \)}}
\newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}


% ---------------- my additions ----------------
% xml format
\newcommand*{\xml}[1]{\texttt{<#1>}}
\usepackage{tcolorbox}
\usepackage{hyperref}



%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{  	{ \includegraphics[scale=.5]{images/ucl_logo.png}}\\
\vspace{5mm}
{{\Huge Can useful biological information be found within Word2Vec embeddings of protein sequences and protein domains?}}\\
{\large Optional Subtitle}\\
		}
\date{Submission date: Day Month Year}
\author{Patrick Lowry\thanks{
{\bf Disclaimer:}
This report is submitted as part requirement for the MSc in  Data Science and Machine Learning at UCL. It is
substantially the result of my own work except where explicitly indicated in the text.
The report may be freely copied and distributed provided the source is explicitly acknowledged
\newline  %% \\ screws it up
\emph{Or:}\newline
The report will be distributed to the internal and external examiners, but thereafter may not be copied or distributed except with permission from the author.}
\\ \\
MSc Data Science and Machine Learning\\ \\
Daniel Buchan}



\begin{document}
 
 \onehalfspacing
\maketitle
\begin{abstract}
The purpose of this dissertation is to investigate whether, by representing eukaryotic proteins as sentences and encoding them with the word2vec language model - do the resulting word embeddings create clusters in space that reflect biologically meaningful relationships? \\

Language models have become household names in recent years. They rely upon turning vast quantities of textual data, called a corpus,  into a numeric encoding - or embedding. Embeddings are multi-dimensional vectors - one embedding for each word in the corpus. In this way 'distances' between words can be calculated within the embedding vector space and used, for example, in word prediction by selecting the word 'closest' in the embedding space to the current word that has been typed.
\paragraph{}
Individual proteins are made up of long, varying-length sequences of amino-acids. Over many years, short regions within these sequences have been identified as having a common ancestry in evolution - these regions are called protein families (\textbf{pfams}). Other regions, called \textbf{disordered regions}, don't adopt a well-defined 3D structure. In between, there are sections of protein whose function is unknown or non-existent.
\paragraph{}
Thus, a protein can be viewed as consisting of a number 'Pfam' words or tokens, a number of 'disordered' regions and 'gap' areas in between. Looking at it another way, each protein can be represented as a 'sentence' where the words within the sentence are Pfam identifiers, Disordered regions and with Gap words in between.
\paragraph{}
By creating a corpus from a dataset consisting of 78M eukaryotic proteins, 300M protein families and 4BN lines of disordered region information, this dissertation iterated through a large number of word2vec models - each producing its word embeddings in a multi-dimensional vector space. The vector embeddings of the Pfam words were then clustered via a KMeans algorithm to see if the resulting 'K' clusters mirrored biologically meaningful relationships. Pfam clans were used as the target relationships to test whether the K clusters aligned.
\paragraph{}
Unfortunately, despite exhaustive searching through different model configurations and corpus constructions, no correlation could be found between the clusters of word embeddings and the Pfam clans. This suggests that the word2vec algorithm can not provide meaningful biological insight. It is possible that a more customised encoding algorithm would provide more meaningful correlations.
\end{abstract}


% -------------------------- Table of Contents --------------------------------------
\tableofcontents


\setcounter{page}{1}

%\chapter{UCL Samples}
% This is just a bare minimum to get started.  There is unlimited guidance on using latex, e.g. {\tt https://en.wikibooks.org/wiki/LaTeX}.   You are still responsible to check the detailed requirements of a project, including formatting instructions, see
%{\tt https://moodle.ucl.ac.uk/pluginfile.php/3591429/mod\_resource/content/7/UGProjects2017.pdf}.
%Leave at least a line of white space when you want to start a new paragraph.

%Mathematical expressions are placed inline between dollar signs, e.g. $\sqrt 2, %\sum_{i=0}^nf(i)$, or in display mode
%\[ e^{i\pi}=-1\] and another way, this time with labels,
%\begin{align}
%\label{line1} A=B\wedge B=C&\rightarrow A=C\\
%&\rightarrow C=A\\
%\intertext{note that}
%n!&=\prod_{1\leq i\leq n}i \\
%\int_{x=1}^y \frac 1 x \mathrm{d}x&=\log y
%\end{align}
% We can refer to labels like this \eqref{line1}. Often lots of citations here (and elsewhere), e.g. \cite{Rey:D} or \cite[Theorem 2.3]{PriorNOP70}.   Bibtex can help with this, but is not essential. If you want pictures, try

%\begin{center}
%\includegraphics[scale=.5]{images/aristotle.jpg}
%\end{center}
%You can use 
%\begin{itemize}
%\item lists
%\item like this
%\end{itemize}
%or numbered
%\begin{enumerate}
%\item like this,
%\item or this
%\end{enumerate}
%but don't overdo it. \\
%If you have a formal theorem you might try this.
%\begin{definition}\label{def}
%See definition~\ref{def}.
%\end{definition}
%\begin{theorem}
%For all $n\in\nats,\; 1^n=1$.
%\end{theorem}
%\begin{proof}
%By induction over $n$.
%\end{proof}



% ----------------------------------------------------------------
%             My Document
% ----------------------------------------------------------------


% ----------------------------------------------------------------
%             Chapter 1 - Introduction and Background
% ----------------------------------------------------------------
\chapter{Introduction and Background}
\section{Audience}
The purpose of this section is to provide the background and motivation for this dissertation. Although it is assumed that the reader has a background in Computer Science, knowledge of Biology, Bioinformatics or Language Modelling cannot be assumed. The first part of this chapter, therefore, aims to introduce these topics to so that the reader can understand the problem domain and the intentions of this dissertation.

\section{An overview of proteins}

Proteins are crucial to almost every biological process. For example, amylase is an enzyme (a type of protein) which breaks down carbohydrates into sugars which can then be used for energy. Haemoglobin is a protein whose shape and structure allows it to bind to oxygen in the lungs and carry it through the bloodstream to other organs, insulin regulates blood sugar levels, antibodies are proteins that help identify and neutralise pathogens; signalling proteins such as hormones are involved in cell communication. They are pervasive and essential for life.

% sanger p37
\subsubsection{Protein creation and DNA}
In eukaryotic organisms (those organisms, including humans whose cells have a nucleus), DNA contains the 'instructions' for creating proteins \todo{check wording}. The famous 'double helix' of DNA is made up of two chains of complementary nucleotides (from a set of 4 - GATC). Through the processes of Transcription and Translation, the DNA double helix is unwound in the nucleus and an effective copy of its nucleotide sequence is made in the form of RNA. Outside of the nucleus, a cell's ribosomes use this RNA blueprint to create the chains of amino acids (i.e. proteins). The amino acids are linked together in the ribosome by a peptide bond, forming a long 'polypeptide chain' This is achieved by matching sequences of 3 RNA bases (or codons) to one of only 20 amino acids.

\paragraph{} Thus the genetic code embedded in DNA is directly linked to the amino acid sequence that make up a protein. \\


%The direct connection between DNA and the sequence structure of a protein molecule %means that a protein's sequence can be maintained through evolutionary time and %across species. Two molecules are described as homologous if they have been derived %from a common ancestor. Thus, by identifying similarity of sequences across species %it is possible to advance our understanding of protein function.



% stryer
\subsection{Composition and structure}
Although proteins are composed from a set of only 20 amino acids there is huge variation in shape and function as a result of the complex makeup of amino acids. Amino acids are non-trivial structures (each has a central carbon atom, a carboxylic acod group, a hydrogen atom and a  \textbf{side chain}). The side chains vary in size, shape and chemical properties across the set of 20 acids, and it is this range of structural and chemical variation that underpins the huge diversity of function across proteins.\\

Structurally, a protein can be described in four ways:
\begin{itemize}
    \item The primary structure is the amino acid sequence itself
    \item The secondary structure refers to local folding patterns within a protein
    \item The tertiary structure is the overall 3D shape of a protein and results from interactions between the secondary and primary structure.
    \item The quartenary structure refers to the association of mutltiple polypeptide chains \todo{check}
\end{itemize}

It is the 3D structure and shape that largely determines a protein's function. This in turn is the result of a process called \textbf{folding} - this refers to the manner by which the long polypeptide chains 'fold' over each other. Folding is driven by a number of factors but ultimately depends upon the properties of the amino acids in the chain, their position in the chain and the environment they find themselves in. For example, positively charged amino acids will attract negatively charged amino acids, hydrophobic amino acids will naturally move to the inside of the chain in a solution, other amino acids are sensitive to pH etc. These forces cause the chain to \textbf{fold} over itself, thus changing its 3D structure.

\paragraph{}The folded structure results in binding sites on the outsides of the protein with very specific shapes allowing proteins to interact only with certain molecules - like a lock that only accepts a certain shape of key.

%With only a few exceptions, all proteins in all species (bacterial, archaeal and eukaryotic) are constructed from these same sets of 20 amino acids. \\ \\
%This basic structure has been in place for billions of years; this link to genetics and evolution is crucial in helping us to identify protein function.

% styer p 48
\subsubsection{Protein domains and Disordered regions}
Certain combinations of secondary structure are present in many different proteins and exhibit similar functions. They are conserved across different species - indicating that they serve an important biological function. These sections, called \textbf{domains} \cite{introprotdomain1} \cite{introprotdomain2}, range from 30 to 400 amino acids in length and fold independently into two or more compact regions.\\

Domains are often connected by more flexible segments whose structure fluctuates under different conditions, however these sections also contribute to a protein's function \cite{introdisordered}. Their lack of structure allow easy access for enzymes for example. They will often adopt a more defined structure only when they bind to other proteins or molecules. They can be considered as providing more dynamism to a protein as they are more flexible.\\

It is widely accepted \cite{introprotdomain3} \cite{introprotdomain4} that the overall function of a protein is determined by the combination of domains along its length. If its possible to find a way of embedding these domains and finding semantic relationships between them that are biologically meangingful \todo{then what?}


\subsubsection{Grouping proteins into families and clans}
Within a protein, there are sets of regions that share a significant degree of sequence similarity, \cite{pfam} suggesting a common ancestor in evolution (homology). Such proteins are grouped into what is called a \textbf{protein family} \cite{pfam2}. In 1995 Sonnahammer et al \cite{pfam0} created Pfam - a centrally managed collection of commonly occurring protein domains that could be used to annotate the protein coding genes of multicellular animals.

\paragraph{} The process of identifying protein families is achieved through a process called Multiple Sequence Alignment (MSA) \cite{pfammsa} whereby sequences are aligned to identify the conserved regions across protein samples. Hidden Markov models \cite{pfamhmm} are then used to identify and group proteins into \textbf{Pfam} families (Pfam domains).

\paragraph{}As research progressed, more and more protein families were identified and further relationships between the protein families themselves were identified. Thus in 2006, the pfam team introduced the idea of a pfam clan \cite{pfamclan}. A clan contains two or more Pfam families that have arisen from a single evolutionary origin. Relatedness is determined by structure, function, significant sequence overalap and profile to profile comparisons.

The latest version of the pfam database is Pfam, 37.0, released in June 2024. It contains 21,979 families and 709 clans.

% sanger p37
\subsubsection{Why understand amino acid sequences?}
Analysing amino acid sequences is important as it allows us to a) understand the function a protein provides b) understand or infer the 3D structure of a protein and c) identify anomalies or malformations in a sequence that can result in disease d) assist with the study of evolution - proteins resemble one another in sequence only if they have a common ancestor


\subsection{Protein databases}
% https://theconversation.com/what-is-a-protein-a-biologist-explains-152870
% https://gfieurope.org/blog/2023-was-a-record-breaking-year-for-uk-alternative-protein-research-funding-heres-a-recap/
Protein research is an area of huge interest and investment \todo{why}. In 2023, UK public investment in protein research topped £15.6 million and globally the alternative protein sector attracted \$2.9billion in private investment in 2022.\\

The number of unique proteins in a human body is estimated to be at least around 20,000 and Uniprot holds \todo{get latest}. With so much research and so much data to process, it is essential that shared repositories are available so that the research community can benefit from the latest research and findings. This is where protein databases come in. Protein databases act as a central store for protein information ranging from their amino acid sequences, structure, interactions and functions. There are many different databases \todo{insert refs}, but this study has leaned heavily on two of them - Uniprot and Interpro.

\paragraph{Uniprot} or 'Universal Protein Resource' is a comprehensive resource of accurate protein sequence and functional information. It stores protein sequence data, functional information, protein names, taxonomies as well as cross references to other databases. It has two main parts - UniProtKB/Swiss-Prot entries have been manually reviewed and annotated, whereas UniProtKM/TrEMBL entries have been automatically annotated but not reviewed \todo{presumably more in Trembl? What is Uniref?}. Each protein sequence in the Uniprot database as a unique identifieer, called an \textbf{Accession code} - this is critical to allow researchers to combine data under a unique identifier.

% https://www.ebi.ac.uk/training/online/courses/interpro-quick-tour/interpro-data/
\paragraph{InterPro}integrates protein signatures from 13 member databases, each of which uses a different method to classify proteins. Interpro 'curators' manually merge signatures that represent the same protein family, domain or site into single InterPro entries and if possible, trace biological relationships between them. InterPro makes its data available though an Interpro API or \href{https://www.ebi.ac.uk/interpro/download/}{FTP} download.


\subsection{The role of technology in understanding proteins}


\section{Language Models and Word Embeddings}
Language Models try to predict the next word in a sentence based upon the current word or words. This is achieved by creating some measure relating words to each other. The predictive text feature we are all familiar with does exactly that - it proposes the next word in a sentence based based upon a measurement such as a probability that the next word follows the current word. These measurements are determined following training on a corpus of text containing all the words in the vocabulary.

\paragraph{Word Embedding.} Any Machine Learning model requires a numerical representation of its input. Image recognition models, for example take as input what we all see as an image but which a model sees as a large vector of RGB values - one per pixel. This representation allows the model to perform its mathematical magic - identifying patterns in the pixels that are imperceptible to the human eye and determine what the image is or, even better, to help spot a formative tumour in a medical scan.

Similarly for language models - words in a body of text need to be transformed into numbers so that they can be analysed programatically and used in word prediction or machine translation. In fact, language models represent the words in their vocabulary as multi-dimensional vectors of numbers. They do this by analysing a large body of input text (called a \textbf{corpus}) and creating vector representations of each word based upon the relationships observed between words in that corpus. This representation is called an '\textbf{embedding}'. As Machine Learning has evolved, the methods of embedding have become more sophisticated and more successful.

\paragraph{A simple embedding based upon counting.}
Imagine a body of text (ie \textbf{corpus}) that has 1,000 sentences and a vocabulary of only 100 unique words. A very simple \textbf{embedding} might be created by reading all the sentences and counting the number of times each unique word appears alongside each other word. This results in a 100 x 100 matrix of counts. To predict the next word in a sentence, one would simply lookup the row in the matrix corresponding to the current word, find the highest count in that row and propose the next word to be that in the corresponding column. This is of course not very useful, but it does explain the concept.

\subsection{N-grams}
One of the earliest embedding approaches are N-gram models. These can be traced back to Markov's study in 1913 where he tried to predict whether an upcoming letter would be a vowel or a consonant. Shannon (1948) also used this technique to compute approximations to English word sequences. In 1975 Jelinek and colleagues at IBM used n-grams in speech recognition systems and from that seemed to trigger a resurgence of interest.\\

The N-gram set of models predict the probability of a word based upon the words that have come before it, how far back they look before the current word $w_{n-1}$ is based upon the parameter $n$. They make a Markov assumption that the probability of the next word, $w$, depending upon all words up to that point (from $1$ to $n-1$) is roughly equivalent to the probability depending only upon the $n-1$ preceding words. 

\begin{center}
$  P(w|w_{1 : n-1}) \simeq P(w|w_{n-1})$ 
\end{center}

A \textbf{bigram}, for example sets n=2 and considers only 1 word behind the current word, a trigram will look 2 words behind (3-1) etc.\\

Although this approach is easy to understand, it is based upon counting the frequency of co-occurring words in a corpus in order to compute a probability; it just uses the parameter $n$ to determine how many words to look at together when counting. As such, the N-gram family of models is unable to accurately capture word context in its embeddings. This is where \textbf{word2vec} comes in.

%Mathematical expressions are placed inline between dollar signs, e.g. $\sqrt 2, \sum_{i=0}^nf(i)$ \\or in display mode
%\[ e^{i\pi}=-1\] and another way, this time with labels,
%\begin{align}
%\label{line1} A=B\wedge B=C&\rightarrow A=C\\
%&\rightarrow C=A\\
%\intertext{note that}
%n!&=\prod_{1\leq i\leq n}i \\
%\int_{x=1}^y \frac 1 x \mathrm{d}x&=\log y
%\end{align}
%We can refer to labels like this \eqref{line1}.



\subsection{Word2Vec}
% https://arxiv.org/pdf/1301.3781 Jurafsky 6.8
The Word2Vec algorithm was developed by Mikolov et al. in 2013 \cite{word2vecoriginal} and was a game changer in language modelling. Using a neural network approach, it is capable of creating word embeddings that capture semantic relationships between words, not just the frequency with which they appear alongside each other. It does this by building vector representations of word using the contextual information of its neighbouring words - allowing it to produce an embedding that, for example, positions the the word 'student' close to the word 'teacher'.

% vecotr semantics - convergence of two bbig ideas s.6.2 Jurafsky
\paragraph{}Without delving too much into neural networks, they consist of an input layer, an output layer and one or more hidden layers of 'neurons' in between. To take a classification example, the input layer takes some sample 'X' as an input and the output layer produces a value that the neural network thinks(!) represents the input; For example, a digit-recognition neural network would take as input, a 2D greyscale image of a hand-written digit (unfurled into one long vector of pixel values), the output would be one of 10 numbers, 0-9 indicating what digit has been recognised.

\paragraph{}The hidden layers' role in this is to manipulate the outputs from their upstream neighbour in such a way that the output at the end is the correct classification number. The connections between each layer have \textbf{weights} and these weights are updated as the network 'learns' to give the correct classification through running through thousands of labelled training examples.

\paragraph{}Word2Vec is an example of a shallow neural network - meaning it has only one hidden layer connecting inputs to outputs. Under the covers, it trains a binary classifier using a method called negative sampling. For each target word, the model treats that word and a neighbouring context word as a \textbf{positive} example, it randomly samples other words to get a \textbf{negative} sample and then trains a binary classifier to distinguish between those two cases. 

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\begin{center} The learned weights of a word2vec model are the word embeddings \end{center}}
\end{tcolorbox}


%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%	{\small \textbf{word2vec} aims to create vector representations of each word in its vocabulary such that similar words are 'close' to each other in the multi-dimensional vector space. 'Closeness' in this case is measured by the cosine distance between the two points. A good embedding would thus position the words 'student' and 'teacher' close to each other in the vector space. In fact, Mikolov even showed that simple algebraic operations upon their word2vec vectors make semantic sense, the classic example being (where $wv$ indicates the word vector representation of a word:
% \begin{center} $wv(king) - wv(man) + wv(woman) = queen$  !\end{center} }
%\end{tcolorbox}

\subsubsection{Word2Vec architecture and method}
Word2Vec has two different architectures:
\begin{itemize}
    \item \textbf{Continuous Bag of Words (CBOW)}: Predicts the current 'target' word given the context words (i.e., the words around it).
    \item \textbf{Skip-Gram}: Does the opposite - predicting the surrounding context words given the current word.
\end{itemize}

A detailed discussion of the mathematical formulation behind word2vec is beyond the scope of this document. Essentially, both of the models work by sliding a \textbf{window} of fixed length across the text. The centre word is the current word being investigated, the context words are the neighbouring words either side according to the window size hyperparameter. Thus in the example below \ref{fig:w2vwindowsize}, the centre word is 'silently' and with a window size of 2 the context words are 'fish' and 'moved' to the left and 'through' 'the' to the right. In the next iteration, the centre word will become 'through' and the context words 'moved', 'silently' 'the' and 'night' etc.\\

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/w2v_windowsize.png}
    \caption{Word2Vec target word and context words with a window size of 2}
    \label{fig:w2vwindowsize}
\end{figure}

\todo{todo: explain how this build up the embedding}\\
\todo{todo: brief mention of doc2vec and other variations?}

%\subsubsection{Word2Vec variants}
%Doc2Vec \cite{doc2vec} is an extension of word2vec but creates vector representations of entire %documents, rather than individual words, it is generally used for document classification and document %similarity analysis.

\subsection{Transformers and Large Language Models}
Neural networks suffer from what is called the '\textbf{vanishing gradient}' problem. As the depth of a network increases, the updates to network weights essential for 'learning' reduce towards zero. Practically what this means is that the networks struggle to capture '\textbf{long-range}' relationships between words across large bodies of text or long sentences. The Transformer \cite{transformer} architecture introduced by Vaswani et al. in 2017 resolved this issue and resulted in a step-chane in Natural Language Processing. 

\paragraph{}The key differentiator of Transformer models is the introducton of the 'attention' concept. This means that the model can focus on (pay attention to) different parts of an input sentence when producing each element of the output. It can also do this multiple times in parallel (called Multi-Head Attention).

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.5\linewidth]{images/transformer.png}
    \caption{Transformer Architecture \cite{transformer}}
    \label{fig:enter-label}
\end{figure}
\paragraph{} The result of this architecture is that the models can process large amounts of data and  incorporate context and word relationships from throughout a document (often referred to 'long-range relationships). This has significantly improved the performance of NLP tasks and their ability to encode huge quantities of textual data - ultimately the Transformer architecture has lead to the Large Language Models such as ChatGPT that are so popular today. 


\subsection{Protein Language Models}

Protein Language Models take the same concepts of the Natural Language Processing models presented above but apply them to the world of proteins. Essentially, instead of encoding words from language, PLMs encode amino acid sequences or protein domain architectures. Similarly, sentences refer to entire proteins. They have proven highly capable of learning the underlying intrinsic properties of proteins by processing large quantities of sequencing information \cite{plmrives2019} (Rives et al 2019) \cite{plmheinzingerElmo2019} (Heinzinger et al. 2019).

\paragraph{}More recently, the Transformer's \cite{transformer} ability to efficiently process large quantities of data, and establish long-range semantic relationships across large bodies of text, has lent itself extremely well to analysing the vast quantities of available protein information and protein sequence data. Some of these models are described briefly below. 

% 
% https://ucl.primo.exlibrisgroup.com/discovery/fulldisplay?docid=cdi_scopus_primary_2_s2_0_85135461799&context=PC&vid=44UCL_INST:UCL_VU2&lang=en&search_scope=MyInst_and_CI&adaptor=Primo%20Central&tab=Everything&query=any%2Ccontains%2CProtein%20Language%20Models&offset=50 

%Where NLP embeddings reflect grammar, pLM embeddings decode aspects of the language of life as written in protein sequences (Heinzinger et al., 2019; Ofer et al., 2021). This suffices as exclusive input to many methods predicting aspects of protein structure and func- tion without further pLM optimization through a second step of su- pervised training (Alley et al., 2019; Asgari and Mofrad, 2015; El- naggar et al., 2021; Heinzinger et al., 2019; Madani et al., 2020; Rao et al., 2019; Rives et al., 2021) or by refining the pLM through another supervised task (Bepler and Berger, 2019, 2021; Littmann et al., 2021b). Embeddings can outperform homology-based inference based on the traditional sequence comparisons opti- mized over five decades (Littmann et al., 2021a, 2021b). With little optimization, methods using only embeddings even outperform advancedMSA-basedmethods(Elnaggaretal.,2021;Sta€rk et al., 2021). Simple embeddings mirror the last ‘‘hidden’’ states/ values of pLMs. Slightly more advanced are weights learned by so-called transformers; in NLP jargon, these are referred to as ‘‘attention heads’’(Vaswani et al., 2017). These directly capture complex information about protein structure (Rao et al., 2020), e.g., allowing the transformer-based pLM ESM-1b to predict structure without supervision (Rives et al., 2021).

\begin{itemize}
    \item \textbf{TAPE-Transformer}: The TAPE Transformer (Task Assessing Protein Embedding) is an adaptation of the original Transformer \cite{transformer} model where each amino acid is represented as a token (although the team used 25 amino acid tokens, not 20, including tokens for ambiguous or unknown aminos as well). The model is pre-trained on a large corpus of protein sequences using masked language modelling \todo{todo clarify} and then fine tuned for downstream tasks across five biologically relevant downstream tasks - such as secondary sequence prediction. This model is significant in that it demonstrates the effectiveness of transfer learning whereby models are initially pre-trained on large datasets and then fine tuned. It provides a foundation for much of the subsequent models in this area.
    
    \item \textbf{ESM (Evolutionary Scale Modeling)} \cite{plmesm}: Developed by researchers at Facebook, this model also uses a  Transformer \cite{transformer} architecture with amino acids as its vocabulary. It is also pre-trained on a large set of protein sequences (the first release, ESM-1b was trained on 86 billion amino acids across 250 million sequences spanning evolutionary diversity). This model is significant due to the scale of its pre-training task and its ability to capture both evolutionary and biochemical information at scale. Significantly, the team showed that, without prior knowledge the learned representations enabled stat-of-the-art supervised prediction of mutational effect and secondary protein structure. 
    
    \item \textbf{ProtTrans} \cite{plmprottrans}: ProtTrans is actually a suite of protein language models based upon a variety of transformer architectures (BERT \cite{bert}, Albert \cite{albert}, XLNet \cite{xlnet} and T5\cite{t5}). The main differentiators between ProtTrans and other PLMs is that it provides a systemsatic comparision of different transformer architectures, it is also quite efficient and thus useful for more downstream tasks; the T5 based models can also handle a number of diverse tasks in parallel.
    
    
    
    
    % https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1
    \item \textbf{ProteinBERT}: A universal model that integrates various biological data sources to enhance its predictions4. % https://arxiv.org/abs/2211.16742
\end{itemize}


%PLMs can leverage vast amounts of unlabeled sequence data, capture long-range %%dependencies, and transfer learning across different protein-related tasks %[@Rives2021]. They have shown success in various tasks, including:
%   \begin{itemize}
%       \item Protein structure prediction [@Senior2020]
%       \item Protein function annotation [@Littmann2021]
%      \item Protein engineering and design [@Madani2021]
%       \item Evolutionary analysis [@Biswas2021]
%   \end{itemize}




\newpage
\chapter{Literature Review}
This section presents an overview of the latest research in this important field.

Despite recent game-changing advances (often aided by Machine Learning and/or Computer Science), we still have a lot to do in order to fully comprehend the structure and function of proteins.\\
As proteins are long lines of single-character amino-acid codes, they lend themselves quite well to Language Modelling. Thus a number of studies have used LLM models in an attempt to identify patterns or meaning within the sequences in order to predict function or aid in downstream tasks. From a review of relevant literature, these tend to have adopted one of two approaches.\\

\textbf{Sequence-based} studies have focussed on the low level amino acid sequences that make up the proteins \todo{what do these do?}. \textbf{Domain-based} studies create sentences based upon the higher-level architecture of proteins - such as their families. The former type have access to a much wider set of data - simply because we do not understand enough about proteins to have created a comprehensive architecture for each one! This particular dissertation extends a previous domain-focussed study by broadening its parameters. 

\subsection{Sequence based approaches}

\textbf{Yang et al- Learned protein embeddings for machine learning}
% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6061698/

Yang et al used embedding techniques to analyse vast quantities of unmeasured protein sequences, they encoded the amino acid sequences themselves rather than the known properties of the proteins. They used a 2 step appraoch - firstly breaking each 524,529 unlabelled sequences into 'k-mers' and feeding those into a doc2vec model which learns to predict the vectors for centre k=mers. They then use a supervised approach.\\

%-- Yang et al
"There have been efforts to apply word2vec and doc2vec to represent protein sequences (Asgari and Mofrad, 2015; Kimothi et al., 2016; Mazzaferro, 2017; Ng, 2017). These embeddings treat the amino acid sequence as a document and fragments of the amino acid sequence of constant length k (k-mers) as words. As shown in Figure 1, a sequence of nine amino acids can be divided into three sets of non-overlapping 3-mers. The learned k-mer embeddings place k-mers that occur in similar contexts near each other in the embedded space by learning to predict a k-mer from its surrounding context k-mers and the sequence embedding. These embeddings have achieved high accuracy in differentiating ordered and disordered proteins and modest accuracy in classifying proteins from SwissProt into families based only on their primary sequence (Asgari and Mofrad, 2015)."\\

\textbf{Asgari and Mofrad - BioVec (2015)}
% https://pubmed.ncbi.nlm.nih.gov/26555596/
% Asgari and Mofrad
We introduce a new representation and feature extraction method for biological sequences. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. In the present paper, we focus on protein-vectors that can be utilized in a wide array of bioinformatics investigations such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction.\\


\textbf{Kimothi et al.- seq2vec (2016)}
% https://arxiv.org/abs/1608.05949
% Kimothi et al. 
Embedding a biological sequence over a nucleotide or amino acid alphabet in a lower dimensional vector space makes the data more amenable for use by current machine learning tools, provided the quality of embedding is high and it captures the most meaningful information of the original sequences. Motivated by recent advances in the text document embedding literature, we present a new method, called \textbf{seq2vec}, to represent a complete biological sequence in an Euclidean space. The new representation has the potential to capture the contextual information of the original sequence necessary for sequence comparison tasks.\\

\textbf{Mazzaferro - Predicting protein binding affinity ....}
% https://www.biorxiv.org/content/10.1101/128223v1
"..extend the current efforts in the field [to group of proteins named Major Histocompatibility Complex (MHC), to which epitopes (also proteins sometimes named antigenic determinants), bind to eliciting a response] by applying a variety of network architectures based on RNNs and word embeddings (WE). 


\subsection{Domain based approaches}
These studies focus on the known domain architecture of proteins in the hope that the models may be able to point towards new domains.\\

\textbf{Melidis et al. - Capturing domain structure and
function using self-supervision on protein domain
architectures}
In this study, Melidis et al \cite{llmdom2vec} create a corpus from 128,660,257 proteins containing Interpro signatures, each of their sentences consisted of the Interpro annotations for those proteins. Using both CBOW and SKIPGRAM word2vec architectures to embed the words. They refer to their approach as dom2vec indicating that the words in their corpus are protein \textbf{dom}ains.
\paragraph{}They evaluated the predictive ability of the resulting embedding space in four ways.  they evaluated the resulting embedding space  and evaluated them against their ability to They conclude that their approach outperforms sequence-based approaches for toxin and enzymatic function prediction and and is comparable with sequence embeddings in cellular location prediction. \\

\textbf{Buchan and Jones - Learning a functional grammar of protein domains}
In this paper, Buchan and Jones train the word2vec algorithm on a corpus consisting of the protein domains of eukaryotic proteins. They focussed on eukaryotic proteins as there are few proteins in the bacterial and archaeal kingdoms that have multiple domains with independent evolutionary histories.  They benchmarked nearest neighbour classifier performance on predicting the three main GO ontologies of a Pfam domain and propose that this approach could be used to suggest putative GO assignments for Pfam domains of unknown function. IN creating an embedding space, the main difference with the Melidis paper is that Buchan and Jones used eukaryotic proteins only, in the word2vec configuration the skipgram architecture was used and most of the default settings were adopted.\\



\section{Motivation and Objectives}
This dissertation also adopts a domain-centric approach that is similar to the Melidis \cite{llmdom2vec} and Buchan \cite{llmword2vec} approaches. However, there are some key differences:

\begin{enumerate}
    \item In contrast to Melidis \cite{llmdom2vec}, this paper follows the lead of Buchan and Jones \cite{llmword2vec} by focussing on eukaryotic proteins only
    \item Different corpus constructs are also used. The Pfam domain tokens from each sequence are maintained but there are different approaches to the treatment of 'GAP' areas. Melidis \cite{llmdom2vec} used a gap of 30 amino characters to include the word "GAP" as a token in a sentence; Buchan and Jones \cite{llmword2vec} experimented with different rules and words \todo{what are these?}. This study includes the word 'GAP' only if the actual gap is either 1 character, 50 or 100 characters long. It also uses the word "START\_GAP" and "STOP\_GAP" to cover the scenarios where there is a gap at the start and end of a sequence. Different models are created using these 3 different corpus configurations as inputs.
    \item Rather than just one or two models, this study creates a multitude of word2vec models to test the impact of different hyperparameters. This includes the model architecture (both cbow and skipgram are used) as well as different minimum word counts (ranging from 1 to 8), window sizes (ranging from 3 to 44) and vector sizes ranging from 5 to 1,000
    \item To identify a model that may provide biological pointers, each model is compared to a supplied distance matrix to identify a 'best candidate' for clustering. The supplied matrix contains pairwise similarity measures for one representative protein from each Pfam domain. This is compared with the pairwise distance matrix ov each models vocab.
    \item To asses the biological significance, this paper investigates whether KMeans can find clusters within the embedding space that correspond to Pfam clans as opposed to GO terms. \todo{why?}
\end{enumerate}



% ----------------------------------------------------------------
%             Chapter 2 - Methods
% ----------------------------------------------------------------
\chapter{Methods and Approach}

The section describes the end to end approach and methodology followed in preparing this dissertation and provides an explanation of any decisions made as well as insights made along the way.

\section{Guiding principles}
There are many different ways of approaching the objectives of this dissertation and decisions are necessary along the way. It's useful to have guiding principles in place to help make decisions - and these should be lead by a set of wider objectives. \\

Within the context that this dissertation is for a Masters thesis and not (yet) for a Nobel prize, the objective is to produce a high quality dissertation that achieves an academically useful and interesting outcome, puts into practice the material learned through the taught elements of the course and provides opportunity to learn additional skills. \\

With this in mind, the principles adopted in undertaking this dissertation are:

\begin{itemize}
    \item Do not have any preconceived results - if the data is pointing you in one direction, follow the data.
    \item Use the most appropriate tool for the job - there's no point reinventing the wheel if there is a proven, suitable alternative available.
    \item Mistakes will be made 1 - choose methods that provide transparency such that errors can be identified as quickly as possible. In practical terms, this includes having good logging in place, reconciling inputs and outputs at each point in the process etc.
    \item Mistakes will be made 2 - It is certain that some steps will need to be repeated following an error. The approach should allow this to happen with minimal loss of time. In practical terms, this includes adopting good coding techniques (e.g. modular code), using version control (github), storing interim data at various points along the way etc.
    \item A preference that all steps can be reproduced on a regular laptop within a reasonable time frame. 
    \item Where additional computing or processing power is required, there is a preference for using tools that are also ubiquitous in industry. This principle is driven from a personal desire to maximise exposure to popular industry tools prior to returning to work. In practice it means considering cloud platforms over the UCL High Performance Compute environment from time to time.
\end{itemize}

% ----------------------- Methods - Overall Process
\section{Method - A 10,000ft overview}
At a 10,000ft level, this dissertation was prepared in five key steps from raw data download to clustering and analysis (as per figure \ref{fig:e2e_flow} ). At the end of analysis, results were investigated and the process repeated with slight modifications to further understand or improve the results. In summary, the process followed was:

\begin{enumerate}
	\addtolength\itemsep{-2mm}
	\item Download raw data from Interpro and Uniprot including proteins, protein families and disorder regions. Parse and prepare this data - extracting key tokens. Combine these tokens to create a corpus
	\item Using this corpus, create a range of word2vec models using different word2vec hyper-parameters
	\item Identify the 'best' of these models by performing a comparison with another distance matrix (provided), derived directly from representative Pfam protein sequences
	\item With that 'best' model, evaluate whether its associated vectorized word-encodings produce 'clusters' in the encoding space that correlate with actual protein family clusters (called 'clans') queried from Interpro
	\item Investigate and analyse the results, use these outcomes to motivate further experiments - iterating through different corpi or word2vec models
\end{enumerate}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/end2end_flow_2.png}
	\caption[Overview of the end to end approach]{Overview of the end to end approach}
	\label{fig:e2e_flow}
\end{figure}
\pagebreak



%
%
%
% ----------------------- Methods - Creation of corpus
%
%
%
%


% \section{Corpus preparation} The preparation of the corpus was a significant undertaking due to the large quantity of information to parse, cleanse and restructure prior to creating the corpus itself. Wherever possible this was undertaken locally on a Macbook. \\
%There was little uncertainty over the source of raw data and tokens to be used - there are well known protein databases online that contain this information for download. Furthermore, as this dissertation was building on a previous paper with this topic, the type of input data required was the same in order to provide a comparison (i.e. eukaryotic proteins, protein families and disorder regions). \\
%However, the format and size of input files was different - especially for disorder regions, and this presented a number of challenges in order to extract the data in a timely and repeatable manner. \\


\section{Data download and preparation}
The table below \ref{table_datasources} lists the sources of data used in preparing the corpus, their formats and sizes. 

%
% ----------------------- TABLE EXAMPLE ----------------------
%
\begin{table}[hbt!]
\centering
\label{table_datasources}
\begin{tabular}{|p{35mm}|p{16mm}|p{22mm}|p{25mm}|}
	\hline
	data & source & format & size (unzipped)\\
	\hline
	Eukaryotic Proteins &  Uniprot & fasta text files & 62.3 GB\\
	Protein Families&   Interpro & csv files  & 98.7 GB\\
	Disorder Regions&  Interpro & xml & 188.5 GB\\
	\hline
\end{tabular}
\caption{Data sources required in the creation of a corpus.}
\end{table}

%
% ----------------------- Methods - Protein download
%
\paragraph{Proteins - Download and parsing of data from Uniprot} The protein extract was downloaded from the online Uniprot protein database. It is possible to save some local processing by searching Uniprot for an extract of eukaryotic-only proteins rather than all proteins. However, it takes Uniprot up to 12 hours to prepare this extract which is then made available for download as a zip file (62.3 GB when unzipped).\\ The fasta format is widely used in bioinformatics for representing protein sequences. Each fasta entry represents a single protein and consists of two parts - a header line and the amino-acid sequence itself. The header line starts with a '\textgreater' symbol and contains the protein's name and accession number (unique identifier). The accession number provides a unique reference key contained within all the data files required for the corpus - allowing them to be knitted together to create a sentence per protein.
\paragraph{}The figure below \ref{fig:corpus_protein} shows an example of a single protein entry as it appears in Uniprot fasta download. It highlights the unique protein identifier (accession number) and the amino acid sequence. Although the sequence itself is not required for the corpus, its length is in order to create a corresponding sentence to represent it.


\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_protein_fasta.png}
	\caption[protein\_corpus]{Uniref100 Protein fasta extract, highlighting the areas of relevance for the corpus}
	\label{fig:corpus_protein}
\end{figure}

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
The protein information was extracted in python using the SeqIO module from the Biopython library. On a Macbook, it took 1,428s (24 minutes) to parse 95,272,305 eukaryotic proteins and extract the id and length of each protein to a csv file.
\end{tcolorbox}


%
% ----------------------- Methods - PFAM download
%
\vspace{5mm}
\paragraph{Protein Families (pfams)} Protein family information is available for download from Interpro. Interpro integrates data from multiple protein signature databases to provide a single consolidated view of the results of the various functional analyses that have been performed upon protein sequences. They maintain and regularly release updates to this information in the 'protein2ipr.dat' which is available for download as a 19GB (zipped) tab delimited file \href{https://ftp.ebi.ac.uk/pub/databases/interpro}{protein2ipr.dat}.\\ Each line of the file represents one entry from the underlying signature database mapped to its protein succession number. Thus for one protein there will be multiple entries. Specifically, for the purpose of the word2vec corpus, each line contains the following information:
\begin{itemize}
    \item The unique protein accession number (e.g. A0A010Q340)
    \item The source database entry for that protein identified by a unique key for the database source. For example, entries from the Pfam database have and id starting with 'PF' e.g. PF00172. Entries from the SMART database (which identifies repeated sequence motifs) have an id starting with SM e.g SM00906.
    \item The start and end position on the protein's sequence of the source database entry.
\end{itemize}
The extract is not limited to eukaryotic proteins, thus resulting in a 98.78GB file containing over 1.3bn lines. \\
Parsing the file itself requires identifying only those lines with PFAM entries, then extracting the PFAM id, Protein Accession number and the start and end positions of the pfam domain along on the protein's sequence.\\
An example is shown in figure \ref{fig:corpuspfam} below.

% ----------------------- Graphic: PFAM extract
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_pfam}
	\caption{Pfam data extract - relevant data for the corpus}
	\label{fig:corpuspfam}
\end{figure}
% ----------------------- Performance: PFAM extract
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\small The protein2ipr.dat file, consisting of 1,355,591,115 records was processed using standard python regular expressions. On a Macbook, it took 13,042s (\texttildelow 4 hours) to producing a csv file containing only protein id, pfam id and start and end positions of the pfam domain.}
\end{tcolorbox}


%
% ----------------------- Methods - Disorder region download
%
\paragraph{Disorder regions} A disordered region on a protein refers to a portion which lacks a stable 3-dimensional structure. The disorder region information is also available from Interpro and contained within the 'extra.xml' file. This file contains metadata which supplements the main Interpro dataset but which is not included in the main protein2ipr.dat file. \\ Parsing this file proved to be one of the more challenging tasks in data preparation. \\ The XML has a relatively simple structure only 3 levels deep - \xml{protein} elements at the top level identify each protein by its unique accession code. \xml{match} child elements provide information on various metadata entries for that protein. This includes GO Terms (Gene Ontology), cross reference links to external databases, taxonomy information and more. Disorder regions are identified by the attribute {\small MobiDBLite} within the \xml{match} elements. The position of these items along the length of the protein are contained within \xml{lcn} tags nested underneatrh the \xml{match} element. \\ Although this file is well structured XML, because it serves as a 'catch all' for a wide range of protein meta-data it is quite large. The parsing challenge is due to the sheer volume of information it contains.
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\small The extra.xml file from Interpro, is 188 GB in size. It consists of 4 billion lines of xml and 230,397,847 \xml{protein} tags. Nested within this structure are the 57 million disorder entries we are interested in.}
\end{tcolorbox}

\paragraph{} Attempts to extract this information locally using fast lex processors failed - these processors work by parsing a file sequentially rather than trying to read the whole tree structure into memory. Larger Amazon Web Service (AWS) servers were also tried, but these too ran out of memory, leading to the suspicion that there may be a bug in the python parsing library which should not have been using so much memory. \\ Finally, the only realistic strategy was to split the xml files into separate, smaller chunks and then parse those individually using the aforementioned python xml parsers.  This was achieved by producing 24 separate, well-formatted xml files, each containing 10M \xml{protein} xml elements. Standard python regular expressions were then used to extract the relevant information from each of these into a csv format which could then be loaded into a local database.

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\small It took on average 5mins to chunk a set of 5M proteins from extra.xml (chunk\_disorder\_xml.cpp), and XXXX mins to then run the parser and finally 2min 25s to load the disorder information into the database.}
\end{tcolorbox}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_disorder}
	\caption[corpus metadata]{extra.xml - relevant disorder data for corpus}
	\label{fig:corpusdisorder}
\end{figure}




% --------------------------------------------------------- 
%               CORPUS CREATION
% ---------------------------------------------------------



\section{Corpus creation}
With the raw input files parsed and the key data elements (protein ids, pfam domains and disorder regions) extracted into csv files, the next step is to consolidate these constituent pieces of data to create a single 'sentence' for each protein. The collection of sentences for each protein is the \textbf{corpus} that will be used to train the word2vec model. 
\paragraph{}Each sentence consists of words that represent either \textbf{pfam domains} or \textbf{disorder regions}, arranged in the same order that they appear on the protein sequence itself (with overlaps removed).
\paragraph{}The data components required to form a sentence are all linked by the unique protein accession id. Also, although the protein information downloaded from Uniprot already contains only eukaryotic proteins, the pfam and disorder regions are more wide-ranging and needed to be filtered down. This is ideal territory for a relational database and the approach for this next step was largely driven by that choice of technology.
\paragraph {Data load into a database}
A fast database called 'duckdb' was used locally to store the data from the previous stage. DuckDB uses a columnar-vectorized query execution engine where queries are still interpreted but large batches are processed in one operation. It is easily installed on a Macbook, has a low memory and file-system footprint, has full integration with Python and can load large csv files in seconds with a single line of python code. As per the principles adopted for the method, adopting this approach also facilitated debugging and is quick to re-run should an issue be identified later on in the process.
\paragraph{}Thus the data from the csv files was loaded into two separate tables - one to hold the protein information (W2V\_PROTEIN) and another to hold both the pfam and disorder details (W2V\_TOKEN) as per below:
\vspace{5mm}

\begin{center}
	\begin{tabular}{|p{25mm}|p{85mm}|}
	\hline
	\multicolumn{2}{|c|}{\textbf{W2V\_PROTEIN}} \\
	\hline
	COUNTER&A simple integer counter to help with table iteration when combining data into a corpus  \\
	\hline
	UNIPROT\_ID&The unique accession id of the protein \\
	\hline
	LENGTH&  Length of the protein sequence \\
	\hline
\end{tabular}
\end{center}

\vspace{5mm}

\begin{center}
	\begin{tabular}{|p{25mm}|p{85mm}|}
	\hline
	\multicolumn{2}{|c|}{\textbf{W2V\_TOKEN}} \\
	\hline
	UNIPROT\_ID&A reference to the unique accession id of the protein    \\
	\hline
	TYPE&Whether the token is for a pfam entry or a disorder region on the protein  \\
	\hline
	TOKEN&The token itself - for a pfam entry this is the pfam id, for a disorder region it is the description of the region    \\
	\hline
	START &  The start position of the token along the protein sequence  \\
	\hline
	END &  The end position of the token along the protein sequence  \\
	\hline
\end{tabular}
\end{center}

\vspace{5mm}

\paragraph{Extracting only eukaryotic protein data}
With the data in the database, it was a relatively simple matter to create a table join between the w2v\_protein and w2v\_token tables and extract only the information for eukaryotic proteins. This step produced in one output line per token per protein saved to a file - referred to as the pre-corpus \ref{fig:corpusmetadata}. \\

\begin{table}[hbt!]
\centering
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm,code={\onehalfspacing},]
\textbf{Observation/Lesson learned}: By adding a 'COUNTER' to the w2v\_protein table, join queries were extremely quick and allowed 10M proteins and their associated tokens to be processed in 30s. \\ \\ An alternative approach using the inbuilt 'group by' and 'count' keywords of the SQL language did not perform - these instructions cause the entire dataset to be loaded first into memory prior to applying any group by, count or ordering clauses; this causes memory issues and is extremely slow - taking up to 1 hour to process 10M rows, with the duration increasing with each querty . \\ \\ By contrast, adding a COUNTER column (integer from 0 increasing by 1 for each subsequent row), allows queries to be 'paginated' - each query only returns the rows corresponding to the COUNTER values requested. The COUNTER values can be tracked in python as each query returns. \\ \\ 
The SQL used is shown below with the 'start' and 'end' variables tracked with python code.\\ \\
	{\small {SELECT T1.UNIPROT\_ID, T1.TOKEN, T1.TYPE, T1.START, T1.END FROM W2V\_TOKEN T1 WHERE UNIPROT\_ID IN ( SELECT UNIPROT\_ID FROM W2V\_PROTEIN T2 WHERE T2.COUNTER \textgreater= start and T2.COUNTER \textless end ORDER BY T2.COUNTER)}}
    \end{tcolorbox}

\caption{SQL join to extract pfam and disordered 'tokens' for eukaryotic proteins}
\end{table}


\paragraph{Combination of all tokens per protein}
The databae query described above, creates an output file with multiple lines for each protein (each line containing the information for one token). \\ \\These are then combined to create a \textbf{pre-corpus}\ref{fig:corpusmetadata} file containing all the metadata required to produce a single sentence per protein. A protein's meta-data consists of the protein identifier, the sequence length, the total number of tokens (pfam domains or disorder domains) as well as the number of each type of token and their start and end positions along the length of the protein. \\
This information is used both to combine all the relevant data into a single 'sentence' of tokens per protein, and to identify information about the average length and numbers of tokens in a protein - this proves useful when deciding upon hyperparameter options for the word2vec models.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_metadata}
	\caption[corpus metadata]{Pre-corpus metadata - all possible tokens for a protein}
	\label{fig:corpusmetadata}
\end{figure}

\paragraph{Final corpus creation}
Finally, the pre-corpus metadata was parsed to remove overlapping regions and insert the word 'GAP' where the region on the protein had no definition. There were many gaps within each sequences, thus the words 'START\_GAP' and 'END\_GAP' were used to differentiate between unidentified areas at various points on the sequence.

As an example, combining all the information from the previous sections, the resulting sentence for the protein '\textbf{A0A010Q340}' is shown below. The final corpus output consisted of 50,894,561 sentences. \\
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_line}
	\caption{Final corpus \- The sentence to be included in the corpus for protein A0A010Q340 after removing overlapping tokens and filling gaps with the word GAP and adding START\_GAP and STOP\_GAP token to gaps at the extremities.}
	\label{fig:corpusline}
\end{figure}


% --------------------------------------------------------- 
%               CORPUS CREATION
% ---------------------------------------------------------

\section{Model creation and selection}
As described in more detail in the background section, \textbf{Word2Vec} is a neural-network model used in word prediction problems. It was developed by \textbf{Tomas Mikolov} and colleagues at \textbf{Google} in 2013. It works by parsing a supplied corpus of words/sentences and building a vector representation of each word based upon how often it appears alongside other words in the corpus. The idea is that words that are related to each other (i.e. they appear close together in sentences) will also be close to each other in the multi-dimensional vector space produced by the model.\\
The model has two main variants:

\begin{itemize}
    \item \textbf{CBOW} (Continuous Bag of Words) : This attempts to predict a word based upon its surrounding context words
    \item \textbf{Skip-Gram} : Tries to predict context words based upon the target word.
\end{itemize}
To align on terminology, the vector representation of a word created by the model is referred to as its \textbf{embedding}; the list of unique words that end up being embedded are referred to as the  model's \textbf{vocab}.\\ 
For each model variant, \textbf{hyper-parameters} influence how the vector representations are created. For our purposes a number of different models were created for each of the CBOW and Skip-Gram variants using different values for these hyper-parameters:

\begin{itemize}
    \item \textbf{vector size} : The number of dimensions used to embed each word.
    \item \textbf{window size} : The maximum distance (ie number of words) either side of each word to consider when determining relationships between that word and its neighbouring words.
    \item \textbf{minimum count} : Sets the threshold below which words will be discounted from the embedding process if they are infrequent. This can be used to eliminate rare words that could skew the results. The higher this number is, the smaller the resulting model vocabulary will be as certain words are ignored.
\end{itemize}

\subsection{Selection of hyper-parameters and model creation}
The choice of hyper-parameters to use and the number of models of each variant to create was determined by analysing the makeup of the corpus itself and with consideration of cost (time and financial) and available libraries - resources are limited so it is not possible to cover all possible variations!

\paragraph{Corpus analysis and model hyper-parameters}
The corpus was analysed to determine the minimum count and window size.


\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
	\hline
	metric & max & min & mean & std dev & 90th & 95th & 97.5th & 99th\\
	\hline
	Tokens per sentence & 3,991 & 1 & 2.83 & 3.91 & 6.0 & 9.0 & 12.0 & 18.0 \\
	pfam tokens per sentence & 3,991 & 0 & 1.79 & 2.64 & 3.0 & 5.0 & 7.0 & 10.0 \\
	disorder tokens per sentence & 285 & 0 & 1.04 & 2.91 & 3.0 & 6.0 & 9.0 & 13.0 \\
	\hline
\end{tabular}
\caption{Analysis of the different tokens in a protein sentence}
\end{table}

From analysis of this information it is clear that:
\begin{itemize}
    \item There are some outliers whose protein sequence consists only of pfam tokens and nothing else - the longest sentence has 3,991 tokens and these are all pfam domains.(e.g. 3,991 pfam tokens!). These outliers are included in the model - if these sentences contain pfam tokens that are not observed frequently, they will be ignored by the model anyway. 
    \item On average, each protein sentence contains just under 3 tokens (2.83) of which roughly 2 (1.79) are pfam domains and 1 (1.04) is a disorder region. From a performance perspective, this indicates that there's no point running models with a very high min-count, as this would not actually make much of a difference.
    \item The 95th percentile number of tokens is 9 - i.e. the majority of sentences have only 9 words or less
    \item The 95th percentile number of pfam tokens on a protein is 5 and is only 7 at the 97.5th quantile.is 9 - i.e. the majority of sentences have only 9 words or less
\end{itemize}

In terms of model execution, the approach taken was to first trial a few configurations and determine execution times and then decide how to parallelise production. These initial tests showed that each model took between 20 and 30 minutes to run - dependent largely upon the minimum word count and window size.
\paragraph{} A key objective of this dissertation is to determine which combination of word2vec hyperparameters result in the 'best' model. However, an exhaustive search needs to be balanced by time and cost. Given the availability of relatively cheap compute resource from Amazon Web Services, it was decided to instantiate a number of EC2 (EC2: Elastic Cloud Compute - essentially a cloud-based server) instances, and get them to run in parallel, generating different models with different hyperparameter configurations.
\paragraph{}Eventually a cluster of 4 AWS servers was created to generate all the cbow models, with the skipgram models being run on a local Macbook. The AWS services ran continuously over 12 hours are a cost of about \$40.
\paragraph{} By the end of this process, the following model configurations were created - each of these for both CBOW and Skip-Gram word2vec variants.

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	parameter & value 1 & value 2 & value 3 & value 4 & value 5 & value 6 \\
	\hline
	min word count & 1 & 3 & 5 & 8 & - & - \\
	window size & 3 & 5 & 8 & 13 & 21 & 44 \\
	vector size & 5 & 10 & 25 & 50 & 75* & 100* \\
	\hline
\end{tabular}
\caption{Data sources required in the creation of a corpus.}
\end{table}
* These models were not created for cbow

\subsection{Selecting a 'best' candidate model}\label{distancematrix}
As we are trying to find biological meaning in the data, the best model to take forward is determined by correlating the encodings of the vocabulary from the word2vec model with a similarity matrix prepared directly from pfam sequences. This 'evolutionary' matrix is therefore derived more directly from amino-acid sequences, whereas the word2vec only looks at relationships between the relative positions of a sequence on a protein.

To perform this comparison, it was necessary to compute pairwise distances between the words in the vocabulary of the word2vec models. The size of the vocabulary in each model is directly related to the 'min count' hyper-parameter - this setting causes the model to ignore words that do not appear less than 'min count' times in the vocabulary.


\paragraph{The Pfam distance matrix}
Pfam version 100 and EMBOSS 6.4.0 were downloaded. Pfam 100 contains
20,651 Pfam families. A single random Pfam family a single random 
member of each Pfam family was chosen to act as a representative 
sequence. These 20,651 were composed into a single fasta file dataset of 
Pfam “reps”.
Using the EMBOSS implementation of Needleman and Wunsch (NW) all 
pairs of Pfam Rep-to-rep alignments were calculated for a total of 
426,463,801 comparisons. NW alignment scores were recorded for all 
alignments and the alignments discarded. Scores were composed into an 
n x n, symmetric similarity matrix and this matrix was normalised to 
between 0 and 1 such that the maximum similarity is 1. And this matrix 
was then converted to a distance matrix but taking 1 – sim(x,y) for each 
cell in the matrix.

DB: 
rand\_rep was shorthand to note that this data was created by picking one random from each pfam family and using that as the representative sequence when doing the all-against-all sequence comparison/alignment. It probably introduces some noise/error in to the distance matric because one random sequence from the pfam family can not be guaranteed to be the cluster centroid for that family. But I guess that isn't hugely significant. Though with regards interpretaion, it might be a reason you don't get great correlations with the embedding distance matrix (presupposing the embeddings are better at find some kind of centroid-like entity).


\paragraph{Comparison of vocabulary sizes}
As a result, the word2vec vocabulary have the following sizes:

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|}
    \hline
	unique pfams in corpus & 15,481 \\
	\hline
	unique pfams in r\_and\_rep matrix & 20,651 \\
	\hline
\end{tabular}
\caption{Comparison of Vocabulary sizes between w2v corpus and r\_and\_rep pfams}
\end{table}

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|c|c|}
	\hline
	w2v hyperparameter & resulting w2v vocabulary size \\
	\hline
	1 & 15,481 \\
    3 & 13,535 \\
    5 & 12,815 \\
    8 & 11,884 \\
	\hline
\end{tabular}
\caption{Vocabulary sizes for each word2vec min\_count hyperparameter.}
\end{table}

\paragraph{Creating Distance Matrices for word2vec models}
A distance matrix simply contains the calculatoins of pairwise distances between vectors in vector space. Thus for the word2vec model with a minimum word count of 1, there are 15,485 words in the vocab and the same number of vectors. The pairwise distance matrix has 15,485 x 15,485 = 233,785,225 entries although the diagonal entries are 0.0 and the matrix is symmetrical.

There are generally two ways of calculating vector distances - euclidean and cosine. Euclidean distance would appear to be the better measure considering the nature of the problem, but out of interest, both measures were calculated for each model.

\begin{table}[hbt!]
\centering
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\textbf{Euclidean distance}: Measures the point to point distance between to points in space. \\
 \textbf{Cosine distance}: Measures the angular distance between tow vectors. If two vectors point in the same direction they will be 'closer' together by this measure.}
    \end{tcolorbox}
\caption{Euclidean v Cosine distance measures}
\end{table}

Pairwise distance matrices were calculated using the sklearn.metrics.pairwise module's cosine\_distances and euclidean\_distances functions and then normalised.

\paragraph{Preparing matrices for comparison}
Prior to comparing matrices, they required some manipulation in order to compare like for like. Both matrices contain distances between pairs of pfam domains, but the word2vec vocab of pfams is smaller due to the way word2vec encodes - and in particular wrt to the min count parameter.

Thus, as per figure \ref{fig:distance_matrices} some manipulatoin was required before comparing. This required
\begin{itemize}
    \item Removing rows and columns from the r\_and\_rep matrix for pfam entries that do not exist in the word2vec vocab
    \item Doing the same for the word2vec matrices \- remove the entries that are not in r\_and\_rap
    \item Make sure that the order of the matrices are the same (ie that the common pfam entries appear in the same rows/columns within each matrix.
\end{itemize}

This exercise was performed with the help of the \textbf{skbio} python library. This library contains a very useful set of functions for comparing different distance matrices. This requires firstly loading the two matrices into two wrapper classes of type \textbf{DistanceMatrix}. Once in that format, skbio can resize them to contain only common indices.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/distance_matrices.png}
    \caption{Preparing distance matrices for comparison}
    \label{fig:distance_matrices}
\end{figure}


\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|}
    \hline
	description & pfam count \\
	\hline
	total pfams in r and rep & - \\
    total pfams in w2v mc1 & - \\
    pfams in r and rep but not in w2v & - \\
    pfams in w2v but not in r and rep & - \\
    resulting matrix size & - \\
	\hline
\end{tabular}
\caption{Common entries on distance matrices}
\end{table}

\paragraph{Performing matrix comparison}
Finding the 'best' word2vec model to take forward for further analysis, requires finding the word2vec model whose distance matrix was closest to the r\_and\_rep matrix. There are a number of commonly used metrics for determining the correlation between matrices. These include:

\paragraph{Pearson Correlation}
The Pearson correlation coefficient measures the \textbf{linear} relationship between two datasets. 

% https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr

\paragraph{Spearman Correlation}
Spearman's rank correlation is a non-parametric measure of the monotonicity of the relationship between two datasets; it's commonly used when the relationship between variables is non-linear. For distance matrices, it measures how well the rank order of distances in one matrix matches the rank order in the other matrix. 
% https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html#scipy.stats.spearmanr

\paragraph{Mantel Test}
A Mantel test  compares two distance matrices by computing the correlation between the distances in the lower (or upper) triangular portions of the symmetric distance matrices. However, the correlation calculation still relies upon either  Pearson’s or Spearman’s rank correlation coefficient.

% https://scikit.bio/docs/dev/generated/skbio.stats.distance.mantel.html#rcee8d6e1aac4-1

\paragraph{}All these correlation coefficients vary between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.

\paragraph{}The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. Although calculation of the p-value does not make strong assumptions about the distributions underlying the samples, it is only accurate for very large samples (>500 observations).

\paragraph{} All of these statistics have implementationt in the scipy python library and are very quick to create once two correctly sized matrices are in place. Thus it was a simple matter to create all 3 for comparison. The results are show in the Experiments section.


% ----------------------------------------------------------
%
% ----------------------  CLUSTERING -----------------------
%
%-----------------------------------------------------------

\section{Clustering encoded pfams}
The key objective of this dissertation is to establish whether the vector representations of pfam 'words' as generated by the word2vec models, are positioned in vector space such that the clusters they form in that space bear some correlation to a biological or evolutionary cluster derived from more traditional means. If successful this could allow useful new insights to be gained from the vector clusters.

\paragraph{}There are two parts to this - on the one hand, there are numerous Machine Learning algorithms that will cluster a dataset based upon the features of each sample (pfam word encoding). The KMeans algorithm is a good example of this.  On the other hand, another grouping of protein family domains is required to provide a comparison. For this, pfam clans were used.
\paragraph{} A \textbf{Pfam clan} \cite{pfam} groups together multiple Pfam families that are believed to share a common ancestry through evolution. This grouping is determined based on shared features including sequence motifs, sequence structure, or other evidence that points to a common evolutionary origin. Pfam clans provide a higher-level organization of protein families, making it easier to study the evolutionary relationships between large groups of proteins.

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	\textbf{Characteristics of Pfam Clans}
 \begin{itemize}
     \item Common Evolutionary Origin: Members of a Pfam clan are derived from a single evolutionary ancestor, although they may represent different protein families today.
     \item Hidden Markov Models (HMMs): Each family within a clan is represented by an HMM - a statistical model used to describe protein sequence patterns that have been conserved through evolution. Clans group related HMMs together.
     \item Functional and Structural Similarity: Clans often consist of protein families that share structural or sequence similarities due to their common origin - even if they have different functions.
     \item Hierarchical Organization: Pfam clans provide a higher-level organization of protein families, making it easier to study the evolutionary relationships between large groups of proteins.
 \end{itemize}
    \end{tcolorbox}
\vspace{5mm}
With this understanding, the problem statement can be more succinctly rephrased as:
\vspace{5mm}
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	\textbf{Problem Statement} \\ \textit{Given a dataset of pfam domains encoded into a multi-dimensional vector space, can Machine Learning techniques identify clusters that correlate with pfam clans - which themselves group multiple Pfam families that share a common evolutionary ancestor}
    \end{tcolorbox}
\subsubsection{Retrieving pfam clans} 
Retrieving pfam clans is straightforward. Interpro provide a simple webservice API that returns details for a pfam entry, including its clan (if it is defined). As as shown in figure \ref{fig:queryclan}, this is easily achieved in python.
\paragraph{}The vocab (pfam ids) is retrieved from a model and for each word in that vocab, the Interpro API is queried, the json response is parsed and the clan id extracted with a regular expression. The pfam to clan relationship is then stored in the local database in keeping with the principles of the method (whilst also removing the need to continually call the Interpro API!).

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/queryclan.png}
    \caption{Querying Interpro for a pfam's clan}
    \label{fig:queryclan}
\end{figure}

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
Interpro API example: https://www.ebi.ac.uk/interpro/api/entry/pfam/PF13041
\end{tcolorbox}

\subsection{Selecting a clustering algorithm}
There are multiple options available for clustering the word vectors. These methods are well reference elsewhere, so a summary of the key features of some of the main ones is presented below.\\

\Paragraph{\textbf{K-Means Clustering}}
K-Means is a popular Unsupervised model. Unlike Supervised models, Unsupervised models are not 'trained' upon a set of data and their 'correct' labels. Instead they try to find patterns and structures within the data itself and use those patterns to group similar samples together (K-Means uses distance measures to group related samples). KMeans works by initially assigning each sample at random to one of 'K' clusters - each of which has a centroid. It then iteratively re-assigns samples to clusters and recomputes the cluster centroids with the objective of minimising the sum total distance between each sample and its assigned centroid. KMeans is very easy to implement, fast to execute and works well with relatively low dimensional data (of the top 10 w2v models, 6 of them have vectors of only 25 dimensions, the other 4 in the top 10 have 5 dimensions). The fact that it looks for patterns within the data itself and does not require separating the dataset into training and test sets makes it a good candidate for our purposes.\\ 

%MacQueen, J. (1967). "Some methods for classification and analysis of multivariate observations." Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, 1, 281-297. The foundational paper introducing K-Means clustering.

\Paragraph{\textbf{K-Nearest Neighbours}}
K-Nearest Neighbours is a very popular and simple distance-based Supervised model - i.e. it requires its training dataset to have known labels (or classes). These models simply store the training dataset and associated classes. When presented with an unseen sample, KNN finds its position in vector space and assigns it the class shared by the majority of the 'K' neighbours closest to it. In our case it would only work if the pfams for one clan were already clustered together - but that is what we are trying to establish.\\

\Paragraph{\textbf{DBScan}}
(Density-Based Spatial Clustering of Applications with Noise)
is a density-based clustering algorithm that groups points that are closely packed together and separates regions of lower density as noise. It is well-suited for multi-dimensional data where the clusters may not have a regular shape (e.g., elliptical or irregular) and is robust to noise and outliers. \\

%Advantages:
%%Can find arbitrarily shaped clusters.
%Resistant to noise and outliers.
%No need to pre-specify the number of clusters.
%Limitations:
%Struggles with varying densities.
%Performance may degrade in very high dimensions.
%Academic Reference:
%Ester, M., Kriegel, H. P., Sander, J., & Xu, X. (1996). "A density\-based algorithm for discovering clusters in large spatial databases with noise." KDD Proceedings, 226-231. This paper introduces DBSCAN and explains its effectiveness in clustering.
\Paragraph{\textbf{Gaussian Mixture Models (GMMs)}}
GMM's are probabilistic models that assume that all data points are generated from a mixture of a set number of Gaussian distributions with unknown parameters. They can be thought of as a generalised K-Means algorithm that incorporates information about the underlying covariance and means of the data in each cluster. GMMs typically use the Expectation Optimisation algorithm to determine the optimal covariance and means for each the clusters.

Similarly to KMeans, the success of GMMs is largely dependent upon the choice of the number of Gaussians to use as well as their initial covariance and means. The EM optimisation routine also makes them computationally intensive. However, they are more likely to be able to find more complex shaped clusters.\\

\Paragraph{\textbf{Principal Component Analysis}}
A useful pre-step in clustering is to perform Principal Component Analysis on the pfam vectors. PCA provides a useful way to reduce the dimensionality of data whilst maintaining as much of the variance as possible to allow the data to be separated. The advantage of PCA is that it can speed up traditional clustering techniques by vastly reducing the feature space, the PCA components can also be mapped onto a 2D space to provide visual clues as to the clusters that may emerge .

\subsection{Execution and analysis of clustering output}
The objective of clustering is to determine whether the vectors form natural groups that correlate with the pfam clan groupings. The approach adopted is summarised in figure\ref{fig:clustering}.\\
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/clustering.png}
    \caption{Clustering of pfam vectors and comparison with pfam clans}
    \label{fig:clustering}
\end{figure}


\paragraph{Data cleanse}
There were many clans that only had 1 pfam entry - this could skew the results so the related pfams were removed from the dataset prior to clustering. Additionally, it did not make sense to cluster pfam domains that mapped to a clan that had only one pfam entry. Thus these pfam domains were also removed.

\paragraph{Clustering} Clustering was undertaken using the scikit learn implementation of KMeans clustering algorithm. The hyperparameter for the number of clusters (k) was simply the number of clans that encompassed all the pfams remaining after data cleansing.

\paragraph{Interpretation of output} The clustering algorithm assigns a cluster id to each data sample item it is provided with. The next step is to check if the resulting clusters in anyway correlate with the actual clans for the pfam entries provided.\\
This is achieved by calculating, for each cluster from KMeans the jaccard index between it and each pfam clan.

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|c|}
    \hline
	metric & number of unique pfams & unique pfams with a clan count \\
	\hline
    pfams in w2v mc1 model & - & - \\
    pfams in r and rep & - & - \\
    \hline
\end{tabular}
\caption{Summary quantities of of pfam ids and clans}
\end{table}

Additionally, it did not make sense to cluster pfam domains that mapped to a clan that had only one pfam entry. Thus these pfam domains were also removed.\\


\section{Experiments and Analysis}
All the foundations are now in place to perform a set of experiments and subsequent analysis of the results with further experiements or modificatoins 




% ----------------------------------------------------------------
%             Chapter 3 - Experiments
% ----------------------------------------------------------------
\chapter{Experiments}
This dissertation makes the assumption that a word2vec model whose distance matrix is 'closest to' (has the highest correlation with) a provided \textbf{r\_and\_rep} distance matrix, has the best chance that its vector embeddings contain some biologically meaningful information that can be extracted from the embedding space. For the purposes of this dissertation, '\textbf{biologically meaningful}' is measured by how successfully a clustering algorithm, when provided with the word2vec embeddings from the 'best' model, finds clusters of Pfam domain words that align with real Pfam clans.\\

Thus the experiments consist of:

\begin{itemize}
    \item Iterating through models and for each, creating a distance matrix of its pairwise vector distances and comparing that with the r\_and\_rep distance matrix; then selecting the model with the closest correlation
    \item Running the \textbf{KMeans} algorithm against the vectors of that model and comparing the output K clusters to the Pfam clan groups
    \item Analysing the output to motivate the creation of other models with different configurations and test if they yield alternative results
\end{itemize}

\section{Corpus with minimum gap size of 1 and wide W2V hyperparameter search space}

\subsection{Distance comparison}
Finding the was achieved by performing both a pearson and spearman test as described in \ref{distancematrix}. The Mantel test was not used as the results were identical to either pearson or spearman, depending upon which variation of Mantel was used.

\paragraph{}The initial tests were conducted on the following model configurations:
\begin{table}[hbt!]
\centering
\label{gap1hyperparams}
\begin{tabular}{|l|l|}
	\hline
	metric & values  \\
	\hline
    corpus GAP size * & 1  \\
    model architecture & cbow and skipgram   \\
    model minimum word count & 1, 3, 5, 8   \\
    model window size & 3, 5, 8, 13, 21, 44   \\
    model vector size & 5, 10, 25, 50, 100   \\
	\hline
\end{tabular}
\caption{Model configurations for first round of experiments}
\end{table}

* The corpus gap size refers to how gaps in a protein sequence between tokens for pfam domains and tokens for disorder regions are added to a sentence in the corpus (using the word "GAP"). With a gap size of '1' any space between the start and end of consecutive tokens results in the word 'GAP' in the line of the corpus for that protein sequence. 

\begin{table}[hbt!]
\centering
\label{table_dist_results_pearson g1}
\begin{tabular}{|l|c|c|c|c|c|}
	\hline
	model type & min word count & window size & vector size & distance type & pearson \\
	\hline
	cbow & 8 & 13 & 5 & euc  & 0.0841 \\
    cbow & 8 & 44 & 5 & euc  & 0.0827 \\
    cbow & 8 & 21 & 5 & euc  & 0.0824 \\
    cbow & 5 & 44 & 5 & euc  & 0.0823 \\
    cbow & 8 & 8 & 5 & euc  & 0.0817 \\
    cbow & 5 & 8 & 5 & euc  & 0.0816 \\
    cbow & 5 & 21 & 5 & euc  & 0.0813 \\
    cbow & 8 & 5 & 5 & euc  & 0.0807 \\
    cbow & 3 & 13 & 5 & euc  & 0.0787 \\
	\hline
\end{tabular}
\caption{Initial experiments - Top 10 Distance Matrix correlations using Pearson}
\end{table}


\begin{table}[hbt!]
\centering
\label{table_dist_results_pearson g1}
\begin{tabular}{|l|c|c|c|c|c|}
	\hline
	model type & min word count & window size & vector size & distance type & spearman \\
	\hline
	skip & 8 & 44 & 25 & euc  & 0.0914 \\
    skip & 5 & 21 & 25 & euc  & 0.0894 \\
    skip & 8 & 21 & 25 & euc  & 0.0893 \\
    skip & 3 & 44 & 25 & euc  & 0.088 \\
    cbow & 8 & 13 & 5 & euc  & 0.088 \\
    skip & 8 & 13 & 25 & euc & 0.087 \\
    cbow & 8 & 21 & 5 & euc  & 0.087 \\
    cbow & 8 & 8 & 5 & euc  & 0.0865 \\
    cbow & 8 & 5 & 5 & euc  & 0.0863 \\
    cbow & 5 & 8 & 5 & euc  & 0.0862 \\
	\hline
\end{tabular}
\caption{Initial experiments - Top 10 Distance Matrix correlations using Pearson}
\end{table}
\pagebreak


\paragraph{Analysis}
These results are disappointing - there is very little correlation between the two distance matrices, the maximum correlation being \textbf{0.0914} using the spearman test against the skipgram w2v model (min word count 8, window size of 44 and vector size of 25).
\paragraph{} That said, some patterns emerge from these experiments that motivate further experiments later:
\begin{enumerate}
    \item There was no clear winner - with the exception of 1 model, all the test metrics were very close to each other - in the region between 8\% and 9\% correlation. The only model outside of that range had a statistic of 7.8\%, just under the others.
    \item None of the distance matrices in the top 10 were created using cosine distances
    \item The Pearson metric clearly preferred the cbow models whereas the spearman metric was a bit more mixed (although the top 4 were all skipgram models)
    \item Word2Vec models with a minimum word count of 8 dominate the results (12 instances) with a sprinkling of models with a word count of 5 (5 instances) and 2 with word counts of 3 and none with a word count of 1.
    \item Window sizes are more mixed, although the larger windows (13, 21, 44) feature more than the smaller window sizes (8, 5) in the top 5 results by each test statistic
    \item Regarding vector sizes, the Pearson metric clearly preferred the smallest vector size  of 5, spearman preferred 5 or 25; no metric favoured the larger 50 or 100 vector sizes
\end{enumerate}


\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
The original expectation was that there would be one clear winner and that would be taken forward to identify clusters. Instead, with no obvious candidate, the next step of the experiments were conducted using a selection of the models to cover different hyperparameters and model types.
\end{tcolorbox}


\subsection{Clustering of word2vec embeddings}
These set of experiments involve finding clusters of data within the embedding space and seeing if they correlate with the groups of Pfam domains (clans) that have been identified through research.

\paragraph{} Given the inconclusive results from the distance matrix comparison, it was decided to take the top models as measured by the distance metric as well as a selection of other models forward into the KMeans clustering experiments. In total, 10 different embeddings were tested from 10 different models:\\

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|lp{10cm}|lp{30cm}|}
	\hline
	\textbf{Reason} & \textbf{Model name (vectors to use for KMeans clustering)}\\
	\hline
	Highest overall pearson &  w2v\_20240911\_cbow\_mc8\_w13\_v5 \\
 \hline
    Highest overall spearman &  w2v\_20240910\_sg1\_mc8\_w44\_v25  \\
    \hline
    Highest cbow model type&  w2v\_20240911\_cbow\_mc8\_w13\_v5 	 \\
    \hline
    Highest skipgram model type& included above \\
    \hline
    Highest of each min word count &  w2v\_20240911\_sg1\_mc1\_w21\_v25,  w2v\_20240910\_sg1\_mc3\_w44\_v25, w2v\_20240911\_skip\_mc5\_w21\_v25, (mc8 already included)  \\
    \hline
    Highest of each window size &  w2v\_20240911\_cbow\_mc8\_w5\_v5, w2v\_20240911\_cbow\_mc8\_w8\_v5, (others already included) \\ 
    \hline
    Highest of each vector size &  w2v\_20240911\_cbow\_mc8\_w3\_v10,  w2v\_20240911\_skip\_mc5\_w44\_v50 ,  w2v\_20240910\_sg1\_mc8\_w44\_v100 \\
	\hline
\end{tabular}
\caption{A broad selection of models were included for clustering experiments due to the inconclusive results from the Distance Matrix comparison}
\end{table}

\subsubsection{KMeans data preparation}

\paragraph{Analysis of Clan sizes and selection of 'K' for KMeans}
As described above, a clan is a grouping of Pfam domains that have shared evolutionary origins, functional similarities, or structural features. Not all Pfam domains have yet been assigned to clans and some clans have a low number of Pfam domains. Thus some data preparation was required to ensure the clustering exercise was not impacted by clans that were too small.\\ 
\paragraph{}As a reference, the graphic below \ref{fig:clansizes} shows the sizes of the larger clan families that encompass the pfam domains within our word2vec models (using the model with the largest vocabulary - i.e. the hyperparameter 'minimum [word] count' of 1.). 


\begin{figure} [hbt!]
    \centering
    \includegraphics[width=1.0\linewidth]{images/clans.png}
    \caption{Count of pfams in each clan}
    \label{fig:clansizes}
\end{figure}

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|l|}
	\hline
	number of pfams per clan & number of clans \\
	\hline
	1 & 689 \\
    2 & 631 \\
    3 & 479 \\
    5 & 293 \\
    8 & 197 \\
    10 & 147 \\
	\hline
\end{tabular}
\caption{Number of clans according to different clan sizes (number of pfam entries in that clan)}
\end{table}

\newpage
\paragraph{\textbf{KMeans execution and results}}

One of the disadvantages of KMeans clustering, is that it has to be provided with a value for 'K' - the number of clusters to find. Given the objective is to find clusters of clans and that we know beforehand what those clans are, it was decided to provide the algorithm with different values of 'K' corresponding to different sizes of Pfam clans. Then, it was only provided with vectors that 'belonged' to one of those clans to test if it would create the correct clusters that aligned with those clans.

\paragraph{} In practise this meant setting a \textbf{minimum clan} size for each experiment (i.e. the minimum number of Pfam domains a Pfam clan must contain in order for those Pfam vectors to be included in the KMeans clustering) and then provide the corresponding vectors to the algorithm.

\paragraph{} The \textbf{Jaccard} similarity metric was used to gauge the ability of the clustering algorithm to create accurate clusters. It works by comparing the Pfam entries in each of the K clusters produced by the KMeans algorithms, with the Pfam entries in each of the Pfam clans and calculating the extent to which clusters overlap.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/kmeansprocess.png}
    \caption{Simplified KMeans Process - Reducing the number of Pfam vectors to cluster if  they are not part of a Pfam clan}
    \label{fig:enter-label}
\end{figure}

\todo{TODO: insert formula}\\
\todo{TODO: show graphic of where in space it puts the clusters v real cluster}

\paragraph{} Each of the top 10 models were run through a KMeans clustering algorithm using minimum clan sizes of 2, 10, 25, 50, 100, 150, 200 and 250, with the top results per minimum clan size show in table \ref{kmeansg1results}.


\begin{table}[hbt!]
\centering
\label{kmeansg1results}
\begin{tabular}{|l|c|c|c|}
\hline
	model & min clan size & K & Average Jaccard similarity with actual clan clusters\\
\hline

w2v\_20240910\_skip\_mc3\_w44\_v25\_g1  &  2  & 600  & 0.0005  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v25\_g1  &  10  & 113  & 0.003  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v25\_g1  &  25  & 47  & 0.0083  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v25\_g1  &  50  & 20  & 0.0208  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v25\_g1  &  100  & 7  & 0.0749  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v100\_g1  &  200  & 4  & 0.1427  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v100\_g1  &  250  & 2  & 0.3355 \\
\hline

\end{tabular}
\caption{Initial Experiments - KMeans clustering results for different clan sizes (thus different values of K}
\end{table}

As can be seen, as the minimum clan size criteria is increased from 2 to 250, the number of clans that meet that criteria reduces decrease from 600 to only 2. But even with only 2 clusters to compare, both with 250 or over pfams, the correlation is very low - at only 33\%.
 

\pagebreak

\subsubsection{Results Analysis}
Clearly the KMeans algorithm is struggling to identify clusters that map to Pfam clans. \\

So what is going on? Do certain model configurations provide better possibilities? Should different models be tried. To answer these questions and motivate further experiments, further analysis of the vector space was undertaken.
\paragraph{}
A clear visual explanation can be seen in the diagrams below. These plots use the embeddings from a number of models and plot their first 3 Principal Components. These are colour coded according to the correct clan each point belongs to. 

\paragraph{}The first plots in \ref{fig:0910g1kmbestc2} are of the 2 'best' models and use a minimum clan size of '2' - thus all clans are shown. It is visually clear that all the vectors are very tightly packed in space, making it difficult for distance-based clustering algorithms to separate them.

\begin{figure} [hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/0910g1_km_best_c2.png}
    \caption{First 3 Principal components for vectors of the 2 'best' models}
    \label{fig:0910g1kmbestc2}
\end{figure}

This behaviour is repeated through all the top 10 models. These plots are not shown to save space with one exception. As the vector size is increased, with all other parameters remaining the same, the skipgram models stretch out slightly more than the cbow models as per \ref{fig:0910g1kmv50c2}. This is easier to see in \ref{fig:0910g1kmv50c150} where only the 4 largest clans are plotted.

\begin{figure} [hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/0910g1_km_v50_c2.png}
    \caption{First 3 Principal components for cbow v skipgram w/ vector size of 50}
    \label{fig:0910g1kmv50c2}
\end{figure}

\begin{figure} [hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/0910g1_km_v50_c150.png}
    \caption{First 3 Principal components for cbow v skipgram w/ clan size of 1550}
    \label{fig:0910g1kmv50c150}
\end{figure}
\vspace{10mm}
\newpage

\section{Corpus with minimum gap size of 50 and higher dimension embeddings}

\subsection{Impact of corpus GAP distances and vector dimensions}
With the initial set of models failing to find clusters in the vector space, further experiments were undertaken. These focussed on the changing the makeup of the corpus (and in particular how GAP words are included in a sentence) and including larger vector sizes as hyperparameters to the word2vec models. \\

To execute this, new "corpi" were created - with an additional condition that the word "GAP" would only be inserted into a corpus sentence if the distance between the start and end of consecutive tokens (disorder or pfam domain) was greater than or equal to either 50 or 100. The purpose of this was to reduce the number of long sentences with many 'GAP' words which could potentially dominate the words we are really interested in (Pfam tokens). \\

Reducing the number of GAP words should, in theory, bring the Pfam words closer together in the vector space and help ensure the contextual relationships between them are captured within the window size hyperparameters. Increasing the vector size for the embedding space, may, in theory create more 'space' for clusters to be found.\\

The results are described below. \\

\subsubsection{Corpus GAP size 50}
This was models were created with a corpus with a minimum gap size of 50. Thus the word "GAP" is only inserted into the corpus if the distance between the start and end of a token (disorder or pfam domain) is greater than or equal to 50. \\

The other key difference between the initial models was to try larger vector sizes of 100, 250, 500. This was in an attempt to provide more 'space' for the model to create clusters. Note that the distance metrics were calculated only using euclidean measurement only (simply because no cosine distance measure featured in the top matrix distances in the initial models).\\

\paragraph{\textbf{Model Hyperparameters}}

\begin{table}[hbt!]
\centering
\label{gap50hyperparams}
\begin{tabular}{|l|l|}
	\hline
	metric & values  \\
	\hline
    corpus GAP size & 50  \\
    model architecture & cbow and skipgram   \\
    model minimum word count & 3, 5, 8   \\
    model window size & 13, 21, 44   \\
    model vector size & 5, 25, 50, 100, 250, 500   \\
	\hline
\end{tabular}
\caption{Model configurations created with a corpus with a minimum GAP size of 50}
\end{table}

\paragraph{\textbf{Distance Correlation Results}}

The results of this configuration are only marginally better than the default models using a gap size of 1. \\ \\
One difference is that the top 10 correlations via the pearson and spearman metrics are the same and in the same order. Also, all the top 10 models are of type skipgram, compared to the gap 1 models which have a mixture of cbow and skipgram architectures.\\ \\
Despite creating models with larger vector sizes (100, 250 and 500), there is a clear preference for a vector sizes of 25.

\begin{table}[hbt!]
\centering
\label{table_dist_results_pearson g1}
\begin{tabular}{|c|c|c|c|c|c|}
	\hline
	min word count & window size & vector size & pearson & spearman \\
	\hline
    8 & 44 & 25 & 0.0847 & 0.1092 \\
    8 & 21 & 25 & 0.082 & 0.1063  \\
    5 & 21 & 25 & 0.0812 & 0.1052  \\
    3 & 44 & 25 & 0.0817 & 0.1051  \\
    8 & 13 & 25 & 0.081 & 0.1051  \\
    5 & 44 & 25 & 0.0796 & 0.1033  \\
    3 & 13 & 25 & 0.0796 & 0.1032  \\
    3 & 21 & 25 & 0.0779 & 0.1018  \\
    5 & 13 & 25 & 0.0781 & 0.1017  \\
    8 & 44 & 50 & 0.0745 & 0.0984  \\
	\hline
\end{tabular}
\caption{Top 10 Distance Matrix correlations with minimum GAP distance in corpus of 50 spaces}
\end{table}








\begin{table}[hbt!]
\centering
\label{kmeansg50results}
\begin{tabular}{|l|c|c|c|}
\hline
	model & min clan size & K & Average Jaccard similarity with actual clan clusters\\
\hline

w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  2  & 565  & 0.0005 \\
w2v\_20240923\_skip\_mc8\_w21\_v25\_g50  &  50  & 20  & 0.0217 \\
w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  10  & 108  & 0.0032 \\
w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  25  & 44  & 0.0089 \\
w2v\_20240923\_skip\_mc8\_w21\_v25\_g50  &  50  & 20  & 0.0217 \\
w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  100  & 7  & 0.0735 \\
w2v\_20240923\_skip\_mc8\_w44\_v50\_g50  &  150  & 5  & 0.1098 \\
\hline
\hline
w2v\_20240923\_skip\_mc8\_w44\_v250\_g50  &  150  & 5  & 0.1012 \\
w2v\_20240923\_skip\_mc8\_w44\_v500\_g50  &  150  & 5  & 0.0686 \\
\hline
\end{tabular}
\caption{Experiments 2 - KMeans clustering results for different clan sizes (thus different values of K}
\end{table}

The top part of this results table shows the best correlations for different minimum clans sizes applied to the top 10 models. The second part shows 2 additional models that were included with larger vector embeddings of 250 and 500.

\paragraph{\textbf{Experiments 2 - Results Analysis}}





\clearpage
\section{Corpus with minimum gap size of 100}
The same hyperparameters were used in this case except that the model architecture was limited to skipgram only and an extra vector size of 1,000 was included.

\paragraph{\textbf{Model configurations}}
\begin{table}[h!]
\centering
\label{gap100hyperparams}
\begin{tabular}{|l|l|}
	\hline
	metric & values  \\
	\hline
    corpus GAP size & 100  \\
    model architecture & skipgram only  \\
    model minimum word count & 3, 5, 8   \\
    model window size & 13, 21, 44   \\
    model vector size & 5, 25, 50, 100, 250, 500 ,1000   \\
	\hline
\end{tabular}
\caption{Model configurations created with a corpus with a minimum GAP size of 100}
\end{table}

\paragraph{\textbf{Distance Correlation Results}}

\begin{table}[H]
\centering
\label{table_dist_results_pearson g1}
\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
	min word count & window size & vector size & pearson & spearman \\
	\hline
	8 & 44 & 25 & 0.0896 & 0.1126 \\
    8 & 21 & 25 & 0.0879 & 0.1107 \\
    5 & 44 & 25 & 0.0867 & 0.1104 \\
    3 & 44 & 25 & 0.0864 & 0.1095 \\
    5 & 21 & 25 & 0.0857 & 0.1083 \\
    \hline
    3 & 13 & 25 & 0.0843 & 0.1068 \\
    8 & 13 & 25 & 0.0843 & 0.1062 \\
    5 & 13 & 25 & 0.0836 & 0.1054 \\
    3 & 21 & 25 & 0.0833 & 0.1059 \\
    8 & 44 & 5 &  0.0744 & 0.0853 \\
	\hline
\end{tabular}
\caption{Top 10 Distance Matrix correlations with minimum GAP distance in corpus of 100 spaces *}
\end{table}

The results of this configuration provide the best of the experiments - but only marginally - the best correlation being 11.26\%. Again, the top 10 models using either the pearson or spearman metrics are the same and again, there is a clear preference for a vector sizes of 25 despite the addition of an even larger vector size of 1,000.

\paragraph{Clustering results}
For this clustering attempt, it was decided to deviate somewhat from the distance matrix 'Top 10' and explore other vector sizes. Thus the clustering used the only the top 5 models according to the spearman metric and also included models with vector sizes of 100, 250, 500 and 1,000 - each of which with a minimum word count of 8 and a widow size of 44.

Again, the top correlations of the KMeans clusters with the actual Pfam clans are shown below.

\begin{table}[hbt!]
\centering
\label{kmeansg100results}
\begin{tabular}{|l|c|c|c|}
\hline
	model & min clan size & K & Average Jaccard similarity with actual clan clusters\\
\hline
w2v\_20240922\_skip\_mc8\_w44\_v25\_g100  &  2  & 557  & 0.0006 \\
w2v\_20240922\_skip\_mc8\_w21\_v25\_g100  &  10  & 106  & 0.0032 \\
w2v\_20240922\_skip\_mc8\_w44\_v25\_g100  &  25  & 43  & 0.0087 \\
w2v\_20240922\_skip\_mc8\_w44\_v25\_g100  &  50  & 20  & 0.0216 \\
w2v\_20240922\_skip\_mc5\_w44\_v25\_g100  &  100  & 7  & 0.0705 \\
w2v\_20240922\_skip\_mc8\_w21\_v25\_g100  &  150  & 5  & 0.1075 \\
\hline
w2v\_20240922\_skip\_mc8\_w44\_v150\_g50  &  150  & 5  & 0.1001 \\
w2v\_20240922\_skip\_mc8\_w44\_v250\_g50  &  150  & 5  & 0.0984 \\
\hline
w2v\_20240922\_skip\_mc8\_w44\_v500\_g50  &  150  & 5  & 0.0868 \\
w2v\_20240922\_skip\_mc8\_w44\_v1000\_g50  &  150  & 5  & 0.042 \\
\end{tabular}
\caption{Experiments 3 - KMeans clustering results for different clan sizes (thus different values of K}
\end{table}

Yet again the results do not show any correlation. The top part f the table shows the best correlations across the different minimum clan sizes, the middle shows the same for larger vector sizes (150 and 250) and 500 and 1,000 whcih were introduced to this experiment.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/0922_g100_km_c2.png}
    \caption{3 Principal Components for various vector sizes with g100 config and a minimum clan size of 2}
    \label{fig:0922g100kmc2}
\end{figure}

To make it clearer, the following graphs show the clustering with a minimum clan size of 200 - which only includes 4 Pfam clans.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/0922g100_km_1_c150.png}
    \caption{Minimum cluster size of 150, vector sizes 5 and 25}
    \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/0922g100_km_2_c150.png}
    \caption{Minimum cluster size of 150, vector sizes 100 and 250}
    \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/0922_g100_km_3_c150.png}
    \caption{Minimum cluster size of 150, vector sizes 500 and 1000}
    \label{fig:enter-label}
\end{figure}


% ----------------------------------------------------------------
%             Chapter 4 - Analysis and Conclusiona
% ----------------------------------------------------------------
\chapter{Conclusions}

\section{Summary findings}
Throughout the course of this study, 78 million eukaryotic protein sequences, 300 million Pfam domains entries and over 4 billion lines of disorder information have been analysed and used to create 3 different corpi. By experimenting with a range of word2vec hyperparameters and architectures over 400 models have been created - each producing a unique encoding of Pfam word entries ranging in vector size from 5 to 1,000.

\paragraph{}Providing these vectors as inputs into a number KMeans clustering algorithm, no combination resulted in clusters that mirrored anything meaningful in a biological sense. The figures below that plot the 3 Primary Components of these vectors, colour coded according to their underlying Pfam clans, are representative of the findings. 

\paragraph{}As per figure \ref{fig:w2vclusterclan2vector100}, showing all Pfam clans with a minimum size of 2 or \ref{fig:w2vclusterclan150vector100} showing only those with a minimum of 150 entries for visibility, it is clear that the word2vec embeddings simply do not create vectors that can be easily separated in space. 
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.5\linewidth]{images/skip_clan2_v100.png}
    \caption{word2vec vector size of 100 - maximum number of Pfam clans shown}
    \label{fig:w2vclusterclan2vector100}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/skip_clan150_v100.png}
    \caption{word2vec vector size of 100 - showing only Pfam clans with more than 150 members}
    \label{fig:w2vclusterclan150vector100}
\end{figure}

\newpage
\section{Next Steps}
This study has exhausted the ability of word2vec to produce word vectors that reflect the grouping of Pfam domains within Pfam clans. Given the broad range of parameters used to configure the various models, it is unlikely that further investigations using this model type or approach to building a corpus will lead to different results.

\paragraph{}The word2vec model, although a game changer when it was introduced, has been superceded by Transformer based models when analysing the vast quantities of textual data. Protein Language Models based upon this architecture have gained significant traction in recent years and that is where current research momentum lies. 

\paragraph{}Going forward a recommendation from this report would be to investigate Transformer based encodings which have proven better at processing vast quantities of data and identifying long-range relationships between words. That might include:

\begin{itemize}
    \item One could possibly investigate the dom2vec approach, but that has been tried before (albeit on all proteins, not just eukaryotic one)
    \item More interesting would be to run a set of out of the box BERT models using the same corpi used here, extracting the encodings and again, passing them through a set of KMeans clustering algorithms
    \item Another approach would be to investigate a different corpus representation. In creating the corpus for this study, all pfam domain tokens were treated the same in the corpus - is this also true biologically or should more weight be given to tokens depending upon where they are placed along the polypeptide chain?
    \item And of course, a custom model could be created although that might also be a trial and error approach without somehow trying to understand what factors have influenced the embeddings created by word2vec
\end{itemize}



\newpage



\appendix


\begin{thebibliography}{Bibliography }

% ----- Background and Introductoin



% --------------- Pfam --------------- 
% Describes the Pfam database, including the concept of Pfam clans, and how they are used to group related protein families

\bibitem{pfam0}Sonnhammer, Erik LL and Eddy, Sean R and Durbin, Richard. Pfam: a comprehensive database of protein domain families based on seed alignments (1997). Proteins: Structure, Function, and Bioinformatics, vol. 28 No. 3, pp 405-420


\bibitem{pfam1}Punta M, Coggill PC, Eberhardt RY, Mistry J, Tate J, Boursnell C, Pang N, Forslund K, Ceric G, Clements J, Heger A, Holm L, Sonnhammer EL, Eddy SR, Bateman A, Finn RD. The Pfam protein families database. Nucleic Acids Res. 2012 Jan;40(Database issue):D290-301. doi: 10.1093/nar/gkr1065. Epub 2011 Nov 29. PMID: 22127870; PMCID: PMC3245129.


\bibitem{pfam2} Finn, Robert D and Bateman, Alex and Clements, Jody and Coggill, Penelope and Eberhardt, Ruth Y and Eddy, Sean R and Heger, Andreas and Hetherington, Kirstie and Holm, Liisa and Mistry, Jaina and others  (2014). "Pfam: the protein families database." Nucleic Acids Research, 42(D1), D222-D230. DOI: 10.1093/nar/gkr1065


\bibitem{pfamclan}Finn RD, Mistry J, Schuster-Böckler B, Griffiths-Jones S, Hollich V, Lassmann T, Moxon S, Marshall M, Khanna A, Durbin R, Eddy SR, Sonnhammer EL, Bateman A. Pfam: clans, web tools and services. Nucleic Acids Res. 2006 Jan 1;34(Database issue):D247-51. doi: 10.1093/nar/gkj149. PMID: 16381856; PMCID: PMC1347511.

% An introduction to Hidden Markov Models used in Pfam, which are central to identifying clans
\bibitem{pfamhmm} Finn, R. D., Clements, J., \& Eddy, S. R. (2011). "HMMER web server: interactive sequence similarity searching." Nucleic Acids Research, 39(suppl\_2), W29-W37. DOI: 10.1093\/nar\/gkr367






% --------------- Domains --------------- 

\bibitem{introprotdomain1}Moore, A.D., Bjorklund,  A.K., Ekman, D., Bornberg\-Bauer, E., Elofsson, A.: Arrangements in the modular evolution of proteins. Trends in Biochemical Sciences 33(9), 444–451 (2008)

\bibitem{introprotdomain2}Forslund, S.K., Kaduk, M., Sonnhammer, E.L.: Evolution of protein domain architectures, 469–504 (2019)

\bibitem{introprotdomain3}Das, S. and C.A. Orengo, Protein function annotation using protein domain family resources. Methods, 2015.

\bibitem{introprotdomain4}Nepomnyachiy, S., N. Ben-Tal, and R. Kolodny, Complex evolutionary footprints revealed in an analysis of reused protein segments of diverse lengths. Proc Natl Acad Sci U S A, 2017. 114(44): p. 11703-11708.

% --------------- Disordered regions --------------- 
\bibitem{introdisordered}Romero, P. et al. (1998) Thousands of proteins likely to have long disordered regions. Pac. Symp. Biocomput. 1998, 437–448

% ------------- LLMS  ------------- 

% good summary of various techniques
\bibitem{llmfuncprot}Unsal, S., Atas, H., Albayrak, M. et al. Learning functional properties of proteins with language models. Nat Mach Intell 4, 227–245 (2022). %https://doi.org/10.1038/s42256-022-00457-9


% word2vec mikolov
\bibitem{word2vecoriginal}Mikolov, T., Chen, K., Corrado, G.S., \& Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. International Conference on Learning Representations.

% doc2vec
\bibitem{doc2vec}Le Q., Mikolov T. (2014) Distributed representations of sentences and documents. Int. Conf. Mach. Learn. ICML 2014, 32, 1188–1196.

% transformer
\bibitem{transformer} Vaswani A. et al. Attention Is All You Need (2013) %https://arxiv.org/abs/1706.03762


% ------------- PLMs  General ------------- 

\bibitem{plmrives2019}Rives, A., et al. (2019). “Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences.”


\bibitem{plmtransfer} Heinzinger M, Elnaggar A, Wang Y, Dallago C, Nechaev D, Matthes F, Rost B. Modeling aspects of the language of life through transfer-learning protein sequences. BMC Bioinformatics. 2019 Dec 17;20(1):723. doi: 10.1186/s12859-019-3220-8. PMID: 31847804; PMCID: PMC6918593.

\bibitem{plmheinzingerElmo2019} Michael Heinzinger, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes, Burkhard Rost. Modeling the language of life – Deep Learning Protein Sequences. 2019


% ------------- PLMs Key models ------------- 

\bibitem{plmtape} Rao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, P., Canny, J., Abbeel, P., \& Song, Y. (2019). Evaluating Protein Transfer Learning with TAPE. Advances in Neural Information Processing Systems, 32.

\bibitem{plmesm}Rives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, Guo D, Ott M, Zitnick CL, Ma J, Fergus R. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc Natl Acad Sci U S A. 2021 Apr 13;118(15):e2016239118. doi: 10.1073/pnas.2016239118. PMID: 33876751; PMCID: PMC8053943.

% Prot Trans
\bibitem{plmprottrans} Elnaggar, A. et al. (2022) ProtTrans: towards cracking the lan- guage of lifes code through self-supervised deep learning and high performance computing. IEEE Trans. Pattern Anal. Mach. Intell. 44, 7112–7127

\bibitem{bert} Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2019). BERT: Pre\-training of Deep Bidirectional Transformers for Language Understanding. \textit{arXiv preprint arXiv:1810.04805}.

\bibitem{t5} Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... \& Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. \textit{Journal of Machine Learning Research}, 21(140), 1-67.


\bibitem{albert} Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., \& Soricut, R. (2020). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. \textit{arXiv preprint arXiv:1909.11942}.

\bibitem{electra} Clark, K., Luong, M. T., Le, Q. V., \& Manning, C. D. (2020). ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. \textit{arXiv preprint arXiv:2003.10555}.


% Prot Bert
\bibitem{protbert}Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial, ProteinBERT: a universal deep-learning model of protein sequence and function, Bioinformatics, Volume 38, Issue 8, March 2022, Pages 2102–2110



% ---------------- latest thinking


% from db - domain to vec
\bibitem{dom2vec}Damianos P. Melidis, Brandon Malone, Wolfgang Nejdi, dom2vec: Capturing domain structure and function using self-supervision on protein domain architectures


\bibitem{llmdom2vec}Melidis at al. dom2vec: Capturing domain structure and function using self-supervision on protein domain architectures.

\bibitem{llmword2vec}Buchan, D.W., Jones, D.T. Learning a functional grammar of protein domains using natural language word embedding techniques. Proteins: Structure, Function, and Bioinformatics 88(4), 616-624 (2020)


% ---------------- stats
\bibitem{statmantel} Mantel, N. (1967). "The detection of disease clustering and a generalized regression approach." Cancer Research, 27(2), 209-220.

\bibitem{statpearson} Pearson, K. (1895). "Note on regression and inheritance in the case of two parents." Proceedings of the Royal Society of London, 58, 240-242.

\bibitem{statspearman} Spearman, C. (1904). "The proof and measurement of association between two things." The American Journal of Psychology, 15(1), 72-101.



% do i reference these?
\bibitem{x} Mikolov, T., Sutskever, I., Chen, K., Corrado, G., \& Dean, J. (2013). \textit{Distributed Representations of Words and Phrases and their Compositionality}. Advances in Neural Information Processing Systems (NIPS), 3111-3119. Retrieved from \url{https://arxiv.org/abs/1310.4546}.
    
\bibitem{y} Firth, J. R. (1957). \textit{A synopsis of linguistic theory 1930–1955}. Studies in Linguistic Analysis. Reprinted in Palmer, F. (ed.), 1968. Selected papers of J. R. Firth 1952-1959. London: Longman.
    
\bibitem{z}  Goldberg, Y., \& Levy, O. (2014). \textit{word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method}. arXiv preprint arXiv:1402.3722. Retrieved from \url{https://arxiv.org/abs/1402.3722}.




% -------- References to databases and pfam etc



\end{thebibliography}

\chapter{Github code - location and organisation}
The code for this project is freely available on \href{https://github.com/greyneuron/COMP_0158_MSC_PROJECT/tree/main}{github}. It is organised as follows at the top level:

\begin{itemize}
    \item \textbf{code} contains all python code, shell scripts and one or two C++ file organised by functional area
    \item \textbf{data} contains some sample data, restructed due to github limitations on sizes
    \item \textbf{logs} contains outputs logs from the various
    \item \textbf{database} contains the duckdb local database to hold the output of the data preparation exercises (not uploaded due to github restrictions)
\end{itemize}

%The folders beneath \textbf{code} and \textbf{data} have a mirrored structure and are (hopefully) self-explanatory

\begin{itemize}
    \item \textbf{data\_prep} contains all code used to download and parse raw data into tab delimited files
    \item \textbf{corpus} contains the code to create the corpus files
    \item \textbf{model} contains the code to create the models themselvves and run through the various hyperparameter combinations
    \item \textbf{distance} contains the code to compare the word2vec distances with the rand\_rep distance matrices
    \item \textbf{clustering} contains the code to run and analyse the outputs of KMeans cllustering algorithms
    \item \textbf{terraform} contains Terraform ccritps to create environments on AWS including networks, securoty groups, EC2 compute instances, EBS storage, and database instances
\end{itemize}

Note that within the various sub directories of the code folder, there will often be some Jupyter notebooks - called ***\_helper.ipynb. These were used as sandbox areas to quickly try out code. These are useful for testing, but once working, the code was transferred into regular python files within the same directory. 





% ---------------- my document ----------------

\end{document}
