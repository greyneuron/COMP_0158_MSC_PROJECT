Sat July 20th
- Tried mysql locally but wsa taking ages
- At 4pm started a scrpt to create pre-corpus but it is taking 1hr to parse 1M - kept it running
- Went to Noel Gallagher concert

Sun July 21st
- Pre-corpus script painfully slow
- Loaded all dat files into S3 to see if I could use Glue (also loaded into EBS)
- Got frustrated with that after a while as too much to learn and not convinced it would be fast enough or cheap enough
- Resorted back to an RDS instance but had difficulty loading the data from my Terraform instance
- Resorted to creating an RDS instance from the console and after some tinkering managed to get it to load my local files into W2V_TOKEN and W2V_PROTEIN
- Loaded all sata into these tables and applied indices - showed that 1M proteins could be executed very quickly
- Close of play:
    # - New rds instance created and db created called W2V with user 'admin' and password 'w0rd2v3c'
    # - Created W2V_TOKEN table and uploaded all pfam and disorder tokens
    # - Created an index on that table
    # - Loaded all protein data into W2V_PROTEIN - byut there are duplicates



# --------------------------------------------------------------------------------

scp plowry@knuckles.cs.ucl.ac.uk:/cs/student/msc/dsml/2023/plowry/profile_study/distance_matrix/rand_rep_distance_matrix/pfam_rep_distance_matrix.npy .


# ssh to ecs
export dns="ec2-63-32-44-188.eu-west-1.compute.amazonaws.com"
ssh -i "w2v_rsa" ec2-user@$dns

# after restart of ec2
lsblk
sudo mkdir /data
sudo mount /dev/nvme1n1 /data
cd /data/dev/ucl
. w2venv/bin/activate

sudo dnf update -y
sudo dnf install mariadb105

. w2venv/bin/activate

# to activate venv
cd /data/dev/ucl
. w2venv/bin/activate

# to execute code (activate venv first)
export dns=
scp -i "w2v_rsa" ~/dev/ucl/comp0158_mscproject/code/mysql_tools.py ec2-user@$dns:/data/dev/ucl/code
cd code

# mysql from ec2 ssh
export endpoint="w2v-dev-db.cligs4ak0dtg.eu-west-1.rds.amazonaws.com"
mysql -h $endpoint -P 3306 -u w2v -p

# --------------------------------------------------------------------------------
                    Week commencing Monday July 22nd
*       Friday Target: Have a distance matrix for each pfam entry
# --------------------------------------------------------------------------------

* Tasks
1. Create pre-corpus
    - For each protein query the database
2. Create corpus
3. Run word2vec on corpus
4. Get unique pfam entries
5. For each pfam entry get its vector representation from word2vec and store it
6. Determine distances between each pfam entry

day 1 target: Create corpus by end of day (or at least pre-corpus)



# --------------------------------------------------------------------------------
Monday 22 July

- Created new rds instance from snapshot on 21 July, made it publicly available
- Had to add security groups again via the console so that I could access from ec2

- Tried running from laptop to RDS:
    - Loaded mysql client into conda on laptop 0 but couldn;t connect via laptop
    - Also tried adding a new securoty group such as in this way: https://stackoverflow.com/questions/37212945/aws-cant-connect-to-rds-database-from-my-machine
      that didn't work

- Resorted to running python code from EC2
    - Develped code on laptop
    - SSH to EC2 and create a venv then pip3 install mysqlclient

        # created venv again on ec2
        python3 -m venv w2venv
        source w2venv/bin/activate
        pip3 install mysql-connector-python

    - SCP code from laptop to ec2
        cd to terraform/w2v_ec2 (thats where the key is)
        get dns of ec2 instance
        export dns=<dns of ec2>
        assumes already have setup ssh
        scp -i "w2v_rsa" ~/dev/ucl/comp0158_mscproject/code/mysql_tools.py ec2-user@$dns:/data/dev/ucl/code

    - Ran mysql_tools.py in batches of 500,000 - see pre_corpus_fill_log_20240722.log
    - Renamed those files 1 to 9 to have extension 01 to 09
    - Also ran query to find unique pfam entries (also in mysql_tools.py) [20,725 unique pfams]

    - Tar'd up the files
        - cd to precorpus
        - tar -czvf archive_name.tar.gz precorpus.tar.gz

    - Copied them and the unique pfam entires to s3
        - aws configure
        - aws s3 cp precorpus.tar.gz s3://w2v-bucket/corpus/precorpus.tar.gz

        - aws s3 cp unique_corpus_pfam_20240722.dat s3://w2v-bucket/pfam/unique_corpus_pfam_20240722.dat

    - Created RDS Snapshot RDS
    - Stopped EC2
    - Stopped RDS TODO

# --------------------------------------------------------------------------------
Tuesday 23 July

Noticed that I had left out the protein start and end lengths - took all day to fis due to weird behaviour of query

Attempt 1 - Direct connect from macbook to RDS
- Restarted RDS from console and made it publisly available - just selected all the defaults
- The added new route to vpc sec groups to 0.0.0.0/0
- Was then able to connect from laptop - but connection kept dropping

Attempt 2 - Go back to EC2 connecting to RDS
- Restarted again from snapshot
- Selected w2v-security-group but left all others as is
- From EC2 this worked: 
    % mysql -h $endpoint -P 3306 -u admin -p

Changed query to inculde protein start and end but it struggled on db.t3g.xlarge

- Restarted with a larger db, but honestly it amde no difference
- Eventually settled on a modified version of Mondasy's code (precorpus_v2()) but with the benefit that this combines everything into one line per protein
- Running with a chink size of 250,000 proteins it was taking at least 2mins per chunk = 8min for 1M = 10 hours for 78M!
- Will need 312 files of output

# ALTERNATIVE - USE DIRECT SQL OUTPUT AND THE TIDY IT UP WITH AWK
- SSH to ec2
- create a folder and give it write access
- the following will write 1M results to a separate file
mysql -h $endpoint -P 3306 -u admin W2V -p -e 'SELECT W2V_PROTEIN.*, W2V_TOKEN.* FROM ( SELECT UNIPROT_ID, START, END FROM W2V_PROTEIN W2V_PROTEIN ORDER BY UNIPROT_ID LIMIT 0, 1000000) AS W2V_PROTEIN INNER JOIN W2V_TOKEN AS W2V_TOKEN ON W2V_PROTEIN.UNIPROT_ID = W2V_TOKEN.UNIPROT_ID' > sql/sql_output_0_1M.txt

cat sql_output_0_1M.txt | awk '{FS ="\t"} {print $1 ":" $2 ":" $3 "|" $5 ":" $6 ":" $7 ":" $8}'

# Close of play
- Have left create_pre_corpus_v2 runnig this evening from about 1830 - taking 2.5min for 250k proteins - 10min per 1M proteins = 13 hours
- Used the following:
    
    chunk_size      = 250000
    num_iterations  = 320
    iteration       = 1
    result          = 0
    start           = 0

created a new file for each iteration

- 10M target = 40 x 2.5min = 1hr 3/4 = ideally by 8pm get to v2_40
    18:48:30    : v2_12
    20:26:30      : v2_44 = 1hr 40 for 32 iterations

- Stopped on 90
- Need to redo number 1

 # --------------------------------------------------------------------------------
Wed 24 July

Dublin


# --------------------------------------------------------------------------------
Thurs 25 July - Nightmare day

- Summary: Simply tried to redo what I'd done on Tuesday and finish it (as I had stopped after onlu 90
  of 360 iterations - because it was going to take 12hours and Ihad to get up at 0600 Wed to go to Dublin)
  Anyway, when I retried it was SOOO slow and I began to lose confidence in this approach. So I made the 
  decision to just use the SQL script direct from a MySQL connection (this is the one that does a join between
  W2V_PROEIN and W2V_TOKEN and which I worked out how to do in chunks and output to a file). The positive side to this
  approach is that the sql is much faster to execute, but the if you want to output to a file on your clien you cannot 
  format it.

 mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c -e "SELECT W2V_PROTEIN.*, W2V_TOKEN.* FROM ( SELECT UNIPROT_ID, START, END FROM W2V_PROTEIN W2V_PROTEIN ORDER BY UNIPROT_ID LIMIT ${start_pos}, ${chunk_size}) AS W2V_PROTEIN INNER JOIN W2V_TOKEN AS W2V_TOKEN ON W2V_PROTEIN.UNIPROT_ID = W2V_TOKEN.UNIPROT_ID" >> output/sql_output_20M_${i}.txt

Nevertheless I realised that the ouput is tab delimted so was realised I could use awk to modify it into a tab
delimited .dat file:

cat ${file} | awk '{FS ="\t"} {if (!($1~/^UNIPROT/)) print $1 ":" $2 ":" $3 "|" $5 ":" $6 ":" $7 ":" $8}'

Although the database was slow, I eventually found a process that worked and started to execute the sql in chunks of 250k - this seemed ot work but it had taken me the whole day to work out
how to do it and I was pretty stressed as every step forward takes ages. I did this for about 20M proteins and called it a day, reasonably happy that I could pick it up again on Friday.

I went for a pint to destress

-------------


- Tried to restart DB form snapshot with local access but couldn;t add 0.0.0.0/0 due to conflict
- There are loads of child subnets and securoty groups already
- decided to delete and recreate the vpc!
- done

# restore snapshot rds from console
- w2v-db-1

- db.t4g.medium (2 CPU 4 GiB)  (could this be too small - previoulsy used db.t4g.xlarge : 4 cpu 16Gb)

- Single instance
- vpc: used default vpc
- vpc sec group: use existing (which is default)
- make public
- eu-west-1a

- didn't have to do this next bit for some reason as rule was already there
- once says 'backing up' go to db instance and select vpc-sec-group
- select the sec group add new inbound for 0.0.0.0/0 on 3306
- copy endpoint

- db slow - not enough RAM?

brew install mysql-client
/usr/local/opt/mysql-client/bin/mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c

echo endpoint=""
/usr/local/opt/mysql-client/bin/mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c


Was using t3.xlarge as opposed to 


db.t4g.xlarge is haning again - especially on 221000 proteins
runs fast even from local pythin up to that point
locally it did 100k proteins in 27s


start_pos=0
chunk_size=2

for i in {0..5}
do
    echo "starting at" $start_pos
    # works
    #/usr/local/opt/mysql-client/bin/mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c -e "SELECT * FROM W2V_PROTEIN ORDER BY UNIPROT_ID LIMIT ${start_pos}, ${chunk_size}"

    # also works (hooray)
    /usr/local/opt/mysql-client/bin/mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c -e "SELECT W2V_PROTEIN.*, W2V_TOKEN.* FROM ( SELECT UNIPROT_ID, START, END FROM W2V_PROTEIN W2V_PROTEIN ORDER BY UNIPROT_ID LIMIT ${start_pos}, ${chunk_size}) AS W2V_PROTEIN INNER JOIN W2V_TOKEN AS W2V_TOKEN ON W2V_PROTEIN.UNIPROT_ID = W2V_TOKEN.UNIPROT_ID"

    start_pos=$((start_pos + chunk_size))
done


going to revert to the shell script above, querying a db.t4g.large

1. Start the rds on db.t4g.large
2. Change the endpoint variable in extract_tokens_from_db.sh
3. Change the params in extract_tokens_from_db.sh
4. Run extract_tokens_from_db.sh
5. Convert tokens to a single line per protein

# --------------------------------------------------------------------------------
Friday 27th July

CONTINUE WITH DB EXTRACT APPROACH

SUMMARY: I picked up where I'd left off on Thursday evening, all I ha to do was write a script to execute the sql in chunks, rather than me having to watch the command line. I also made some tests to check that the script was working as expected - for example, the very first protein A0A0A10PZJ8 should have 4 tokens (3 disorded and 1 pfam) - which it did! Hooray

I then had the usual issues with dtabase and ec2 instances that seem to sometimes be better than others. I initialy started to do batches of 10M proteins in chuinks of 250k - thus resulting in 40 output files per batch (10M / 250k = 40). This was taking 3 1/2 mins per query. I tried it with a chunk size of 500k and this was also taking 3 1/2 mins - so I went with that. I started in earnest at about 1pm and it was taking an hour for each block of 10M. Hoever the first 2 blocks stalled and i had to restar which was a faff. By the end of the day I had done all 7 blocks of 10M, backed up my EBS and left it at that.


- Updated extract_tokens_from_db.sh to loop
- Restarted DB used db.m6 or somethinng > it was quite slow
- Restarted DB used db.t4g.2xlarge > not sure it really makes a difference
- Used t3.xlarge EC2 whch seemed fine (4 CPU 16G - $0.182 per hour)

Have 0 - 10,000,000 from Thursday
Started at 12,000,000 in 250k chunks - but set iterator to 9 instead of 

Want to do batches of 10M in chunks of 250k - thus 40 iterations

Noticed that batch size of 250k takes 3min - but so does batch size of 500k

Finished 10M - 20M but there are different batch sizes so that are not 20 files

10:50 - Started 20M to 30M in batches of 500k over 0 .. 9 iterations : each is taking about 3min 30 +> 10M will take about 38min
12:49 - Started 30M to 40M in batches of 500k again about 3'30 per 500k

Also created/modified
- convert_db_tokens_dat.sh to convert the raw sql output into dat fils
- combine_db_dat_tokens.py which will create one long line


# --------------------------------------------------------------------------------
Sat 28th July

Today is about getting my sql output into a set of dat fioles with one protein per line

I noticed some issue with the 20M_30M block from Friday so started RDS reran that and then killed RDS

All 500k sql txt output files are now in directores 00M_10M 10M_20M
Modified my script to convert these to propoer dat files and moved them to a separate folder:

/data/dev/ucl/data/precorpus_token_dat

I tar'd these up and copied them to s3

aws s3 cp /data/dev/ucl/data/precorpus_token_dat.tar.gz s3://w2v-bucket/corpus/precorpus_token_dat.tar.gz

Then I downloaded these to my laptop asI can convert them relatively quickly to a precorpus and then a corpus!

Example pre-corpus
First section has the uniprot id : start : end : num pfam tokens : num disorder tokens
Subsequent sections are either for pfam or disorder entries giving the start and end poisitions plus the PFAM token


A0A010PZJ8:1:494:1:3|DISORDER:1:30|DISORDER:1:32|DISORDER:468:493|PF01399:335:416
A0A010PZN2:1:354:0:4|DISORDER:225:353|DISORDER:227:304|DISORDER:307:325|DISORDER:326:353


# --------------------------------------------------------------------------------
Mon 29th July

IMproved creation of moel from sentences
Wrote script to buils sentences from corpus
Write script to get pfam encodings from model

Tues 30th July
- Revision for Stats and Intro to ML

Wed 31st July
- Revision for Stats (AM)
- Started write up (PM)
- Started testing code again - especially dat prep whilst still fresh


Thurs 1st Aug
- Tidied up code
- Re-ran protein load against UNiRef100 but didn;t load into db as its not all eukaryotic
- Started taxonomy work

Friday 2nd Aug
- Parsed names, categories and nodes from taxonomy files
- Loaded names and categories into DuckDB and added indices (duckdb_dat_loader.ipynb)

- TODO: Write python script to traverse a tree

Saturday
- NOticed that UniRef100 download of eukaryotic had UiRef100 entries!
- Re-downloaded but select '100%' and 2759


Sunday
NB: 
- W2V_TOKEN has all pfam entries from protein2ipr
- W2V_PROTEIN has all eukaryotic proteins from uniprot in TrEMBL format - this excludes taxonomy
- W2V_PROTEIN_UREF100_E has eukaryotic proteins in UniRef100 format including taxonomy

- precorpus.dat was created from the join of W2V_PROTEIN and W2V_TOKEN so should only have eukaryotic proteins

Monday August 5th
- Created a number of differnet models but noticed that when I tried to encode the pfam ids OI had loads of pfam ids
  that had no encoding. THis is becuase the pfam tokens were taken directly from protein2ipr.day which has all proteins
  in it, but my initial list of pfam ids was created from this list. BY contrast, the corpus was created from a join across 
  W2V_PROTEIN and W2V_TOKEN thus the corpus already has eukaryotic proteins only but my list of unique pfam ids did not.
  
- I decided to create a new pfam entry list by selecting only pfam tokens in W2V_TOKEN that had an entry W2V_PROTEIN (actually
  I used W2V_PROTEIN_UREF100_E from my Sunday download)

- ** IMPORTANT ** It is SOOO much quicker t put a count column on the protein table rather than using OFFSET and LIMIT which 
  appears to parse the whole table each time. THus I re-ran in the UNiref100 fasta file but added a counter field. I relaoded 
  this into the W2V_PROTEIN_UREF100_E table and then generated a list of pfam ids and the sorted it and made it unique


Wed August 7th
- Met with Daniel (I was in office, he was online)
- Downloaded evo files

Thurs Aug 8th - Friday 9th
- Decided to review all code up to this point as don;t want to start on analysis with issues
- Decided to load all UNiref100 proteins as opposed to TrEMBL as really want as much data as possible

Sat 10th, Sun 10th
- Ran a number of models and noticed really low vocab numbers - did a lot of analysis, tracing pfam ids all the way through
  from protein2ipr through to my token file, through to the corpus
- Found a small bug in corpurs creation but fixed it
- Reason for small vocabs was due to the min_count parameter for word2vec which removes a lot of words
- Reran a number of models with varying min_count sizes

Mon 11th, Tues 12th, Wed 14th (am)
- Wrote and debugged mantel test
- Tried to run on DB' matrix but fond an issue - there is a defect that he is fixing and needs to rerun his matrices
- Also found issue with my mantel test - I was not reordering the rows and columns properly to reflect the source matrix. fixed wed am
- DB has created a smal subset (as of Tue 12th pm) - have downloaded and will test with them


ssh -l plowry knuckles.cs.ucl.ac.uk
cd /cs/research/bioinf/dbgroup/profile_study/

# get data from knuckles to local

scp plowry@knuckles.cs.ucl.ac.uk:/cs/student/msc/dsml/2023/plowry/rand_rep_distance_matrix.npy .

# put data from local to knuckles
cd to local directory

scp mycode.py plowry@knuckles.cs.ucl.ac.uk

# -----------------
TODO

Graph the embeddings (see bootom of this: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html )

Word2Vec parameters: https://radimrehurek.com/gensim/models/word2vec.html

sg = 0 : bag of words (this is the default)
sg = 1 : skipgram

--------
HCP
--------

# Log on to gateway
ssh -l plowry knuckles.cs.ucl.ac.uk --> works
ssh plowry@morecambe.cs.ucl.ac.uk   --> works

scp model_task_manager.py plowry@knuckles.cs.ucl.ac.uk:/cs/student/msc/dsml/2023/plowry/word2vec/code
scp model_tasks.py plowry@knuckles.cs.ucl.ac.uk:/cs/student/msc/dsml/2023/plowry/word2vec/code

cd to data/corpus

scp uniref100_e_corpus_20240810.txt plowry@knuckles.cs.ucl.ac.uk:/cs/student/msc/dsml/2023/plowry/word2vec/data/corpus


qrsh -l tmem=5G, h_vmem=5G, h_rt=3600

-------------------------------
Sat 31st August - Mon 2nd Sep
-------------------------------
- Gave up on HPC as memory requirements mean ti was taking too long to schedule - reverted to AWS instead
- Created 4 AWS EC2 instances and 4 separate EBS (tried to share EBS originally but it seemed to be slowing things down)

- See 
    code/terraform/hpc - created 4 instances
    code/hpc : code to execute on AWS

- See code/terraform/Readme.txt to see how I set these up

- ONce models and distances created:

# copy models to s3
cd /word2vec/models
aws configure

find . -name "*model" 
find . -name "*model" | wc -l

find . -name "*model" | xargs zip models_aws04_mc8.zip
aws s3 cp models_aws04_mc8.zip s3://w2vmodels/models_aws04_mc8.zip
find . -name "*model" -delete


--------------------------
# get data from knuckles to local

scp plowry@knuckles.cs.ucl.ac.uk:/cs/student/msc/dsml/2023/plowry/rand_rep_distance_matrix.npy .

# find files and move to a directory
find skip_mc1 -name "*npy" -exec mv {} eucdist_mc1 \;

# list files in a directory and count them
for i in 3 5 8 13 21; do echo $i; find . -name "*mc8*w$i*npy" | wc -l; find . -name "*mc5*w$i*model"; done

for j in 1 3 5 8; do for i in 3 5 8 13 21; do echo mc_$j w$i; find skip_mc$j -name "*mc$j*w$i*model" | wc -l; done; done

for j in 1 3 5 8; do for i in 3 5 8 13 21; do echo $i $j; done; done

# individually zip up all files names *.npy
find . -name "*npy" -exec gzip {} \;

find . -name "*model" -exec gzip {} \;

find . -name "*gz" -exec gzip -d -v {} \;
