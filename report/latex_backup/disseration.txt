\documentclass{report}
\usepackage{setspace}
%\usepackage{subfigure}

\pagestyle{plain}
\usepackage{amssymb,graphicx,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}

\newtheorem{theorem}{THEOREM}
\newtheorem{lemma}[theorem]{LEMMA}
\newtheorem{corollary}[theorem]{COROLLARY}
\newtheorem{proposition}[theorem]{PROPOSITION}
\newtheorem{remark}[theorem]{REMARK}
\newtheorem{definition}[theorem]{DEFINITION}
\newtheorem{fact}[theorem]{FACT}

\newtheorem{problem}[theorem]{PROBLEM}
\newtheorem{exercise}[theorem]{EXERCISE}
\def \set#1{\{#1\} }

\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}



\newcommand{\nats}{\mbox{\( \mathbb N \)}}
\newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}


% ---------------- my additions ----------------
% xml format
\newcommand*{\xml}[1]{\texttt{<#1>}}
\usepackage{tcolorbox}
\usepackage{hyperref}



%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{  	{ \includegraphics[scale=.5]{images/ucl_logo.png}}\\
\vspace{5mm}
{{\Huge Can useful biological information be found within Word2Vec embeddings of protein sequences?}}\\
{\large Optional Subtitle}\\
		}
\date{Submission date: Day Month Year}
\author{Patrick Lowry\thanks{
{\bf Disclaimer:}
This report is submitted as part requirement for the MSc in  Data Science and Machine Learning at UCL. It is
substantially the result of my own work except where explicitly indicated in the text.
\emph{Either:} The report may be freely copied and distributed provided the source is explicitly acknowledged
\newline  %% \\ screws it up
\emph{Or:}\newline
The report will be distributed to the internal and external examiners, but thereafter may not be copied or distributed except with permission from the author.}
\\ \\
MSc Data Science and Machine Learning\\ \\
Daniel Buchan}



\begin{document}
 
 \onehalfspacing
\maketitle
\begin{abstract}
Language models have become household names in the last couple of years. They rely one way or another upon turning vast quantities of textual data, called a corpus,  into some form of numeric encoding - or embedding. These embeddings take the form of multi-dimensional vectors - one vector embedding exists for each word in the corpus. In this way 'distances' between words can be calculated within the vector space and used, for example, to try and predict the word 'closest' to the last word that has been typed.
\paragraph{}
Proteins can also be viewed as a type of sentence. Individual proteins are 'simply' made up of long, varying-length chains of amino-acids. Through years of painstaking lab work, scientists have identified regions within these chains that perform a unique funtion (many sequences provide no apparent function at all and are leftovers from years of evolution). With some variations, these sequences proivde similar functions across species and are grouped into families, called 'protein familiies' (or pfams). Each has been given a unique name, called a pfam entry. 
\paragraph{}
Thus, a protein can be viewed as consisting of a number of 'gap' areas that perform no obvious function and a number of 'functional' areas, each with a unique pfam identifier. Looking at it another way, each protein can be viewed as a 'sentence' where the words within the sentence are gaps or pfam identifiers.
\paragraph{}
The purpose of this dissertation is to investigate whether, by creating sentence representations of proteins and encoding them in a popular language encoding algorithm - do the resulting word (pfam) embeddings allow us to determine biologically meaningful relationships between the functional parts of a protein? 
\end{abstract}


% -------------------------- Table of Contents --------------------------------------
\tableofcontents


\setcounter{page}{1}

%\chapter{UCL Samples}
% This is just a bare minimum to get started.  There is unlimited guidance on using latex, e.g. {\tt https://en.wikibooks.org/wiki/LaTeX}.   You are still responsible to check the detailed requirements of a project, including formatting instructions, see
%{\tt https://moodle.ucl.ac.uk/pluginfile.php/3591429/mod\_resource/content/7/UGProjects2017.pdf}.
%Leave at least a line of white space when you want to start a new paragraph.

%Mathematical expressions are placed inline between dollar signs, e.g. $\sqrt 2, %\sum_{i=0}^nf(i)$, or in display mode
%\[ e^{i\pi}=-1\] and another way, this time with labels,
%\begin{align}
%\label{line1} A=B\wedge B=C&\rightarrow A=C\\
%&\rightarrow C=A\\
%\intertext{note that}
%n!&=\prod_{1\leq i\leq n}i \\
%\int_{x=1}^y \frac 1 x \mathrm{d}x&=\log y
%\end{align}
% We can refer to labels like this \eqref{line1}. Often lots of citations here (and elsewhere), e.g. \cite{Rey:D} or \cite[Theorem 2.3]{PriorNOP70}.   Bibtex can help with this, but is not essential. If you want pictures, try

%\begin{center}
%\includegraphics[scale=.5]{images/aristotle.jpg}
%\end{center}
%You can use 
%\begin{itemize}
%\item lists
%\item like this
%\end{itemize}
%or numbered
%\begin{enumerate}
%\item like this,
%\item or this
%\end{enumerate}
%but don't overdo it. \\
%If you have a formal theorem you might try this.
%\begin{definition}\label{def}
%See definition~\ref{def}.
%\end{definition}
%\begin{theorem}
%For all $n\in\nats,\; 1^n=1$.
%\end{theorem}
%\begin{proof}
%By induction over $n$.
%\end{proof}



% ----------------------------------------------------------------
%             My Document
% ----------------------------------------------------------------


% ----------------------------------------------------------------
%             Chapter 1 - Introdcution and Background
% ----------------------------------------------------------------
\chapter{Introduction and Background}

\section{A brief overview of protein structure and protein domains}
\subsection{protein structure and domains}

\subsection{protein families and clans}
Process of Identifying Protein Families (Pfams) and Clans
\begin{itemize}
    \item Collect Protein Sequences Sequences from databases like UniProt are gathered, representing proteins across various organisms. These sequences form the starting point for further analysis.
    \item Multiple Sequence Alignment (MSA) Protein sequences are aligned to identify conserved regions, which are sequences that are similar across different proteins and are often associated with structural or functional importance. Tools like ClustalW or MUSCLE are used to perform MSA. The alignment allows researchers to find shared motifs, which serve as signatures for related proteins.
    \item 3. Create Hidden Markov Models (HMMs)
Based on the multiple sequence alignment, a statistical model called a Hidden Markov Model (HMM) is created. This model captures conserved patterns in the aligned sequences and is used to identify and group proteins into Pfam families. HMMER is a popular tool that implements this process, allowing new sequences to be scanned against existing models for classification into families.
    \item 4. Classification into Protein Families (Pfams)
Sequences are assigned to protein families (Pfams) based on how well they match the Hidden Markov Model associated with a particular family. A protein family is a set of proteins that share significant sequence conservation, suggesting that they are evolutionarily related and likely perform similar functions.
Pfam database is widely used for classifying proteins into these families.
\item 5. Identify Higher-Level Clans
Pfam Clans are created by grouping together related protein families that share a common evolutionary origin. Even if individual families have diverged in function, sequence, or structure, they can still be linked to a shared ancestor. Evidence from sequence similarity, structural data, or scientific literature is used to define these clans.
    \item Clans represent a broader evolutionary grouping compared to individual protein families.
\end{itemize}

\section{Computer Science and Machine Learning application to Bioinformatics}

\section{Language Embedding}
\\subsection{The word2vec algorithm}

\section{Latest Research on word embeddings and protein prediction}

\section{Motivation and Objectives}

% ----------------------------------------------------------------
%             Chapter 2 - Methods
% ----------------------------------------------------------------
\chapter{Methods and Approach}

The section describes the end to end approach and methodology followed in preparing this dissertation and provides an explanation of any decisions made as well as insights made along the way.

\section{Method principles}
There are many different ways in which the objectives of this dissertation can be tackled. The following principles were used to help decision making. These are a mixture of standard industry best practice combined with some personal preferences.
\begin{itemize}
    \item Use the most appropriate tool for the job - there is no point reinventing the wheel if there is a proven, suitable alternative available.
    \item Choose methods that provide transparency such that processing errors can be identified as quickly as possible. In practical terms, this means having good logging and reconciling inputs and outputs at each point in the process.
    \item Each step must be repeatable and deterministic. It is certain that mistakes will be made and that some steps will need to be repeated. To facilitate this, the approach should allow this to happen with minimal loss of time. In practical terms, this includes adopting good coding techniques (e.g. modular code), using version control (github) but also storing interim data at various points along the way in a digestible format.
    \item A preference that all steps can be reproduced on a regular laptop within a reasonable time frame. 
    \item Where additional computing or processing power is required, there is a preference for using tools that are also ubiquitous in industry. This principle is driven from a personal desire to maximise exposure to popular industry tools prior to returning to work. In practice it meant using \textbf{Amazon Web Services} and \textbf{Terraform} for some tasks (as long as it did not add too much time and effort overhead).
\end{itemize}

% ----------------------- Methods - Overall Process
\section{End to end view of method}
At a 10,000ft level, this dissertation was prepared in five key steps as per figure \ref{fig:e2e_flow} below. They are:

\begin{enumerate}
	\addtolength\itemsep{-2mm}
	\item Produce a corpus from raw data including proteins, protein families and disorder regions
	\item Using this corpus, create a number of word2vec models using different word2vec hyper-parameters
	\item Identify the 'best' of these models by performing a comparison with another distance matrix, derived directly from biological information
	\item With that 'best' model, evaluate whether its vectorized word-encodings are 'clustered' in the multi-dimensional encoding space in a way that correlates with actual protein family clusters (called 'clans')
	\item Undertake further investigation to explain the success or otherwise of this clustering
\end{enumerate}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/end2end_flow.png}
	\caption[Overview of the end to end approach]{Overview of the end to end approach}
	\label{fig:e2e_flow}
\end{figure}
\pagebreak



%
%
%
% ----------------------- Methods - Creation of corpus
%
%
%
%


% \section{Corpus preparation} The preparation of the corpus was a significant undertaking due to the large quantity of information to parse, cleanse and restructure prior to creating the corpus itself. Wherever possible this was undertaken locally on a Macbook. \\
%There was little uncertainty over the source of raw data and tokens to be used - there are well known protein databases online that contain this information for download. Furthermore, as this dissertation was building on a previous paper with this topic, the type of input data required was the same in order to provide a comparison (i.e. eukaryotic proteins, protein families and disorder regions). \\
%However, the format and size of input files was different - especially for disorder regions, and this presented a number of challenges in order to extract the data in a timely and repeatable manner. \\


\subsection{Raw data download and initial parsing}
The table below \ref{table_datasources} lists the sources of data used in preparing the corpus, their formats and sizes. 

%
% ----------------------- TABLE EXAMPLE ----------------------
%
\begin{table}[hbt!]
\centering
\label{table_datasources}
\begin{tabular}{|p{35mm}|p{16mm}|p{22mm}|p{25mm}|}
	\hline
	data & source & format & size (unzipped)\\
	\hline
	Eukaryotic Proteins &  Uniprot & fasta text files & 62.3 GB\\
	Protein Families&   Interpro & csv files  & 98.7 GB\\
	Disorder Regions&  Interpro & xml & 188.5 GB\\
	\hline
\end{tabular}
\caption{Data sources required in the creation of a corpus.}
\end{table}

%
% ----------------------- Methods - Protein download
%
\paragraph{Proteins - Download and parsing of data from Uniprot} The protein extract was downloaded from the online Uniprot protein database. It is possible to save some local processing by searching Uniprot for an extract of eukaryotic-only proteins rather than all proteins. However, it takes Uniprot up to 12 hours to prepare this extract which is then made available for download as a zip file (62.3 GB when unzipped).\\ The fasta format is widely used in bioinformatics for representing protein sequences. Each fasta entry represents a single protein and consists of two parts - a header line and the amino-acid sequence itself. The header line starts with a '\textgreater' symbol and contains the protein's name and accession number (unique identifier). The accession number provides a unique reference key contained within all the data files required for the corpus - allowing them to be knitted together to create a sentence per protein.
\paragraph{}The figure below \ref{fig:corpus_protein} shows an example of a single protein entry as it appears in Uniprot fasta download. It highlights the unique protein identifier (accession number) and the amino acid sequence. Although the sequence itself is not required for the corpus, its length is in order to create a corresponding sentence to represent it.


\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_protein_fasta.png}
	\caption[protein\_corpus]{Uniref100 Protein fasta extract, highlighting the areas of relevance for the corpus}
	\label{fig:corpus_protein}
\end{figure}

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
The protein information was extracted in python using the SeqIO module from the Biopython library. On a Macbook, it took 1,428s (24 minutes) to parse 95,272,305 eukaryotic proteins and extract the id and length of each protein to a csv file.
\end{tcolorbox}


%
% ----------------------- Methods - PFAM download
%
\vspace{5mm}
\paragraph{Protein Families (pfams)} Protein family information is available for download from Interpro. Interpro integrates data from multiple protein signature databases to provide a single consolidated view of the results of the various functional analyses that have been performed upon protein sequences. They maintain and regularly release updates to this information in the 'protein2ipr.dat' which is available for download as a 19GB (zipped) tab delimited file \href{https://ftp.ebi.ac.uk/pub/databases/interpro}{protein2ipr.dat}.\\ Each line of the file represents one entry from the underlying signature database mapped to its protein succession number. Thus for one protein there will be multiple entries. Specifically, for the purpose of the word2vec corpus, each line contains the following information:
\begin{itemize}
    \item The unique protein accession number (e.g. A0A010Q340)
    \item The source database entry for that protein identified by a unique key for the database source. For example, entries from the Pfam database have and id starting with 'PF' e.g. PF00172. Entries from the SMART database (which identifies repeated sequence motifs) have an id starting with SM e.g SM00906.
    \item The start and end position on the protein's sequence of the source database entry.
\end{itemize}
The extract is not limited to eukaryotic proteins, thus resulting in a 98.78GB file containing over 1.3bn lines. \\
Parsing the file itself requires identifying only those lines with PFAM entries, then extracting the PFAM id, Protein Accession number and the start and end positions of the pfam domain along on the protein's sequence.\\
An example is shown in figure \ref{fig:corpuspfam} below.

% ----------------------- Graphic: PFAM extract
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_pfam}
	\caption{Pfam data extract - relevant data for the corpus}
	\label{fig:corpuspfam}
\end{figure}
% ----------------------- Performance: PFAM extract
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\small The protein2ipr.dat file, consisting of 1,355,591,115 records was processed using standard python regular expressions. On a Macbook, it took 13,042s (\texttildelow 4 hours) to producing a csv file containing only protein id, pfam id and start and end positions of the pfam domain.}
\end{tcolorbox}


%
% ----------------------- Methods - Disorder region download
%
\paragraph{Disorder regions} A disordered region on a protein refers to a portion which lacks a stable 3-dimensional structure. The disorder region information is also available from Interpro and contained within the 'extra.xml' file. This file contains metadata which supplements the main Interpro dataset but which is not included in the main protein2ipr.dat file. \\ Parsing this file proved to be one of the more challenging tasks in data preparation. \\ The XML has a relatively simple structure only 3 levels deep - \xml{protein} elements at the top level identify each protein by its unique accession code. \xml{match} child elements provide information on various metadata entries for that protein. This includes GO Terms (Gene Ontology), cross reference links to external databases, taxonomy information and more. Disorder regions are identified by the attribute {\small MobiDBLite} within the \xml{match} elements. The position of these items along the length of the protein are contained within \xml{lcn} tags nested underneatrh the \xml{match} element. \\ Although this file is well structured XML, because it serves as a 'catch all' for a wide range of protein meta-data it is quite large. The parsing challenge is due to the sheer volume of information it contains.
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\small The extra.xml file from Interpro, is 188 GB in size. It consists of 4 billion lines of xml and 230,397,847 \xml{protein} tags. Nested within this structure are the 57 million disorder entries we are interested in.}
\end{tcolorbox}

\paragraph{} Attempts to extract this information locally using fast lex processors failed - these processors work by parsing a file sequentially rather than trying to read the whole tree structure into memory. Larger Amazon Web Service (AWS) servers were also tried, but these too ran out of memory, leading to the suspicion that there may be a bug in the python parsing library which should not have been using so much memory. \\ Finally, the only realistic strategy was to split the xml files into separate, smaller chunks and then parse those individually using the aforementioned python xml parsers.  This was achieved by producing 24 separate, well-formatted xml files, each containing 10M \xml{protein} xml elements. Standard python regular expressions were then used to extract the relevant information from each of these into a csv format which could then be loaded into a local database.

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\small It took on average 5mins to chunk a set of 5M proteins from extra.xml (chunk\_disorder\_xml.cpp), and XXXX mins to then run the parser and finally 2min 25s to load the disorder information into the database.}
\end{tcolorbox}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_disorder}
	\caption[corpus metadata]{extra.xml - relevant disorder data for corpus}
	\label{fig:corpusdisorder}
\end{figure}

% ------------------------ 
\subsection{Combining parsed data into a corpus}
With the raw input files parsed and the key data elements (protein ids, pfam domains and disorder regions) extracted into csv files, the next step is to consolidate these constituent pieces of data to create a single 'sentence' for each protein. The collection of sentences for each protein is the \textbf{corpus} that will be used to train the word2vec model. 
\paragraph{}Each sentence consists of words that represent either \textbf{pfam domains} or \textbf{disorder regions}, arranged in the same order that they appear on the protein sequence itself (with overlaps removed).
\paragraph{}The data components required to form a sentence are all linked by the unique protein accession id. Also, although the protein information downloaded from Uniprot already contains only eukaryotic proteins, the pfam and disorder regions are more wide-ranging and needed to be filtered down. This is ideal territory for a relational database and the approach for this next step was largely driven by that choice of technology.
\paragraph {Data load into a database}
A fast database called 'duckdb' was used locally to store the data from the previous stage. DuckDB uses a columnar-vectorized query execution engine where queries are still interpreted but large batches are processed in one operation. It is easily installed on a Macbook, has a low memory and file-system footprint, has full integration with Python and can load large csv files in seconds with a single line of python code. As per the principles adopted for the method, adopting this approach also facilitated debugging and is quick to re-run should an issue be identified later on in the process.
\paragraph{}Thus the data from the csv files was loaded into two separate tables - one to hold the protein information (W2V\_PROTEIN) and another to hold both the pfam and disorder details (W2V\_TOKEN) as per below:
\vspace{5mm}

\begin{center}
	\begin{tabular}{|p{25mm}|p{85mm}|}
	\hline
	\multicolumn{2}{|c|}{\textbf{W2V\_PROTEIN}} \\
	\hline
	COUNTER&A simple integer counter to help with table iteration when combining data into a corpus  \\
	\hline
	UNIPROT\_ID&The unique accession id of the protein \\
	\hline
	LENGTH&  Length of the protein sequence \\
	\hline
\end{tabular}
\end{center}

\vspace{5mm}

\begin{center}
	\begin{tabular}{|p{25mm}|p{85mm}|}
	\hline
	\multicolumn{2}{|c|}{\textbf{W2V\_TOKEN}} \\
	\hline
	UNIPROT\_ID&A reference to the unique accession id of the protein    \\
	\hline
	TYPE&Whether the token is for a pfam entry or a disorder region on the protein  \\
	\hline
	TOKEN&The token itself - for a pfam entry this is the pfam id, for a disorder region it is the description of the region    \\
	\hline
	START &  The start position of the token along the protein sequence  \\
	\hline
	END &  The end position of the token along the protein sequence  \\
	\hline
\end{tabular}
\end{center}

\vspace{5mm}

\paragraph{Extracting only eukaryotic protein data}
With the data in the database, it was a relatively simple matter to create a table join between the w2v\_protein and w2v\_token tables and extract only the information for eukaryotic proteins. This step produced in one output line per token per protein saved to a file - referred to as the pre-corpus \ref{fig:corpusmetadata}. \\

\begin{table}[hbt!]
\centering
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm,code={\onehalfspacing},]
\textbf{Observation/Lesson learned}: By adding a 'COUNTER' to the w2v\_protein table, join queries were extremely quick and allowed 10M proteins and their associated tokens to be processed in 30s. \\ \\ An alternative approach using the inbuilt 'group by' and 'count' keywords of the SQL language did not perform - these instructions cause the entire dataset to be loaded first into memory prior to applying any group by, count or ordering clauses; this causes memory issues and is extremely slow - taking up to 1 hour to process 10M rows, with the duration increasing with each querty . \\ \\ By contrast, adding a COUNTER column (integer from 0 increasing by 1 for each subsequent row), allows queries to be 'paginated' - each query only returns the rows corresponding to the COUNTER values requested. The COUNTER values can be tracked in python as each query returns. \\ \\ 
The SQL used is shown below with the 'start' and 'end' variables tracked with python code.\\ \\
	{\small {SELECT T1.UNIPROT\_ID, T1.TOKEN, T1.TYPE, T1.START, T1.END FROM W2V\_TOKEN T1 WHERE UNIPROT\_ID IN ( SELECT UNIPROT\_ID FROM W2V\_PROTEIN T2 WHERE T2.COUNTER \textgreater= start and T2.COUNTER \textless end ORDER BY T2.COUNTER)}}
    \end{tcolorbox}

\caption{SQL join to extract pfam and disordered 'tokens' for eukaryotic proteins}
\end{table}


\paragraph{Combination of all tokens per protein}
The databae query described above, creates an output file with multiple lines for each protein (each line containing the information for one token). \\ \\These are then combined to create a \textbf{pre-corpus}\ref{fig:corpusmetadata} file containing all the metadata required to produce a single sentence per protein. A protein's meta-data consists of the protein identifier, the sequence length, the total number of tokens (pfam domains or disorder domains) as well as the number of each type of token and their start and end positions along the length of the protein. \\
This information is used both to combine all the relevant data into a single 'sentence' of tokens per protein, and to identify information about the average length and numbers of tokens in a protein - this proves useful when deciding upon hyperparameter options for the word2vec models.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_metadata}
	\caption[corpus metadata]{Pre-corpus metadata - all possible tokens for a protein}
	\label{fig:corpusmetadata}
\end{figure}

\paragraph{Final corpus creation}
Finally, the pre-corpus metadata was parsed to remove overlapping regions and insert the word 'GAP' where the region on the protein had no definition. There were many gaps within each sequences, thus the words 'START\_GAP' and 'END\_GAP' were used to differentiate between unidentified areas at various points on the sequence.

As an example, combining all the information from the previous sections, the resulting sentence for the protein '\textbf{A0A010Q340}' is shown below. The final corpus output consisted of 50,894,561 sentences. \\
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_line}
	\caption{Final corpus \- The sentence to be included in the corpus for protein A0A010Q340 after removing overlapping tokens and filling gaps with the word GAP and adding START\_GAP and STOP\_GAP token to gaps at the extremities.}
	\label{fig:corpusline}
\end{figure}


\section{Model creation}
As described in more detail in the background section, \textbf{Word2Vec} is a neural-network model used in word prediction problems. It was developed by \textbf{Tomas Mikolov} and colleagues at \textbf{Google} in 2013. It works by parsing a supplied corpus of words/sentences and building a vector representation of each word based upon how often it appears alongside other words in the corpus. The idea is that words that are related to each other (i.e. they appear close together in sentences) will also be close to each other in the multi-dimensional vector space produced by the model.\\
The model has two main variants:

\begin{itemize}
    \item \textbf{CBOW} (Continuous Bag of Words) : This attempts to predict a word based upon its surrounding context words
    \item \textbf{Skip-Gram} : Tries to predict context words based upon the target word.
\end{itemize}
To align on terminology, the vector representation of a word created by the model is referred to as its \textbf{embedding}; the list of unique words that end up being embedded are referred to as the  model's \textbf{vocab}.\\ 
For each model variant, \textbf{hyper-parameters} influence how the vector representations are created. For our purposes a number of different models were created for each of the CBOW and Skip-Gram variants using different values for these hyper-parameters:

\begin{itemize}
    \item \textbf{vector size} : The number of dimensions used to embed each word.
    \item \textbf{window size} : The maximum distance (ie number of words) either side of each word to consider when determining relationships between that word and its neighbouring words.
    \item \textbf{minimum count} : Sets the threshold below which words will be discounted from the embedding process if they are infrequent. This can be used to eliminate rare words that could skew the results. The higher this number is, the smaller the resulting model vocabulary will be as certain words are ignored.
\end{itemize}

\subsection{Selection of hyper-parameters and model creation}
The choice of hyper-parameters to use and the number of models of each variant to create was determined by analysing the makeup of the corpus itself and with consideration of cost (time and financial) and available libraries - resources are limited so it is not possible to cover all possible variations!

\paragraph{Corpus analysis and model hyper-parameters}
The corpus was analysed to determine the minimum count and window size.


\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
	\hline
	metric & max & min & mean & std dev & 90th & 95th & 97.5th & 99th\\
	\hline
	Tokens per sentence & 3,991 & 1 & 2.83 & 3.91 & 6.0 & 9.0 & 12.0 & 18.0 \\
	pfam tokens per sentence & 3,991 & 0 & 1.79 & 2.64 & 3.0 & 5.0 & 7.0 & 10.0 \\
	disorder tokens per sentence & 285 & 0 & 1.04 & 2.91 & 3.0 & 6.0 & 9.0 & 13.0 \\
	\hline
\end{tabular}
\caption{Analysis of the different tokens in a protein sentence}
\end{table}

From analysis of this information it is clear that:
\begin{itemize}
    \item There are some outliers whose protein sequence consists only of pfam tokens and nothing else - the longest sentence has 3,991 tokens and these are all pfam domains.(e.g. 3,991 pfam tokens!). These outliers are included in the model - if these sentences contain pfam tokens that are not observed frequently, they will be ignored by the model anyway. 
    \item On average, each protein sentence contains just under 3 tokens (2.83) of which roughly 2 (1.79) are pfam domains and 1 (1.04) is a disorder region. From a performance perspective, this indicates that there's no point running models with a very high min-count, as this would not actually make much of a difference.
    \item The 95th percentile number of tokens is 9 - i.e. the majority of sentences have only 9 words or less
    \item The 95th percentile number of pfam tokens on a protein is 5 and is only 7 at the 97.5th quantile.is 9 - i.e. the majority of sentences have only 9 words or less
\end{itemize}

In terms of model execution, the approach taken was to first trial a few configurations and determine execution times and then decide how to parallelise production. These initial tests showed that each model took between 20 and 30 minutes to run - dependent largely upon the minimum word count and window size.
\paragraph{} A key objective of this dissertation is to determine which combination of word2vec hyperparameters result in the 'best' model. However, an exhaustive search needs to be balanced by time and cost. Given the availability of relatively cheap compute resource from Amazon Web Services, it was decided to instantiate a number of EC2 (EC2: Elastic Cloud Compute - essentially a cloud-based server) instances, and get them to run in parallel, generating different models with different hyperparameter configurations.
\paragraph{}Eventually a cluster of 4 AWS servers was created to generate all the cbow models, with the skipgram models being run on a local Macbook. The AWS services ran continuously over 12 hours are a cost of about \$40.
\paragraph{} By the end of this process, the following model configurations were created - each of these for both CBOW and Skip-Gram word2vec variants.

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	parameter & value 1 & value 2 & value 3 & value 4 & value 5 & value 6 \\
	\hline
	min word count & 1 & 3 & 5 & 8 & - & - \\
	window size & 3 & 5 & 8 & 13 & 21 & 44 \\
	vector size & 5 & 10 & 25 & 50 & 75* & 100* \\
	\hline
\end{tabular}
\caption{Data sources required in the creation of a corpus.}
\end{table}
* These models were not created for cbow

\subsection{Selection of a 'best' model}
As we are trying to find biological meaning in the data, the best model to take forward is determined by correlating the encodings of the vocabulary from the word2vec model with a similarity matrix prepared directly from pfam sequences. This 'evolutionary' matrix is therefore derived more directly from amino-acid sequences, whereas the word2vec only looks at relationships between the relative positions of a sequence on a protein.

To perform this comparison, it was necessary to compute pairwise distances between the words in the vocabulary of the word2vec models. The size of the vocabulary in each model is directly related to the 'min count' hyper-parameter - this setting causes the model to ignore words that do not appear less than 'min count' times in the vocabulary.


\paragraph{The Evolutionary Distance Matrix}
Pfam version 100 and EMBOSS 6.4.0 were downloaded. Pfam 100 contains
20,651 Pfam families. A single random Pfam family a single random 
member of each Pfam family was chosen to act as a representative 
sequence. These 20,651 were composed into a single fasta file dataset of 
Pfam “reps”.
Using the EMBOSS implementation of Needleman and Wunsch (NW) all 
pairs of Pfam Rep-to-rep alignments were calculated for a total of 
426,463,801 comparisons. NW alignment scores were recorded for all 
alignments and the alignments discarded. Scores were composed into an 
n x n, symmetric similarity matrix and this matrix was normalised to 
between 0 and 1 such that the maximum similarity is 1. And this matrix 
was then converted to a distance matrix but taking 1 – sim(x,y) for each 
cell in the matrix.

\paragraph{Comparison of vocabulary sizes}
As a result, the word2vec vocabulary have the following sizes:

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|}
    \hline
	unique pfams in corpus & 15,481 \\
	\hline
	unique pfams in r\_and\_rep matrix & 20,651 \\
	\hline
\end{tabular}
\caption{Comparison of Vocabulary sizes between w2v corpus and r\_and\_rep pfams}
\end{table}

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|c|c|}
	\hline
	w2v hyperparameter & resulting w2v vocabulary size \\
	\hline
	1 & 15,481 \\
    3 & 13,535 \\
    5 & 12,815 \\
    8 & 11,884 \\
	\hline
\end{tabular}
\caption{Vocabulary sizes for each word2vec min\_count hyperparameter.}
\end{table}

\paragraph{Creating Distance Matrices for word2vec models}
A distance matrix simply contains the calculatoins of pairwise distances between vectors in vector space. Thus for the word2vec model with a minimum word count of 1, there are 15,485 words in the vocab and the same number of vectors. The pairwise distance matrix has 15,485 x 15,485 = 233,785,225 entries although the diagonal entries are 0.0 and the matrix is symmetrical.

There are generally two ways of calculating vector distances - euclidean and cosine. Euclidean distance would appear to be the better measure considering the nature of the problem, but out of interest, both measures were calculated for each model.

\begin{table}[hbt!]
\centering
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\textbf{Euclidean distance}: Measures the point to point distance between to points in space. \\
 \textbf{Cosine distance}: Measures the angular distance between tow vectors. If two vectors point in the same direction they will be 'closer' together by this measure.}
    \end{tcolorbox}
\caption{Euclidean v Cosine distance measures}
\end{table}

Pairwise distance matrices were calculated using the sklearn.metrics.pairwise module's cosine\_distances and euclidean\_distances functions and then normalised.

\paragraph{Preparing matrices for comparison}
Prior to comparing matrices, they required some manipulation in order to compare like for like. Both matrices contain distances between pairs of pfam domains, but the word2vec vocab of pfams is smaller due to the way word2vec encodes - and in particular wrt to the min count parameter.

Thus, as per figure \ref{fig:distance_matrices} some manipulatoin was required before comparing. This required
\begin{itemize}
    \item Removing rows and columns from the r\_and\_rep matrix for pfam entries that do not exist in the word2vec vocab
    \item Doing the same for the word2vec matrices \- remove the entries that are not in r\_and\_rap
    \item Make sure that the order of the matrices are the same (ie that the common pfam entries appear in the same rows/columns within each matrix.
\end{itemize}

This exercise was performed with the help of the \textbf{skbio} python library. This library contains a very useful set of functions for comparing different distance matrices. This requires firstly loading the two matrices into two wrapper classes of type \textbf{DistanceMatrix}. Once in that format, skbio can resize them to contain only common indices.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/distance_matrices.png}
    \caption{Preparing distance matrices for comparison}
    \label{fig:distance_matrices}
\end{figure}


\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|}
    \hline
	description & pfam count \\
	\hline
	total pfams in r and rep & - \\
    total pfams in w2v mc1 & - \\
    pfams in r and rep but not in w2v & - \\
    pfams in w2v but not in r and rep & - \\
    resulting matrix size & - \\
	\hline
\end{tabular}
\caption{Common entries on distance matrices}
\end{table}

\paragraph{Performing matrix comparison}
Finding the 'best' word2vec model to take forward for further analysis, requires finding the word2vec model whose distance matrix was closest to the r\_and\_rep matrix. There are a number of commonly used metrics for determining the correlation between matrices. These include:

\paragraph{Pearson Correlation}
The Pearson correlation coefficient measures the \textbf{linear} relationship between two datasets. 

% https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr

\paragraph{Spearman Correlation}
Spearman's rank correlation is a non-parametric measure of the monotonicity of the relationship between two datasets; it's commonly used when the relationship between variables is non-linear. For distance matrices, it measures how well the rank order of distances in one matrix matches the rank order in the other matrix. 
% https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html#scipy.stats.spearmanr

\paragraph{Mantel Test}
A Mantel test  compares two distance matrices by computing the correlation between the distances in the lower (or upper) triangular portions of the symmetric distance matrices. However, the correlation calculation still relies upon either  Pearson’s or Spearman’s rank correlation coefficient.

% https://scikit.bio/docs/dev/generated/skbio.stats.distance.mantel.html#rcee8d6e1aac4-1

\paragraph{}All these correlation coefficients vary between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.

\paragraph{}The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. Although calculation of the p-value does not make strong assumptions about the distributions underlying the samples, it is only accurate for very large samples (>500 observations).

\paragraph{} All of these statistics have implementationt in the scipy python library and are very quick to create once two correctly sized matrices are in place. Thus it was a simple matter to create all 3 for comparison. The results are show in the Experiments section.

\section{Assessing the power of the 'best' model}
The key objective of this dissertation is to establish whether the vector representations of pfam 'words' as generated by the word2vec models, are positioned in vector space such that the clusters they form in that space bear some correlation to a biological or evolutionary cluster derived from more traditional means. If successful this could allow useful new insights to be gained from the vector clusters.

\paragraph{}There are two parts to this - on the one hand, there are numerous Machine Learning algorithms that will cluster a dataset based upon the features of each sample (pfam word encoding). The KMeans algorithm is a good example of this.  On the other hand, another grouping of protein family domains is required to provide a comparison. For this, pfam clans were used.
\paragraph{} A \textbf{Pfam clan} \cite{pfam} groups together multiple Pfam families that are believed to share a common ancestry through evolution. This grouping is determined based on shared features including sequence motifs, sequence structure, or other evidence that points to a common evolutionary origin. Pfam clans provide a higher-level organization of protein families, making it easier to study the evolutionary relationships between large groups of proteins.

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	\textbf{Characteristics of Pfam Clans}
 \begin{itemize}
     \item Common Evolutionary Origin: Members of a Pfam clan are derived from a single evolutionary ancestor, although they may represent different protein families today.
     \item Hidden Markov Models (HMMs): Each family within a clan is represented by an HMM - a statistical model used to describe protein sequence patterns that have been conserved through evolution. Clans group related HMMs together.
     \item Functional and Structural Similarity: Clans often consist of protein families that share structural or sequence similarities due to their common origin - even if they have different functions.
     \item Hierarchical Organization: Pfam clans provide a higher-level organization of protein families, making it easier to study the evolutionary relationships between large groups of proteins.
 \end{itemize}
    \end{tcolorbox}

With this understanding, the problem statement can be more succinctly rephrased as: 
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	\textbf{Problem Statement} \\ \textit{Given a dataset of pfam domains encoded into a multi-dimensional vector space, can Machine Learning techniques identify clusters that correlate with pfam clans - which themselves group multiple Pfam families that share a common evolutionary ancestor}
    \end{tcolorbox}
\vspace{7mm}
\Paragraph{\textbf{Retrieving pfam clans}}\\ 
Retrieving clans is straightforward. Interpro provide a simple webservice API that returns details for a pfam entry, including its clan (where it is defined). As as shown in figure \ref{fig:queryclan}, this is easily done in python. The vocab for a model is retrieved and for each word (i.e. pfam id), the Interpro API is queried and the json response is parsed for the clan id. The pfam to clan relationship is stored in the local database in keeping with the principles of the method (whilst also removing the need to continually call the Interpro API!).

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/queryclan.png}
    \caption{Querying Interpro for a pfam's clan}
    \label{fig:queryclan}
\end{figure}

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
Interpro API example: https://www.ebi.ac.uk/interpro/api/entry/pfam/PF13041
\end{tcolorbox}

Unfortunately not all pfams are mapped to clans - as per the table

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|c|}
    \hline
	metric & number of unique pfams & unique pfams with a clan count \\
	\hline
    pfams in w2v mc1 model & - & - \\
    pfams in r and rep & - & - \\
    \hline
\end{tabular}
\caption{Summary quantities of of pfam ids and clans}
\end{table}

\Paragraph{\textbf{Further modifications prior to clustering}}\\ \\
Additionally, it did not make sense to cluster pfam domains that mapped to a clan that had only one pfam entry. Thus these pfam domains were also removed.


\subsection{Clustering of word vectors}

\subsection{Further analysis}




% ----------------------------------------------------------------
%             Chapter 3 - Experiments
% ----------------------------------------------------------------
\chapter{Experiments}





% ----------------------------------------------------------------
%             Chapter 4 - Analysis and Conclusiona
% ----------------------------------------------------------------
\chapter{Analysis and Conclusions}







\appendix


\begin{thebibliography}{}

\bibitem{statmantel} Mantel, N. (1967). "The detection of disease clustering and a generalized regression approach." Cancer Research, 27(2), 209-220.
\bibitem{statpearson} Pearson, K. (1895). "Note on regression and inheritance in the case of two parents." Proceedings of the Royal Society of London, 58, 240-242.
\bibitem{statspearman} Spearman, C. (1904). "The proof and measurement of association between two things." The American Journal of Psychology, 15(1), 72-101.

% Describes the Pfam database, including the concept of Pfam clans, and how they are used to group related protein families
\bibitem{pfam} Punta, M., Coggill, P. C., Eberhardt, R. Y., Mistry, J., Tate, J., Boursnell, C., ... \& Finn, R. D. (2012). "The Pfam protein families database." Nucleic Acids Research, 40(D1), D290-D301. DOI: 10.1093/nar/gkr1065


% An introduction to Hidden Markov Models used in Pfam, which are central to identifying clans
\bibitem{pfamhmm} Finn, R. D., Clements, J., \& Eddy, S. R. (2011). "HMMER web server: interactive sequence similarity searching." Nucleic Acids Research, 39(suppl\_2), W29-W37. DOI: 10.1093\/nar\/gkr367

\bibitem[Pri70]{PriorNOP70}  %%only an example
A.~Prior.
\newblock The notion of the present.
\newblock {\em Studium Generale}, 23:  245--248, 1970.





\bibitem[Rey97]{Rey:D}
M.~Reynolds.
\newblock A decidable temporal logic of parallelism.
\newblock {\em Notre Dame Journal of Formal Logic}, 38(3):  419--436,
  1997.
\end{thebibliography}

\chapter{Other appendices, e.g. code listing}



% ---------------- my document ----------------

\end{document}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{ucl_logo.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}