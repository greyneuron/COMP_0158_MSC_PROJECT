\documentclass{report}
\usepackage{setspace}
%\usepackage{subfigure}

\pagestyle{plain}
\usepackage{amssymb,graphicx,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{float}
\usepackage{tikz}
\usepackage{graphicx,wrapfig}
\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}

\newtheorem{theorem}{THEOREM}
\newtheorem{lemma}[theorem]{LEMMA}
\newtheorem{corollary}[theorem]{COROLLARY}
\newtheorem{proposition}[theorem]{PROPOSITION}
\newtheorem{remark}[theorem]{REMARK}
\newtheorem{definition}[theorem]{DEFINITION}
\newtheorem{fact}[theorem]{FACT}

\newtheorem{problem}[theorem]{PROBLEM}
\newtheorem{exercise}[theorem]{EXERCISE}
\def \set#1{\{#1\} }

\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}



\newcommand{\nats}{\mbox{\( \mathbb N \)}}
\newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}


% ---------------- my additions ----------------
% xml format
\newcommand*{\xml}[1]{\texttt{<#1>}}
\usepackage{tcolorbox}
\usepackage{hyperref}



%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{  	{ \includegraphics[scale=.5]{images/ucl_logo.png}}\\
\vspace{5mm}
{{\Huge Finding biologically meaningful relationships in Word2Vec protein embeddings}}\\
{\large }\\
		}
\date{Submission date: 7 October 2024}
\author{Patrick Lowry\thanks{
{\bf Disclaimer:}
This report is submitted as part requirement for the MSc in  Data Science and Machine Learning at UCL. It is substantially the result of my own work except where explicitly indicated in the text. The report may be freely copied and distributed provided the source is explicitly acknowledged
\newline  %% \\ screws it up
}
\\ \\
MSc Data Science and Machine Learning\\ \\
Supervisor: Daniel Buchan}



\begin{document}
 
 \onehalfspacing
\maketitle
\begin{abstract}
The purpose of this dissertation is to investigate whether, by representing eukaryotic proteins as sentences and encoding them with the word2vec language model - do the resulting word embeddings create clusters in space that reflect biologically meaningful relationships? \\

Language models have become household names in recent years. They rely upon turning vast quantities of textual data, called a corpus,  into a numeric encoding - or embedding. Embeddings are multi-dimensional vectors - one embedding for each word in the corpus. In this way 'distances' between words can be calculated within the embedding vector space and used, for example, in word prediction.
\paragraph{}
Proteins have their own language - they are made up of long, varying-length sequences of amino-acids, each represented by a single character. Through many years of research, short regions within these sequences have been identified as having a common ancestry and common function. These regions are called protein families (\textbf{pfams}) and are annnotated in large protein databases shared by research teams around the world. However, there are many, many millions of protein sequences that have not been annotated at all and whose function remains unclear.

\paragraph{}
In the last 12 years, language modelling techniques have proven very successful in helping to understand the long amino-acid sequences that make up proteins. They have been used to predict the structure and biochemical properties of proteins with some success. One such model\cite{llmword2vec}, based upon \textbf{word2vec}\cite{word2vecoriginal} has been used previously to try to learn the functional grammar within the embeddings of protein domains, as opposed to the sequences themselves. However, this earlier work used only a single set of hyperparameters for the word2vec model.
\paragraph{}
By creating a corpus from a dataset consisting of 78M eukaryotic proteins, 300M protein families and 4BN lines of disordered region information, this dissertation has iterated through over 400 different configurations of word2vec models - each producing its own set of word embeddings in multi-dimensional vector space. These vectors have been clustered via a KMeans algorithm to see if the resulting 'K' clusters mirror biologically meaningful groupings that are seen in real life. Pfam clans were used as the target relationships to test whether the K clusters aligned.
\paragraph{}
Unfortunately, despite exhaustive searching through different model configurations and corpus constructions, the best average correlation that could be found between the KMeans clusters of word embeddings and the real groupings of protein families into clans was only 20\%. This suggests that the word2vec algorithm can not provide meaningful biological insight when compared in this way. It is possible that a more customised encoding algorithm would provide more meaningful correlations.
\end{abstract}


% -------------------------- Table of Contents --------------------------------------
\tableofcontents
\setcounter{tocdepth}{0}

\setcounter{page}{1}

%\chapter{UCL Samples}
% This is just a bare minimum to get started.  There is unlimited guidance on using latex, e.g. {\tt https://en.wikibooks.org/wiki/LaTeX}.   You are still responsible to check the detailed requirements of a project, including formatting instructions, see
%{\tt https://moodle.ucl.ac.uk/pluginfile.php/3591429/mod\_resource/content/7/UGProjects2017.pdf}.
%Leave at least a line of white space when you want to start a new paragraph.

%Mathematical expressions are placed inline between dollar signs, e.g. $\sqrt 2, %\sum_{i=0}^nf(i)$, or in display mode
%\[ e^{i\pi}=-1\] and another way, this time with labels,
%\begin{align}
%\label{line1} A=B\wedge B=C&\rightarrow A=C\\
%&\rightarrow C=A\\
%\intertext{note that}
%n!&=\prod_{1\leq i\leq n}i \\
%\int_{x=1}^y \frac 1 x \mathrm{d}x&=\log y
%\end{align}
% We can refer to labels like this \eqref{line1}. Often lots of citations here (and elsewhere), e.g. \cite{Rey:D} or \cite[Theorem 2.3]{PriorNOP70}.   Bibtex can help with this, but is not essential. If you want pictures, try

%\begin{center}
%\includegraphics[scale=.5]{images/aristotle.jpg}
%\end{center}
%You can use 
%\begin{itemize}
%\item lists
%\item like this
%\end{itemize}
%or numbered
%\begin{enumerate}
%\item like this,
%\item or this
%\end{enumerate}
%but don't overdo it. \\
%If you have a formal theorem you might try this.
%\begin{definition}\label{def}
%See definition~\ref{def}.
%\end{definition}
%\begin{theorem}
%For all $n\in\nats,\; 1^n=1$.
%\end{theorem}
%\begin{proof}
%By induction over $n$.
%\end{proof}



% ----------------------------------------------------------------
%             My Document
% ----------------------------------------------------------------


% ----------------------------------------------------------------
%             Chapter 1 - Introduction and Background
% ----------------------------------------------------------------
\chapter{Introduction}

\section{Proteins}

Proteins underpin almost every biological process in the body and yet consist  of long lines of only 20 amino acids knitted together according to instructions in our DNA. In fact, it is the range of chemical and electrical properties of these 20 amino acids that creates the huge combinatorial space resulting in such a wide diversity of function across proteins.

%Although proteins are composed from a set of only 20 amino acids there is huge variation in shape and function as a result of the complex makeup of amino acids. Amino acids are non-trivial structures (each has a central carbon atom, a carboxylic acod group, a hydrogen atom and a  \textbf{side chain}). The side chains vary in size, shape and chemical properties across the set of 20 acids, and it is this range of structural and chemical variation that underpins the huge diversity of function across proteins.

%With only a few exceptions, all proteins in all species (bacterial, archaeal and eukaryotic) are constructed from these same sets of 20 amino acids. \\ \\
%This basic structure has been in place for billions of years; this link to genetics and evolution is crucial in helping us to identify protein function.

% --------------------

\subsection{Protein function, protein domains and protein families}
\paragraph{}It is widely accepted \cite{introprotdomain3} \cite{introprotdomain4} that the overall function of a protein is determined by the combination of short amino acid sequences along its length, referred to as \textbf{domains} \cite{domain1} \cite{domain2}. Moore \cite{domain1} described it succinctly, referring to these domains as reusable modules - "\textit{nature tends to reuse rather than reinvent whilst being more optimistic; it is this modularity that provides a set of reusable parts that expedite the speeds with which biological entities can evolve}". Domains can be viewed in two different ways - either sequentially or structurally.



% -- From Forslund
\paragraph{Sequential Domains and Pfam} The sequential view considers the amino acid sequences themselves and how they are conserved through evolution. These domains are identified through the process of \textbf{Multiple Sequence Alignment} whereby the amino-acid sequences of proteins are compared to identify these conserved regions. Alignments reveal both evolutionary relationships and functional similarities between proteins. The alignments are used to create \textbf{Hidden Markov Models} (HMM) to represent each domain; these in turn may be used to search new sequences for the presence of the domain represented by each HMM. 

\paragraph{}\textbf{Structural Domains} refer to the spatial arrangement of sequences of amino-acids. From this perspective, domains are independent regions of a protein whose sequences \textbf{fold} over each other to give a unique three dimensional structure. Folding is determined by the chemical and electrical relationships between the amino-acids positioned along the length of a protein sequence - positively charged amino acids attract negatively charged ones, hydrophobic amino acids will tend to towards the inside of a structure when in an aqueous solution etc. These minuscule forces cause the protein to fold and the resulting spatial structure is a key determinant of a protein's function.


\paragraph{Disordered Regions}There are other regions along the length of a protein's that are not considered to be domains, although they can have function. These regions, called Disorder Regions \cite{introdisordered} do not have a fixed or stable structure but will often adopt a more defined structure when they bind to other proteins or molecules - a process called "induced fit".

\paragraph{}In this study we build a domain-based representation of protein sequences, using sequential domains and disorder regions to create a lexical representation of each protein. We explore whether the embeddings of these domains produced by a language model contain biologically useful information.




\subsection{Cataloging proteins and protein domains}
% https://theconversation.com/what-is-a-protein-a-biologist-explains-152870
% https://gfieurope.org/blog/2023-was-a-record-breaking-year-for-uk-alternative-protein-research-funding-heres-a-recap/

The vast amount of protein-related data including sequences, structure, domains, interactions, and functions requires careful management and oversight to provide consensus views upon which further research can be based. This study has used Pfam, Uniprot and Interpro as the prime sources of data when creating its domain-based representation of each protein.

\paragraph{Pfam \cite{pfam0}} was formed in 1995 in order to create a centrally managed collection of domains identified through the MSA and HMM process described above. The objective was that Pfam could be used to annotate the protein coding genes of multicellular animals. As of June 2024, Pfam contained 21,979 pfam entries - each uniquely identified by an accession number and cataloging its name, taxonomy, evolutionary and structural information and type.


\paragraph{Uniprot} or 'Universal Protein Resource' is a comprehensive resource of accurate protein sequence and functional information. It stores protein sequence data, functional information, protein names, taxonomies as well as cross references to other databases. It has two main parts - UniProtKB/Swiss-Prot entries have been manually reviewed and annotated, whereas UniProtKM/TrEMBL entries have been automatically annotated but not reviewed. Each protein sequence in the Uniprot database has a unique identifier, called an \textbf{Accession code} - this is critical to allow researchers to combine data under a unique identifier.

% https://www.ebi.ac.uk/training/online/courses/interpro-quick-tour/interpro-data/
\paragraph{InterPro}integrates protein signatures from 13 member databases, each of which uses a different method to classify proteins. Interpro 'curators' manually merge signatures that represent the same protein family, domain or site into single InterPro entries and if possible, trace biological relationships between them. The latest release is made available for download from \href{https://ftp.ebi.ac.uk/pub/databases/interpro/}{FTP} download. This dissertation used extracts from version 100, released in May 2024.

%They maintain and regularly release updates to this information in the 'protein2ipr.dat' which is available for download as a 19GB (zipped) tab delimited file. The extract is not limited to eukaryotic proteins,  resulting in a 98.78GB file containing over 1.3bn lines.



\section{Language Models and Protein Word Embeddings}
%Language Models try to predict the next word in a sentence based upon the current word or words. This is achieved by creating some measure relating words to each other. The predictive text feature we are all familiar with does exactly that - it proposes the next word in a sentence based based upon a measurement such as a probability that the next word follows the current word. These measurements are determined following training on a corpus of text containing all the words in the vocabulary.

Language Models are computational models used in Natural Language Processing (NLP) for tasks including word translation, word prediction and sentence generation. Given the complexities of language, the challenge for an NLP model is to represent the \textit{meaning} of words, much of which is driven from the context within which they appear. In traditional NLP, words are regarded simply as discrete entities and represented as one-hot vectors (the dimension of the vector being the size of the vocabulary) - this approach does not allow word similarity to be determined. 

% from stanfor lecture slide 15
\paragraph{}In 1957, Firth \cite{firth} proposed that a words meaning is given by the words that frequently appear close to it ("You shall know a word by the company it keeps") whilst Osgood 1957 \cite{osgood} proposed using a 3 dimensional space to represent the connotation of a word. The term \textbf{vector semantics} is the combination of these two ideas - i.e. representing a word as a point in a multi-dimensional space that is derived from the distributions of word neighbours.  Vectors for representing these words are called \textbf{embeddings}.


%\subsection{N-grams}
%One of the earliest ??? approaches are N-gram models. These can be traced back to Markov's study in 1913 where he tried to predict whether an upcoming letter would be a vowel or a consonant. Shannon (1948) also used this technique to compute approximations to English word sequences. In 1975 Jelinek and colleagues at IBM used n-grams in speech recognition systems and from that seemed to trigger a resurgence of interest.\\

%The N-gram set of models predict the probability of a word based upon the words that have come before it, how far back they look before the current word $w_{n-1}$ is based upon the parameter $n$. They make a Markov assumption that the probability of the next word, $w$, depending upon all words up to that point (from $1$ to $n-1$) is roughly equivalent to the probability depending only upon the $n-1$ preceding words. 

%\begin{center}
%$  P(w|w_{1 : n-1}) \simeq P(w|w_{n-1})$ 
%\end{center}

%A \textbf{bigram}, for example sets n=2 and considers only 1 word behind the current word, a trigram will look 2 words behind (3-1) etc.\\

%Although this approach is easy to understand, it is based upon counting the frequency of co-occurring words in a corpus in order to compute a probability; it just uses the parameter $n$ to determine how many words to look at together when counting. It was used with some success by Vries \cite{vriesngram2008} \cite{vriesngram2017} to help specify conservation profiles in proteins and one of the earliest uses of language models to help analyse proteins.

%Mathematical expressions are placed inline between dollar signs, e.g. $\sqrt 2, \sum_{i=0}^nf(i)$ \\or in display mode
%\[ e^{i\pi}=-1\] and another way, this time with labels,
%\begin{align}
%\label{line1} A=B\wedge B=C&\rightarrow A=C\\
%&\rightarrow C=A\\
%\intertext{note that}
%n!&=\prod_{1\leq i\leq n}i \\
%\int_{x=1}^y \frac 1 x \mathrm{d}x&=\log y
%\end{align}
%We can refer to labels like this \eqref{line1}.

\subsection{Word2Vec}
% https://arxiv.org/pdf/1301.3781 Jurafsky 6.8
The Word2Vec algorithm developed by Mikolov et al. in 2013 \cite{word2vecoriginal} is an example of a Language Model based upon vector semantics and embeddings. It  revolutionised language modelling by successfully capturing the semantic relationships between words. 

The model works by iterating through multiple sentences in a large body of text (the \textbf{corpus}) and analysing the relationships between a \textbf{centre} word and the words that surround it (\textbf{context} words) where the centre and context words are defined within a \textbf{window}, provided as a hyperparameter to the model \ref{fig:w2vwindow}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.7\linewidth]{images/w2v_windowsize.png}
    \caption{word2vec - centre words, context words and window size}
    \label{fig:w2vwindow}
\end{figure}


The model is trained in one of two modes, resulting in two different embeddings, although the architecture is the same.
\begin{itemize}
    \item \textbf{Continuous Bag of Words (CBOW)}: Trained to predict the current 'centre' word given the context words within its neighbourhood
    \item \textbf{Skip-Gram}: Trained to predict the surrounding context words given the current word.
\end{itemize}

\paragraph{}Architecturally, Word2Vec is a shallow neural network, consisting of an input layer, an output layer and only one hidden layer in between (a softmax layer produces probabilistic outputs that sum to 1). The input and output layers have the same dimensions as the vocabulary size; the dimensions of the hidden layer are determined by the '\textbf{vector size}' hyperparameter - it is this that determines the dimensions of the embedding. 

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\linewidth]{images/w2v_arch_new.png}
    \caption{Word2Vec architecture}
    \label{fig:w2varch}
\end{figure}


\paragraph{}The network is trained as the window moves through each sentence in the corpus. Each training iteration performs a binary classification task using the neighbourhood of words defined by the current position of the window - for cbow this means predicting the context words from the centre word. Training is also self-supervised, the training labels for a particular input sample (centre word) are the context words for that iteration. 

\paragraph{}The loss from each training iteration is back propagated through the network resulting in updates to the network weights. Thus, through thousands of training iterations, the weights in the hidden layer come to represent the semantic relationships observed between individual words throughout all sentences in the corpus. 

\paragraph{}Upon completion of training, the weights between each input node and each node in the hidden layer represent a word's position in multi-dimensional space - i.e. their embeddings. If certain words appear close to each other in sentences during training, then their embeddings will position them close to each other in multi-dimensional space (where 'close' is measured by the cosine distance between the embeddings).


\subsection{Embedding Proteins with Word2Vec}
This study uses language model embeddings to find semantic meaning within the language of proteins. Representing a protein as input into a language model can be done in two ways - the \textbf{sequential} approach such as that adopted by \cite{asgari} Asgari, uses the single character amino acid codes. The \textbf{domain} approach creates sentences based upon the higher-level domain architecture of proteins. 

\paragraph{} This study adopts the latter approach, using Pfam ids and disorder regions to build a lexical representation of a protein, maintaining the relative position of these units along its length. To reflect the many sections of uncatalogued or unknown regions that exist in between, the word 'GAP' is used to complete a sentence as per figure \ref{fig:simplesentence}. The detailed mechanism of building sentences is covered in the Method section.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.5\linewidth]{images/w2v_simple_sentence.png}
    \caption{Representing a protein as a sequence of domains}
    \label{fig:simplesentence}
\end{figure}

%The vocabulary for the model is thus the unique set of tokens - i.e. all unique Pfam domains for eukaryotic proteins plus the filler words 'DISORDER' and 'GAP'; the corpus is the full set of sentences for 95 million proteins. 

\paragraph{}By training a number of models on a corpus created from these protein 'sentences', this study investigates whether the resulting embeddings position Pfam domains in multi-dimensional space such that those close to each other in a semantic sense are also close to each other in a biological sense.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/word2vec_diag.png}
    \caption{Word2Vec Training on a protein sentence - illustration}
    \label{fig:w2vdiagram}
\end{figure}


%\paragraph{}As illustration, the example in \ref{fig:w2vdiagram} assumes a vocabulary of only 5 words and a vector size of 8. Thus there are 5 inputs, 5 output, 40 weights connecting the input layer to the hidden layer and a further 40 connecting the hidden layer to the 'summation layer'. The last layer executes a Softmax to determine the probabilities of each 'next word' which is then output in the final layer.

%\paragraph{}Using the cbow model as an example, the objective is to predict context words from centre words. A window determines which context words to consider in each iteration - that being the distance either side of a centre word. In the example shown, the token PF00172 is the centre word and a window size of 1 makes GAP and PF00152 the target words.

%The centre word is encoded as a one-hot vector for the input layer (0 everywhere apart from the position of the word PF00172 in the vocabulary). The correct context words are similarly encoded for the output layer. output for this iteration would be to have `PF02522 and GAP identified as the context word, hence the output layer 'true' label is a one-hot vector with 1's beside PF02522 and GAP.

%Through millions of iterations, reading sentences from a corpus and moving the window along each sentence, the weights in the network are continually updated such that when a centre words is presented as input, the output will eventually consit of the correct contxt words. Follwing training, the weights from the hidden layer represent the word embeddings leared by the network. In theory pfam domains that appear close to each other in protein sentences within the corpus should also be positioned close to each other in the embedding space.


%\paragraph{}The hidden layers' role in this is to manipulate the outputs from their upstream neighbour in such a way that the output at the end is the correct classification number. The connections between each layer have \textbf{weights} and these weights are updated as the network 'learns' to give the correct classification through running through thousands of labelled training examples.

%\paragraph{}Word2Vec is an example of a shallow neural network - meaning it has only one hidden layer connecting inputs to outputs. Under the covers, it trains a binary classifier using a method called negative sampling. For each target word, the model treats that word and a neighbouring context word as a \textbf{positive} example, it randomly samples other words to get a \textbf{negative} sample and then trains a binary classifier to distinguish between those two cases. 

%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%	{\begin{center} The learned weights of a word2vec model are the word embeddings \end{center}}
%\end{tcolorbox}


%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%	{\small \textbf{word2vec} aims to create vector representations of each word in its vocabulary such that similar words are 'close' to each other in the multi-dimensional vector space. 'Closeness' in this case is measured by the cosine distance between the two points. A good embedding would thus position the words 'student' and 'teacher' close to each other in the vector space. In fact, Mikolov even showed that simple algebraic operations upon their word2vec vectors make semantic sense, the classic example being (where $wv$ indicates the word vector representation of a word:
% \begin{center} $wv(king) - wv(man) + wv(woman) = queen$  !\end{center} }
%\end{tcolorbox}

%\section{Producing word embeddings from protein sequences}
%\paragraph{}Each sentence consists of words that represent either \textbf{pfam domains} or %\textbf{disorder regions}, arranged in the same order that they appear on the protein sequence itself (with overlaps removed).
%\paragraph{}The individual 'tokens' required to form a sentence are all linked by the unique protein accession id. Also, although the protein information downloaded from Uniprot already contains only eukaryotic proteins, the pfam and disorder regions are more wide-ranging and needed to be filtered down. This is ideal territory for a relational database and the approach for this next step was largely driven by that choice of technology.

%\todo{
%\begin{itemize}
%    \item \textbf{vector size} : The number of dimensions used to embed each word.
%    \item \textbf{window size} : The maximum distance (ie number of words) either side of each word to consider when determining relationships between that word and its neighbouring words.
%    \item \textbf{minimum count} : Sets the threshold below which words will be discounted from the embedding process if they are infrequent. This can be used to eliminate rare words that could skew the results. The higher this number is, the smaller the resulting model vocabulary will be as certain words are ignored.
%\end{itemize}
%}


\clearpage

\section{Benchmarking embeddings}

\subsection{Clustering}
In order to evaluate whether the embeddings from our models contain biological meaning, we apply clustering techniques to them and compare them to Pfam clans. 

%already provide a grouping of protein families according to common evolutionary ancestry, the challenge of this dissertation is to assess whether the embeddings from word2vec models also form clusters in multi-dimensional space that mirror pfam clans. 

\paragraph{Pfam clans} Pfam introduced the idea of pfam clans \cite{pfamclan} to group together multiple Pfam families. This was necessitated by the growth in research resulting in more and more protein families and new relationships between existing protein families being identified. Clans group together multiple Pfam families that are believed to share a single evolutionary ancestor. They provide a higher-level organization of protein families, making it easier to study the evolutionary relationships between large groups of proteins.


%\subsubsection{Retrieving pfam clans} 
%Retrieving pfam clans is straightforward. Interpro provide a simple webservice API that returns details for a pfam entry, including its clan (if it is defined). As as shown in figure \ref{fig:queryclan}, this is easily achieved in python.
%\paragraph{}The vocab (pfam ids) is retrieved from a model and for each word in that vocab, the Interpro API is queried, the json response is parsed and the clan id extracted with a regular expression. The pfam to clan relationship is then stored in the local database in keeping with the principles of the method (whilst also removing the need to continually call the Interpro API!).

%\begin{figure}[hbt!]
%    \centering
%    \includegraphics[width=1\linewidth]{images/queryclan.png}
%    \caption{Querying Interpro for a pfam's clan}
%    \label{fig:queryclan}
%\end{figure}

%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%Interpro API example: https://www.ebi.ac.uk/interpro/api/entry/pfam/PF13041
%\end{tcolorbox}


\paragraph{KMeans clustering}There are multiple methods available for clustering vectors, for this study, the  KMeans algorithm was selected as it is easy to implement and interpret, fast to execute and works well with relatively low dimensional data. KMeans creates clusters by minimising the distance in space between a set of vectors provided as input and centroids, such that the centroids and their closest vectors form a cluster. The algorithm initially chooses centroids and cluster assignments at random and iteratively re-assigns samples to clusters in order to minimise the sum total distance between each sample and its assigned centroid. The downside of this method is that its success can be influenced by the hyperparameter 'K' which represents the number of clusters to create. 

\paragraph{}For this study we investigate whether the KMeans algorithm can cluster our Pfam embeddings to mirror their respective Pfam clans - thus we set 'K' to the number of Pfam clans that encompass the Pfam ids in our model vocabulary (some of the experiments vary this hyperparameter to gauge its influence).

%MacQueen, J. (1967). "Some methods for classification and analysis of multivariate observations." Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, 1, 281-297. The foundational paper introducing K-Means clustering.

%\paragraph{\textbf{K-Nearest Neighbours}}
%K-Nearest Neighbours is a very popular and simple distance-based Supervised model - i.e. it requires its training dataset to have known labels (or classes). These models simply store the training dataset and associated classes. When presented with an unseen sample, KNN finds its position in vector space and assigns it the class shared by the majority of the 'K' neighbours closest to it. In our case it would only work if the pfams for one clan were already clustered together - but that is what we are trying to establish.\\

%\paragraph{\textbf{DBScan}}
%(Density-Based Spatial Clustering of Applications with Noise)
%is a density-based clustering algorithm that groups points that are closely packed together and separates regions of lower density as noise. It is well-suited for multi-dimensional data where the clusters may not have a regular shape (e.g., elliptical or irregular) and is robust to noise and outliers. \\

%Advantages:
%%Can find arbitrarily shaped clusters.
%Resistant to noise and outliers.
%No need to pre-specify the number of clusters.
%Limitations:
%Struggles with varying densities.
%Performance may degrade in very high dimensions.
%Academic Reference:
%Ester, M., Kriegel, H. P., Sander, J., & Xu, X. (1996). "A density\-based algorithm for discovering clusters in large spatial databases with noise." KDD Proceedings, 226-231. This paper introduces DBSCAN and explains its effectiveness in clustering.
%\paragraph{\textbf{Gaussian Mixture Models (GMMs)}}
%GMM's are probabilistic models that assume that all data points are generated from a mixture of a set number of Gaussian distributions with unknown parameters. They can be thought of as a generalised K-Means algorithm that incorporates information about the underlying covariance and means of the data in each cluster. GMMs typically use the Expectation Optimisation algorithm to determine the optimal covariance and means for each the clusters.

%Similarly to KMeans, the success of GMMs is largely dependent upon the choice of the number of Gaussians to use as well as their initial covariance and means. The EM optimisation routine also makes them computationally intensive. However, they are more likely to be able to find more complex shaped clusters.\\

%\paragraph{\textbf{Principal Component Analysis}}
%A useful pre-step in clustering is to perform Principal Component Analysis on the pfam vectors. PCA provides a useful way to reduce the dimensionality of data whilst maintaining as much of the variance as possible to allow the data to be separated. The advantage of PCA is that it can speed up traditional clustering techniques by vastly reducing the feature space, the PCA components can also be mapped onto a 2D space to provide visual clues as to the clusters that may emerge .

\paragraph{Measuring the effectiveness of our clusters}To gauge the success of clustering, graphical and computational methods are used. Computationally, the \textbf{Jaccard} similarity metric measures the overlap between two sets of data. It is used here to perform a pairwise comparison between the Pfam members of each of the K clusters with the Pfam clans corresponding to those same Pfam members (whose embeddings were used to create the clusters in the first place). Higher values indicate more similarity and thus measure the success of KMeans. The Jaccard formula is show below:

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.4mm, rightrule=0.4mm]
\begin{center} $J(A,B) = |A \cap B|/ |B \cup A| $ \end{center}
\end{tcolorbox}


\paragraph{}Graphically, \textbf{Principal Component Analysis} is used to visualise the position of word embeddings in multi-dimensional space. PCA reduces high dimensional data into lower dimensions called Principal Components whilst maintaining as much of the variance as possible from the original dimensions. Graphing the the Principal Components provides a visual way to interpret the embeddings of word2vec and their positions in space. For our purposes, the embeddings were reduced to their first 3 Principal Components and used to create colour-coded plots with the colours representing different clans. The results are presented in the Experiments section.


\subsection{Distance correlation}
As an interim step between model creation and clustering, we compare our embeddings with those of a supplied dataset derived from Pfam data and protein sequences. This serves as a further benchmark, allowing some models to be filtered out before clustering whilst also providing a comparison between different representations of protein data that ultimately derive from the same data source.

\paragraph{}The embeddings from a word2vec model are first used to create a Distance Matrix; a two-dimensional matrix representing the pairwise distances vectors. It contains the distances (or dissimilarites) between each pair of items such that, for example, entry $D(i,j)$ contains the distance between the i-th and j-th vectors.

\paragraph{}In our case, the vectors used to create the Distance Matrix are the embeddings of the pfam ids extracted from our models. To calculate distances, both euclidean and cosine formulae are used, resulting in 2 distance matrices per word2vec model. 


\begin{table}[hbt!]
\centering
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\textbf{Euclidean distance}: The point to point distance between two points p, q in space. \\
 \begin{center}
 $d(p, q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \dots + (p_n - q_n)^2}$ 
 \end{center}
 
 \textbf{Cosine distance}: The angular distance between two vectors A, B. Two vectors pointing in the same direction  will be 'closer' together by this measure.

  \begin{center}
 $ 1 - \frac{ \mathbf{A} \cdot \mathbf{B} }{ \|\mathbf{A}\| \|\mathbf{B}\| }$
 \end{center}
 
 }
    \end{tcolorbox}

\caption{Euclidean v Cosine distance measures}
\end{table}

\paragraph{}The distance matrix used to benchmark word2vec embeddings against is derived from the Needleman-Wunsch (NW)\cite{nw} algorithm, a dynamic programming algorithm used for sequence alignment. Going forward, this matrix is referred to as the '\textbf{representative pfam matrix}', the method of its creation is described below.  
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
	{\small The supplied matrix contained pairwise similarity measures between protein sequences - each representing a Pfam family. To achieve this, Pfam version 100 (containing 20,651 families) and EMBOSS 6.4.0 were downloaded. A single random 
member of each Pfam family was chosen to act as a representative 
sequence and, using the EMBOSS implementation of Needleman and Wunsch (NW) all pairs of alignments were calculated - giving a total of 426,463,801 comparisons. NW alignment scores were recorded for all alignments and the scores were placed into an 
n x n, symmetric similarity matrix. This matrix was normalised to 
between 0 and 1 such that the maximum similarity is 1 and then converted to a distance matrix but taking 1 – sim(x,y) for each cell in the matrix. D. Buchan \cite{dbuchanreppfam}}
\end{tcolorbox}

In order to measure the similarity between the two distance matrices, the following metrics were used.
\paragraph{Pearson Correlation}
The Pearson correlation coefficient measures the strength and direction of the \textbf{linear} relationship between two variables X and Y. It is calculated as the covariance of the variables divided by the produce of their standard deviations.

\begin{table}[hbt!]
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.4mm, rightrule=0.4mm]
\begin{center}
$r = \frac{\sum_{i=1}^{n} (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum_{i=1}^{n} (X_i - \overline{X})^2} \sqrt{\sum_{i=1}^{n} (Y_i - \overline{Y})^2}}
}
$
\end{center}
\end{tcolorbox}
\caption{Pearson Correlation}
\end{table}
% https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr

\paragraph{Spearman Correlation}
Spearman's rank correlation is a non-parametric measure of the monotonicity of the relationship between two datasets; it's commonly used when the relationship between variables is \textbf{non-linear}. For distance matrices, it measures how well the rank order of distances in one matrix matches the rank order in the other matrix - using rank rather than raw data makes the Spearman coefficent more robust to outliers compared to Pearson. 
% https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html#scipy.stats.spearmanr

\begin{table}[hbt!]
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.4mm, rightrule=0.4mm]
\begin{center}
$
\rho = 1 - \frac{6 \sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}
$
\end{center}
\end{tcolorbox}
\caption{Spearman Correlation}
\end{table}

\paragraph{}Both of these correlation coefficients vary between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases. These measures also produce a p-value indicating the probability of an uncorrelated system producing datasets that have a correlation at least as extreme as the one computed. 

\paragraph{Mantel Test}
The Mantel test is often used in ecology and genetics to compare two distance matrices and measure the correlation between them. The test computes the Pearson correlation coefficient between corresponding elements of the two matrices although the Spearman coefficent may also be used. A significant positive correlation suggests that similar patterns are present in both matrices.

\begin{table}[hbt!]
\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.4mm, rightrule=0.4mm]
\begin{center}
$
Z = \sum_{i=1}^{n} \sum_{j=1}^{n} X_{ij} Y_{ij}
$
\end{center}
\end{tcolorbox}
\caption{The Mantel test statistic Z for a distance matrix of n elements}
\end{table}



\newpage
\section{Literature Review}

There have been many attempts to use NLP models to provide insight into the structure and function of proteins, here we discuss the key models and their applications to this domain.

\subsubsection{NGram Models and Applications}
One of the simplest NLP models is the N-gram whose origins can be traced back to \textbf{Markov}'s study in 1913 \cite{markov} where he tried to predict whether an upcoming letter would be a vowel or a consonant. \textbf{Shannon} (1948) \cite{shannon} also used this technique to compute approximations to English word sequences. 

\paragraph{}In 1975 \textbf{Jelinek} \cite{jelinek} and colleagues at IBM used n-grams in speech recognition systems, predicting the probability of a word based upon the words that have come before it. They made a Markov assumption that the probability of the next word, $w$, depending upon all words up to that point (from $1$ to $n-1$) is roughly equivalent to the probability depending only upon the preceding word. 

\begin{center}
$  P(w|w_{1 : n-1}) \simeq P(w|w_{n-1})$ 
\end{center}

\paragraph{Ganapathiraju at el. (2002) \cite{ganap}}used the n-gram technique to develop a biological language modelling toolikt (\textbf{BLMT}) to analayse amino-acid sequences across organisms. They found that amino-acid n-gram distributions could be used to characterise organisms. Statistically, they found organism-specific phrases in protein sequences which were orders of magnitude of standard deviations away from the mean. Their research indicated that different organisms use different vocabularies and amino-acid phrases and suggested that this could provide novel approaches to drug development by targeting these phrases.

\paragraph{Vries (2008) \cite{vriesngram2008} \cite{vriesngram2017}}also used N-Grams with some success to help specify conservation profiles in proteins. 

\subsubsection{Word2Vec Models and Applications}
The \textbf{Word2Vec} algorithm developed by Mikolov et al. in 2013 \cite{word2vecoriginal} and presented earlier, revolutionised language modelling by successfully capturing the semantic relationships between words. This model has been used to identify semantic relationships between proteins - either using amino sequences or domain architectures as sentences in a corpus.

\paragraph{Asgari and Mofrad (2015) BioVec \cite{asgari}} Shortly after the word2vec \cite{word2vecoriginal} model was released, Asgari and Mofrad used it and N-grams together to classify proteins using their amino-acid sequences. Using N-grams, they produced a set of "\textbf{3-grams}" (sequences of 3 amino acids, also referred to as "k-mers") which were subsequently provided as input to a \textbf{Word2Vec} \textbf{skipgram} model. This resulted in what they refer to as n-dimensional "protein-vectors". Using these vectors and Support Vector Machines they were able to distinguish disordered regions from structured protein sequences with a 99.8\% accuracy. They showed, therefore, that when trained on only sequence data, it is possible to extract accurate information about protein structure.

\paragraph{\textbf{Kimothi et al. - seq2vec (2016) \cite{kimothi}}} Kimothi et al. adopted a similar approach to Asgari and Mofrad - although they used \textbf{doc2vec}, an extension of word2vec. Rather than embedding only "3-grams" (or k-mers), they embedded the entire sequence in order to capture the structure of the protein along its entire length. They refer to this embedding as '\textbf{seq2vec}'. Using this embedding, they were able to classify the resulting vectors into their respective protein families with an accuracy of over 95\%.

%\paragraph{Mazzaferro et al (2017) Predicting protein-ligand binding affinities with a novel machine-learning approach \cite{mazzaferro}} Mazzaferro at el used a novel model to predict the structure of a protein that might elicit a response to create Helper T cells. They used a Recurrent Neural Network (RNN) rather than word2vec and, although they had some success, by their own admission the architecture had "plenty of room for improvement".

\paragraph{Yang et al. (2018) - Learned protein embeddings for machine learning (2018) \cite{yang}} In contrast to the works by Asgari \cite{asgari} and Kimothi \cite{kimothi} and mazzafeno who tried to classify proteins into families, their focus was on investigating whether embeddings from unlabelled sequences could be used to predict specific properties of related proteins. They used a 3 step approach, starting with an unsupervised task using doc2vec on unlabelled sequences, followed by supervised learning and GP regression to make predictions. They concluded that embeddings can produce accurate predictions and suggested that this method may be preferable over other techniques as it does not require alignments or structural data ahead of time.

\paragraph{Melidis et al. (2020) - Capturing domain structure and
function using self-supervision on protein domain architectures \cite{llmdom2vec}}
In this study, Melidis et al use domains rather than sequences to train a word2vec model. Their corpus was created from the Interpro annotations of 128,660,257 proteins and they used both CBOW and SKIPGRAM architectures. By evaluated the predictive ability of the resulting embedding space they concluded that their approach outperforms sequence-based approaches for toxin and enzymatic function prediction and is comparable with them in cellular location prediction. 

\paragraph{Buchan and Jones (2020) - Learning a functional grammar of protein domains \cite{llmword2vec} } In this paper, Buchan and Jones also take a domain-based approach, training a word2vec model on a corpus of the eukaryotic protein domains. They benchmarked nearest neighbour classifier performance on predicting the three main GO ontologies of a Pfam domain and propose that this approach could be used to suggest putative GO assignments for Pfam domains of unknown function. In creating their word2vec models, Buchan and Jones used the skipgram architecture and default settings for word2vec hyperparameters.\\


\subsubsection{Large Protein Language Models}

%The \textbf{Transformer} architecture introduced by Vaswani et al. \cite{transformer} in 2017 started another revolution in NLP. This novel architecture processes large amounts of data in parallel whilst incorporating context and word relationships from throughout a document. This significantly improved the performance of NLP tasks and their ability to encode huge quantities of textual data. Although not the subject of this paper, the Transformer architecture, like the word2vec model before it, lead to further developments in protein language models which are important to understand the current direction of research.

%\paragraph{}The key differentiator of Transformer models is the introducton of the 'attention' concept. This means that the model can focus on (pay attention to) different parts of an input sentence when producing each element of the output. It can also do this multiple times in parallel (called Multi-Head Attention).

%\begin{figure}[hbt!]
%    \centering
%    \includegraphics[width=0.5\linewidth]{images/transformer.png}
%%    \caption{Transformer Architecture \cite{transformer}}
%    \label{fig:enter-label}
%\end{figure}

Both \cite{plmrives2019} (Rives et al 2019) and \cite{plmheinzingerElmo2019} (Heinzinger et al. 2019) used the Transformer architecture to show that, by pre-training a model on a large corpus of protein sequence data, it could then be fine-tuned for downstream tasks of particular interest - such as secondary structure prediction. This transfer learning approach was used to develop a number of important models. 

\paragraph{The TAPE Transformer \cite{plmtape}} (Task Assessing Protein Embedding) is an adaptation of the original Transformer model with each amino acid  represented as a token. The model is pre-trained on a large corpus of protein sequences and then fine tuned for downstream tasks across five biologically relevant downstream tasks - such as secondary sequence prediction. This model is significant in that it demonstrates the effectiveness of transfer learning whereby models are initially pre-trained on large datasets and then fine tuned.
    
\paragraph{ESM (Evolutionary Scale Modeling)} \cite{plmesm}: Developed by researchers at Facebook, this model also uses a  Transformer \cite{transformer} architecture with amino acids as its vocabulary. It is also pre-trained on a large set of protein sequences (the first release, ESM-1b was trained on 86 billion amino acids across 250 million sequences spanning evolutionary diversity). This model is significant due to the scale of its pre-training task and its ability to capture both evolutionary and biochemical information at scale. Significantly, the team showed that, without prior knowledge the learned representations enabled stat-of-the-art supervised prediction of mutational effect and secondary protein structure. 
    
\paragraph{ProtTrans \cite{plmprottrans}} is a suite of protein language models based upon a variety of transformer architectures and trained on up to 393 billion amino acids. These embeddings were used as input for several downstream tasks. Most significantly, for per-residue predictions, the T5 model variant outperformed the state of the art without having to use evolutionary information, thus bypassing expensive database searches.
    
\paragraph{ProteinBERT \cite{protbert}} is also pre-trained on a protein dataset but this training set consists of only 106 million protein sequences. The pre-training also takes place on two simultaneous tasks. The first is bidirectional language modeling of protein sequences; the second is Gene Ontology (GO) annotation prediction, which captures diverse protein functions. This approach enables both local (i.e at the character level) and global (at the entire sequence level) representations to be captured in the word embeddings. Although ProteinBERT is considerably smaller and faster than other models it showed comparable and sometimes superior performance compared to the larger more resource-intensive models.


\paragraph{} The timeline below shows the key developments \\

\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]

\begin{tikzpicture}
% draw a horizontal line
\draw (0,0) -- (13,0);

% draw vertical lines
\foreach \x in {0, 1.3, 2.6 , 3.9 , 5.2, 6.5, 7.8 , 9.1 , 10.4, 11.7, 13.0}
\draw (\x cm,2pt) -- (\x cm,-2pt);

% draw nodes to add events
\draw (0,0) node[below=3pt] {2012};
\draw (1.3,0) node[below=3pt] {2013} node[above=18pt] {\textbf{Word2Vec} (2013) \cite{word2vecoriginal}};
\draw (2.6,0) node[below=3pt] {2014} ;
\draw (3.9,0) node[below=3pt] {2015} node[below=20pt] {\textbf{Asgari} (2015) \cite{asgari} };
\draw (5.2,0) node[below=3pt] {2016} node[below=40pt] {\textbf{Kimothi} (2016) \cite{kimothi} };

\draw (6.5,0) node[below=3pt] {2017} node[above=4pt] {\textbf{Transformer} (2017) \cite{transformer}};
\draw (7.8,0) node[below=3pt] {2018} ;
\draw (9.1,0) node[below=3pt] {2019} ;

\draw (10.4,0) node[below=3pt] {2020} node[above=18pt] {\textbf{Buchan et al.} (\cite{llmword2vec} 2020)};
\draw (10.4,0) node[below=20pt] {d2v grammar (2020)};
\draw (11.7,0) node[below=3pt] {2021} ;
\draw (13.0,0) node[below=3pt] {2022} ;

\end{tikzpicture}

\end{tcolorbox}


%\textbf{Sequence-based} studies have focussed on the low level amino acid sequences that make up the proteins \todo{what do these do?}. \textbf{Domain-based} studies create sentences based upon the higher-level architecture of proteins - such as their families. The former type have access to a much wider set of data - simply because we do not understand enough about proteins to have created a comprehensive architecture for each one! This particular dissertation extends a previous domain-focussed study by broadening its parameters. 

%\paragraph{Asgari and Mofrad (2015) BioVec \cite{asgari}} Not long after the word2vec \cite{word2vecoriginal} model was released, Asgari and Mofrad used it in an attempt to classify protein sequences using language modelling techniques. Using N-grams, they produced a set of "3-grams" (sequences of 3 amino acids, also referred to as "k-mers") which were subsequently provided as input to a Word2Vec skipgram model. This resulted in what they refer to as n-dimensional "protein-vectors". They then used these in classification techniques using Support Vector Machines and wee able to distinguish disordered regions from structured protein sequences with a 99.8\% accuracy. They showed, therefore, that when trained on only sequence data, it is possible to extract accurate information about protein structure.

%\paragraph{\textbf{Kimothi et al. - seq2vec (2016) \cite{kimothi}}} Kimothi et al. adopted a similar approach to Asgair and Mofrad - although they used doc2vec, an extension of word2vec. Also, rather than using it to embed only "3-grams" (or k-mers), they embedded the entire sequence in order to fully capture the structure of the protein along its entire length. They refer to this embedding as '\textbf{seq2vec}'. They used these embeddings to classify the resulting vectors into their respective protein families achieving accuracy of over 95\%.

%\paragraph{Mazzaferro et al (2017) Predicting protein-ligand binding affinities with a novel machine-learning approach \cite{mazzaferro}} Mazzaferro at el tried to create a novel model to predict with the structure of a protein that might elicit a response to create Helper T cells, amongst other things. They chose to use a Recurrent Neural Network (RNN) rather than word2vec and, although they had some success, by their own admission the architecture had "plenty of room for improvement".

%\paragraph{Yang et al. (2018) - Learned protein embeddings for machine learning (2018) \cite{yang}} In contrast to the works by Asgari \cite{asgari} and Kimothi \cite{kimothi} and mazzafeno who tried to classify proteins into families, their focus was on investigating whether embeddings from unlabelled sequences could be used to predict specific properties of related proteins. They used a 3 step approach, starting with an unsupervised task using doc2vec on unlabelled sequences, followed by supervised learning and GP regression to make predictions. They concluded that embeddings can produce accurate predictions and suggested that this method may be preferable over other techniques as it does not require alignments or structural data ahead of time.\\



%\paragraph{Melidis et al. (2020) - Capturing domain structure and function using self-supervision on protein domain architectures \cite{llmdom2vec}} In this study, Melidis et al use domains rather than sequences to train a model using word2vec. Their corpus was created from 128,660,257 proteins containing Interpro signatures - each sentences consisted of the Interpro annotations for those proteins. Using both CBOW and SKIPGRAM word2vec architectures to embed the words.

%\paragraph{}They evaluated the predictive ability of the resulting embedding space in four ways and concluded that their approach outperforms sequence-based approaches for toxin and enzymatic function prediction and and is comparable with sequence embeddings in cellular location prediction. \\

%\paragraph{Buchan and Jones (2020) - Learning a functional grammar of protein domains \cite{llmword2vec} } In this paper, Buchan and Jones train the word2vec algorithm on a corpus consisting of the protein domains of eukaryotic proteins. They focussed on eukaryotic proteins as there are few proteins in the bacterial and archaeal kingdoms that have multiple domains with independent evolutionary histories.  They benchmarked nearest neighbour classifier performance on predicting the three main GO ontologies of a Pfam domain and propose that this approach could be used to suggest putative GO assignments for Pfam domains of unknown function. In creating an embedding space, the main difference with the Melidis paper is that Buchan and Jones used eukaryotic proteins only, in the word2vec configuration the skipgram architecture was used and most of the default settings were adopted.\\



\subsection{Objectives of this paper}
This study picks up on the previous work of Buchan and Jones \cite{llmword2vec} in trying to find biological meaning within the embeddings produced by a Word2Vec language model. 

\paragraph{}It too adopts a domain-based approach, representing proteins as ordered  tokens rather than lower level amino-acid sequences; each token being either a protein domain or disorder region. It uses these tokens to create sentence representations of proteins used as input to a word2vec language model. It also explores whether the resulting embeddings of the pfam domain tokens contain biologically meaningful relationships. However, it differs in a number of ways from the previous work.

\paragraph{}Firstly, different corpus constructs are used - specifically in the treatment of 'GAP' areas. Melidis \cite{llmdom2vec} used a gap of 30 amino characters to include the word "GAP" as a token in a sentence; Buchan and Jones \cite{llmword2vec} experimented with different tokens for different gap lengths (e.g. G200) and also included tokens representing the complexity of a sequence. This study simplifies the sentence structure, ignoring complexity and using the word 'GAP' to represent a gap of any length. Initially this word is only included if the actual gap is either 1 character, 50 or 100 characters long - although this is modified in subsequent experiments. It also uses the word "START\_GAP" and "STOP\_GAP" to cover the scenarios where there is a gap at the start and end of a sequence to provide some level of differentiation.

\paragraph{}Secondly, this study creates a multitude of word2vec models to test the impact of different hyperparameters. This includes the model architecture (both cbow and skipgram are used) as well as different minimum word counts (ranging from 1 to 8), window sizes (ranging from 3 to 44) and vector sizes ranging from 5 to 1,000.

\paragraph{}Thirdly, to help identify a model that may contain biological meaning and explore the correlations between distance matrices created in two different ways, but ultimately from the same data (protein sequences), each model is compared to a supplied distance matrix. This is derived from the alignment of protein sequences - one sequence to represent each Pfam family. Pfam encodings, in order to identify a 'best candidate' for clustering.

\paragraph{}Finally, to measure biological meaning within embeddings, this paper investigates whether KMeans can find clusters within the embedding space that correspond to Pfam clans.



% ----------------------------------------------------------------
%             Chapter 2 - Methods
% ----------------------------------------------------------------
\chapter{Method}

% ----------------------- Methods - Overall Process
\section{Overview}
This dissertation was prepared in five key steps from raw data download to clustering and analysis (as per figure \ref{fig:e2e_flow} ). In summary, the process followed was:

\begin{enumerate}
	\addtolength\itemsep{-2mm}
	\item Download raw data from Interpro and Uniprot including proteins, protein families and disorder regions. Parse and prepare this data - extracting key tokens. Combine these tokens to create a corpus
	\item Using this corpus, create a range of word2vec models using different word2vec hyper-parameters
	\item Identify the 'best' of these models by performing a comparison with another distance matrix (provided), derived directly from representative Pfam protein sequences
	\item With that 'best' model, evaluate whether its associated  word-embeddings produce 'clusters' in the encoding space that correlate with groups of protein families (called 'clans').
	\item Investigate and analyse the results, use these outcomes to motivate further experiments - iterating through different corpi or word2vec models
\end{enumerate}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/end2end_flow_2.png}
	\caption[Overview of the end to end approach]{Overview of the end to end approach}
	\label{fig:e2e_flow}
\end{figure}
\pagebreak



%
%
%
% ----------------------- Methods - Creation of corpus
%
%
%
%


% \section{Corpus preparation} The preparation of the corpus was a significant undertaking due to the large quantity of information to parse, cleanse and restructure prior to creating the corpus itself. Wherever possible this was undertaken locally on a Macbook. \\
%There was little uncertainty over the source of raw data and tokens to be used - there are well known protein databases online that contain this information for download. Furthermore, as this dissertation was building on a previous paper with this topic, the type of input data required was the same in order to provide a comparison (i.e. eukaryotic proteins, protein families and disorder regions). \\
%However, the format and size of input files was different - especially for disorder regions, and this presented a number of challenges in order to extract the data in a timely and repeatable manner. \\


\section{Data gathering}
The table below \ref{tab:datasources} lists the sources of data used in preparing the corpus, their formats and sizes.

%
% ----------------------- TABLE EXAMPLE ----------------------
%
\begin{table}[hbt!]
\centering

\begin{tabular}{|p{50mm}|p{35mm}|p{22mm}|p{25mm}|}
	\hline
	data & source & format & size (unzipped)\\
	\hline
	Eukaryotic proteins &  Uniprot download & fasta text files & 62.3 GB\\
	Pfam domains & Interpro ftp & csv files  & 98.7 GB\\
    Pfam clans&   Interpro API & json  &  - \\
	Disorder regions&  Interpro & xml & 188.5 GB\\
	\hline
\end{tabular}
\caption{Data sources required in the creation of a corpus.}
\label{tab:datasources}
\end{table}

%
% ----------------------- Methods - Protein download
%
\paragraph{Proteins} 
The protein extract was downloaded from the online \textbf{Uniprot} protein database. Local processing may be saved by searching Uniprot for an extract of eukaryotic-only proteins rather than all proteins. It takes Uniprot up to 12 hours to prepare this extract which is then made available for download as a zip file (62.3 GB when unzipped).

\paragraph{Pfam domains} Protein family information is downloaded from the \textbf{Interpro} \href{https://ftp.ebi.ac.uk/pub/databases/interpro}{ftp site} as protein2ipr.dat. This dissertation used version 100.0, released in May 2024. The Interpro extract is not limited to eukaryotic proteins,  resulting in a 98.78GB file containing over 1.3bn lines.

\paragraph{Pfam clans} 
Pfam clans are also retrieved from \textbf{Interpro}. They may be downloaded directly from the \href{https://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/}{ftp site} or via \href{https://www.ebi.ac.uk/interpro/api/entry/pfam/PF00001}{api}. For this dissertation, the API was used and clan information parsed via python regular expressions.

%As as shown in figure \ref{fig:queryclan}, this is easily achieved in python.


\paragraph{Disorder regions} Disorder region information is also available from the \textbf{Interpro} \href{https://ftp.ebi.ac.uk/pub/databases/interpro/current_release/}{ftp site} as the extra.xml file. This dissertation used version 100.0, released in May 2024. This file serves as a 'catch all' for a wide range of protein meta-data not included in the protein2ipr.dat file and is by far the largest of all files to download and process at 188 GB in size and consisting of 4 billion lines. 


\section{Data preparation}
Given the quantity of data to be downloaded and prepared ahead of corpus creation, it was decided to create a local database to hold protein information and the tokens that would be used to create the sentences for the corpus. Although this may have added to the up front data preparation effort, it provided a platform whereby different corpus formats could be generated quickly. 

\paragraph{}To implement this approach, the focus of data preparation was to first parse the raw data files into smaller tab delimited files and load these into a local database (Duck DB). From there the corpus would be created.

\paragraph{Proteins}
Figure \ref{fig:corpus_protein} below shows an example of a single protein entry as it appears in the Uniprot fasta download, highlighting the unique protein identifier (accession number) and the amino acid sequence which need to be extracted. Although the amino acid sequence itself is not required for the corpus, its length is in order to determine start and end points. The protein identifier provides a unique key which is used to 'knit' the various tokens together into a single sentence per protein.

\paragraph{}This information was extracted from the raw Uniprot protein extract using the \textbf{Biopython} library which contains useful methods for quickly extracting protein meta-data from a fasta file.

\paragraph{}The output of this step was a tab delimited file consisting of the protein accession id (protein id) and the sequence length.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_protein_fasta.png}
	\caption[protein\_corpus]{Uniref100 Protein fasta extract, highlighting the areas of relevance for the corpus}
	\label{fig:corpus_protein}
\end{figure}

%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%The protein information was extracted in python using the SeqIO module from the Biopython library. On a Macbook, it took 1,428s (24 minutes) to parse 95,272,305 eukaryotic proteins and extract the id and length of each protein to a csv file.
%\end{tcolorbox}


%
% ----------------------- Methods - PFAM download
%

\paragraph{Pfams} Each entry in the protein2ipr.dat extract from Interpro represents one entry from the underlying signature database mapped to its protein accession number (protein id). For one protein, the extract contains multiple entries, but for the purposes of this dissertation only Pfam tokens are of interest - these will form part of the corpus. The relevant lines can be identified as their tokens start with the characters 'PF' followed by 5 digits \ref{fig:corpuspfam}. The remaining lines are discarded.


% ----------------------- Graphic: PFAM extract
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_pfam}
	\caption{Pfam data extract - relevant data for the corpus}
	\label{fig:corpuspfam}
\end{figure}


% ----------------------- Performance: PFAM extract
%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%	{\small The protein2ipr.dat file, consisting of 1,355,591,115 records was processed using standard python regular expressions. On a Macbook, it took 13,042s (\texttildelow 4 hours) to producing a csv file containing only protein id, pfam id and start and end positions of the pfam domain.}
%\end{tcolorbox}


%
% ----------------------- Methods - Disorder region download
%

\paragraph{Disorder regions}Disorder regions are nested within the extra.xml file from Interpro and identified by the {\small \textbf{MobiDBLite}} attribute. The relevant information is shown in \ref{fig:corpusdisorder} and consists only of the start and end position of each disorder region The protein identifier is also required to provide a foreign key to the protein.

\paragraph{} Despite using efficient XML parsers (ElementTree), initial attempts to parse this file in one go resulted in memory issues - even on larger AWS hardware. Thus this file was first cut into 24 separate and well-formed XML files (each with 10M protein tags) and these were then parsed separately using Python regular expressions to extract the relevant data in python. For maximum speed, the first step of this was implemented in C++ as the Python approach proved too slow.

\paragraph{}The output of this step was a tab delimited file containing the disorder information and the protein identifier.

%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%	{\small It took on average 5mins to chunk a set of 5M proteins from extra.xml (chunk\_disorder\_xml.cpp), and 62 mins to then run the parser and finally 2min 25s to load the disorder information into the database.}
%\end{tcolorbox}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_disorder}
	\caption[corpus metadata]{extra.xml - relevant disorder data for corpus}
	\label{fig:corpusdisorder}
\end{figure}




% --------------------------------------------------------- 
%               CORPUS CREATION
% ---------------------------------------------------------


\clearpage
\section{Corpus creation}
Data preparation produced 3 tab delimited files - one for proteins, one for pfam tokens and another for disorder regions. The unique protein identifier within the lines of each file provided the mechanism for knitting these various pieces of data into sentences for the corpus.

\paragraph{} This process was undertaken in a number of steps. There may be other more efficient ways to achieve the same result, but this approach proved repeatable and efficient, allowing a corpus to be recreated quickly if and when the need arose.

\begin{enumerate}
    \item The tab delimited files were loaded into two database tables
    \item SQL queries then produced a single file with one line per token which were then combined to create one line per protein consisting of all tokens for that protein and their positions along the sequence length
    \item This file was then used to create the final corpus
\end{enumerate}

\paragraph {Database structure}
A fast database called 'duckdb' was used locally to store the data from the 3 tab delimited files. DuckDB uses a columnar-vectorized query execution engine where queries are still interpreted but large batches are processed in one operation. It is easily installed on a Macbook, has a low memory and file-system footprint, has full integration with Python. Notably, DuckDB loads large csv or tab-delimited files in seconds with a single line of python code.

\paragraph{}Two tables were used to hold the data from the tab delimited files - one to hold the protein information (W2V\_PROTEIN) and another to hold both the pfam and disorder details (W2V\_TOKEN) as per below:

\begin{center}
	\begin{tabular}{|p{25mm}|p{105mm}|}
	\hline
	\multicolumn{2}{|c|}{\textbf{W2V\_PROTEIN}} \\
	\hline
	COUNTER&An integer counter to help perform table joins in iterations \\
	\hline
	UNIPROT\_ID&The unique accession id of the protein \\
	\hline
	LENGTH&  Length of the protein sequence \\
	\hline
\end{tabular}
\end{center}

\vspace{5mm}

\begin{center}
	\begin{tabular}{|p{25mm}|p{105mm}|}
	\hline
	\multicolumn{2}{|c|}{\textbf{W2V\_TOKEN}} \\
	\hline
	UNIPROT\_ID& Foreign key reference to the unique protein accession id\\
	\hline
	TYPE&Whether the token is for a pfam entry or a disorder region \\
	\hline
	TOKEN&Identifies the token to be used as a word in a sentence - for a pfam entry this is the pfam id, for a disorder region it is the word 'disorder' \\
	\hline
	START &  The start position of the token along the protein sequence  \\
	\hline
	END &  The end position of the token along the protein sequence  \\
	\hline
\end{tabular}
\end{center}

\vspace{4mm}


%\begin{table}[hbt!]
%\centering
%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm,code={\onehalfspacing},]
%\textbf{Observation/Lesson learned}: By adding a 'COUNTER' to the w2v\_protein table, join queries were extremely quick and allowed 10M proteins and their associated tokens to be processed in 30s. \\ \\ An alternative approach using the inbuilt 'group by' and 'count' keywords of the SQL language did not perform - these instructions cause the entire dataset to be loaded first into memory prior to applying any group by, count or ordering clauses; this causes memory issues and is extremely slow - taking up to 1 hour to process 10M rows, with the duration increasing with each querty . \\ \\ By contrast, adding a COUNTER column (integer from 0 increasing by 1 for each subsequent row), allows queries to be 'paginated' - each query only returns the rows corresponding to the COUNTER values requested. The COUNTER values can be tracked in python as each query returns. \\ \\ 
%The SQL used is shown below with the 'start' and 'end' variables tracked with python code.\\ \\
%	{\small {SELECT T1.UNIPROT\_ID, T1.TOKEN, T1.TYPE, T1.START, T1.END FROM W2V\_TOKEN T1 WHERE UNIPROT\_ID IN ( SELECT UNIPROT\_ID FROM W2V\_PROTEIN T2 WHERE T2.COUNTER \textgreater= start and T2.COUNTER \textless end ORDER BY T2.COUNTER)}}
%    \end{tcolorbox}

%\caption{SQL join to extract pfam and disordered 'tokens' for eukaryotic proteins}
%\end{table}

\paragraph{}A database join across these two tables produced an output file with one line per token per protein, these lines are subsequently combined using standard python to create a \textbf{pre-corpus}\ref{fig:corpusmetadata} meta-data file, with each line containing all the information required to produce a sentence for a protein. 

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_metadata}
	\caption[corpus metadata]{Pre-corpus metadata - all possible tokens for a protein}
	\label{fig:corpusmetadata}
\end{figure}

\paragraph{}\textit{This meta-data file provides the foundation for flexible creation of a corpus. Each entry contains all the information required to create a wide range of corpi reflecting different requirements (e.g. creating a corpus from sequences with only a certain number of tokens). In this dissertation, this format supported the creation of corpi with different criteria by which GAP words are inserted into the final sentences.}

\paragraph{}The final corpus was created using the meta-data according to the following rules:
\begin{itemize}
    \item Where regions overlapped (identified by their start and end positions) only one token was kept - where a disorder region and protein domain overlapped, preference was given to the pfam token. 
    \item Where there was no token along a stretch of sequence, the word '\textbf{GAP}' was used. For the initial set of experiments, the word 'GAP' was inserted whenever two tokens were separated by even one amino acid. Further experiments varied this such that the word '\textbf{GAP}' would only be used if the gap was either 50 or 100 characters or more in length.
    \item For gaps at the start and end of the sequence, the words '\textbf{START\_GAP}' and '\textbf{END\_GAP}' were used to provide some differentiation.
\end{itemize}


\paragraph{} As an example, combining all the information from the previous sections, the resulting sentence for the protein '\textbf{A0A010Q340}' is shown below. 
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=1.0\linewidth]{images/corpus_line}
	\caption{Example of the sentence for protein A0A010Q340 as it appears in the final corpus}
	\label{fig:corpusline}
\end{figure}

\vspace{2mm}
\begin{center}\textbf{The final corpus consisted of 50,894,561 sentences.}
\end{center}




% --------------------------------------------------------- 
%               MODEL CREATION
% ---------------------------------------------------------
\clearpage
\section{Model creation}
The \textbf{gensim} python library was used to create models. This is directly based upon the paper by Mikolov at al. \cite{word2vecoriginal} and supports the cbow and skipgram architectures as well as all the hyperparameters to configure how the embeddings are created.


%\paragraph{}For our purposes a number of different models were created for each of the CBOW and Skip-Gram variants using different values for these hyper-parameters:


\subsection{Selection of hyper-parameters}
The corpus meta-data was analysed to determine an appropriate range of values for the \textbf{minimum count} and \textbf{window size} hyperparameters.

\begin{table}[hbt!]
\centering
\label{tab:sentenceanalysis}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
	\hline
	metric & max & min & mean & std dev & 90th & 95th & 97.5th & 99th\\
	\hline
	Total tokens per sentence & 3,991 & 1 & 2.83 & 3.91 & 6.0 & 9.0 & 12.0 & 18.0 \\
	Pfam tokens per sentence & 3,991 & 0 & 1.79 & 2.64 & 3.0 & 5.0 & 7.0 & 10.0 \\
	Disorder tokens per sentence & 285 & 0 & 1.04 & 2.91 & 3.0 & 6.0 & 9.0 & 13.0 \\
	\hline
\end{tabular}
\caption{Analysis of token distribution within the corpus sentences}
\end{table}

The key points with respect to model creation are:
\begin{itemize}
    \item Outlier sequences exist whose sequences consists solely of pfam tokens - the longest having 3,991 tokens in total (all pfam tokens). These were kept as they have a biological basis which could be important for embedding
    \item For \textbf{minimum word count}, disorder regions and gap tokens will appear multiple times in the corpus, so this hyperparameter really pertains to the frequency of pfam tokens. The view was again taken that any Pfam token contains important information for embedding and should be included. The minimum for this hyperparameter was thus set at 1 with an arbitrary higher value of 8 which to allow for comparison.
    \item For determining \textbf{window size} the length of each sentence is important. A  lower window size results in more training iterations per sentence and thus more opportunity for weights to be modified. The 99th percentile sentence length is 18 tokens, but this excludes GAPs. Assuming a worst case scenario with as many gaps as disorder and pfam tokens, the 99th percentile length would be 36. For the initial experiments, the window size was thus varied in a range from 3 to an arbitrary maximum slightly higher than 36 - at 44. Further changes could be made dependent upon those results.
\end{itemize}

For the \textbf{vector size}, the generally accepted vector size is 100 \cite{word2vecoriginal}, it was decided for the first set of experiments to use a combination of values up to that point to see how the vector embeddings determined the clustering ability. Later experiments look at higher  dimensions.


\paragraph{}In summary,the following model configurations were created - each of these for both CBOW and Skip-Gram word2vec variants.

\begin{table}[hbt!]
\centering
\label{table_corpus_model}
\begin{tabular}{|l|c|c|c|c|c|c|}
	\hline
	parameter & value 1 & value 2 & value 3 & value 4 & value 5 & value 6 \\
	\hline
	min word count & 1 & 3 & 5 & 8 & - & - \\
	window size & 3 & 5 & 8 & 13 & 21 & 44 \\
	vector size & 5 & 10 & 25 & 50 & 75 & 100 \\
	\hline
\end{tabular}
\caption{Hyperparameters used in word2vec models given 144 combinations per architecture}
\end{table}


\vspace{50mm}


% --------------------------------------------------------- 
%               DISTANCES
% ---------------------------------------------------------

\section{Distance correlation to find the 'best' candidate model}

%\label{distancematrix}
%As we are trying to find biological meaning in the data, the best model to take forward is determined by correlating the encodings of the vocabulary from the word2vec model with a supplied similarity matrix prepared directly from pfam sequences.

%The embeddings of each word2vec model contain the vector representation of each word in the model's vocabulary - including all Pfam domain tokens as well as the words GAP and DISORDER. Each vector represents the coordinates of that word in multi-dimensional space.

\paragraph{}To create pairwise distance matrices for a model, the distance from each Pfam token to all other words is calculated. The Pfam token embeddings are extracted using the gensim API and the pairwise distances calculated using Scikit learn's sklearn.metrics.pairwise module's cosine\_distances and euclidean\_distances functions (one matrix for each). 

\paragraph{}The resulting dimensions of the word2vec distance matrices is determined by the size of the vocabulary, which in turn is dependent upon the minimum count hyperparameter used in model creation. Table \ref{tab:table_vocab_size} illustrates how the vocabulary size changes with the minimum-count parameter; the largest being 15,481 words corresponding to the models with a minimum-count hyperparameter value of 1.

\begin{table}[hbt!]
\centering
\begin{tabular}{|c|c|}
	\hline
	w2v hyperparameter & resulting w2v vocabulary size \\
	\hline
	1 & 15,481 \\
    3 & 13,535 \\
    5 & 12,815 \\
    8 & 11,884 \\
	\hline
\end{tabular}
\caption{Vocabulary sizes for each word2vec min\_count hyperparameter.}
\label{tab:table_vocab_size}
\end{table}

The supplied representative Pfam matrix has 20,651 rows and columns - thus prior to performing correlations with the word2vec distance matrices, some re-alignment of data was necessary to ensure both matrices had the same elements for comparison and in the same order. As illustrated in figure \ref{fig:distance_matrices} this consisted of:


%\paragraph{Creating Distance Matrices for word2vec models}
%A distance matrix simply contains the calculations of pairwise distances between vectors in vector space. Thus for the word2vec model with a minimum word count of 1, the pairwise distance matrix has 15,485 x 15,485 = 233,785,225 entries although the diagonal entries are 0.0 and the matrix is symmetrical.

%\paragraph{}There are two ways of calculating vector distances - euclidean and cosine. Euclidean distance would appear to be the better measure considering the nature of the problem, but out of interest, both measures were calculated for each model.

%\begin{table}[hbt!]
%\centering
%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%	{\textbf{Euclidean distance}: Measures the point to point distance between to points in space. \\
% \textbf{Cosine distance}: Measures the angular distance between tow vectors. If two vectors point in the same direction they will be 'closer' together by this measure.}
%    \end{tcolorbox}
%\caption{Euclidean v Cosine distance measures}
%\end{table}

\begin{itemize}
    \item Removing rows and columns from the representative Pfam matrix for Pfam entries that do not exist in the word2vec vocab
    \item Doing the same for the word2vec matrices - remove the entries that are not in representative Pfam matrix
    \item Make sure that the order of the matrices are the same (i.e. that the common Pfam entries appear in the same rows/columns within each matrix
\end{itemize}

This re-alignment exercise was performed using the \textbf{skbio} python library which contains a useful set of functions for creating and comparing different distance matrices. Both matrices are first wrapped into skbio \textbf{DistanceMatrix} objects and once in that format, skbio automatically re-aligns them; the \textbf{Pearson}, \textbf{Spearman} and \textbf{Mantel} tests are conducted directly on these objects. All of these statistics have fast implementations in the \textbf{scipy} python library - it was expedient to create all 3 in a background task.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/distance_matrices.png}
    \caption{Preparing distance matrices for comparison}
    \label{fig:distance_matrices}
\end{figure}






% ----------------------------------------------------------
%
% ------------ METHODS -   CLUSTERING     ------------------
%
%-----------------------------------------------------------



\section{Clustering}
The final step in methods was to cluster the word2vec embeddings. The objective being to determine whether the word embeddings of Pfam tokens form natural groups within multi-dimensional space that mirror the Pfam to Pfam clan groupings in real life. 

\paragraph{}The mapping of Pfam clans to Pfam families is incomplete; many clans have single members and many pfams have no clan. Therfore, prior to clustering, these Pfam entries were removed from the dataset as they may represent unclustered families.

\paragraph{}The KMeans algorithm requires a hyperparameter 'K' that determines the number of clusters to be created. This was set to the number of clans in the resulting dataset. Clustering was performed using the \textbf{scikit learn} implementation of KMeans. 

\paragraph{}To measure the success of the resulting clusters, each of the K clusters were compared with each of the clans to determine the level of overlap. For this the \textbf{Jaccard} index was used.



\subsubsection{Preparation - Selection of Pfam clans and families}
% --------------- KMeans - Finding clans prep
\paragraph{} The graphic below \ref{fig:clansizes} shows the sizes of the larger clan families that encompass the pfam domains within the word2vec models. As can be seen, after the first 15 to 20 largest clans, there is a very long tail of smaller clans. The minimum clan size was thus used as a parameter when deciding with vectors to include in the clustering exercises \todo{todo: reference these results}. 

\begin{figure} [hbt!]
    \centering
    \includegraphics[width=1.0\linewidth]{images/clans.png}
    \caption{Count of pfams in each clan}
    \label{fig:clansizes}
\end{figure}


\begin{table}[hbt!]
\centering
\label{tab:pfam_clans2}
\begin{tabular}{|l|l|}
	\hline
	number of pfams per clan & number of clans \\
	\hline
	1 & 689 \\
    2 & 631 \\
    3 & 479 \\
    5 & 293 \\
    8 & 197 \\
    10 & 147 \\
	\hline
\end{tabular}
\caption{Number of clans according to different clan sizes (number of pfam entries in that clan)}
\end{table}

\paragraph{}Selecting a minimum clan size required filtering out the word vectors for pfams that were not part of a clan of a sufficient size as per \ref{fig:kmeansclanprep}
\\
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/kmeansprocess.png}
    \caption{Reducing the number of Pfam vectors to cluster if they are not part of a large enough Pfam clan}
    \label{fig:kmeansclanprep}
\end{figure}

%The approach adopted is summarised in figure\ref{fig:clustering}.
%\begin{figure}[hbt!]
%    \centering
%    \includegraphics[width=1\linewidth]{images/clustering.png}
%    \caption{Clustering of pfam vectors and comparison with pfam clans}
%    \label{fig:clustering}
%\end{figure}



\clearpage
\section{Technical Architecture}

\subsection{Software}
The following software packages were used:

\begin{table}[hbt!]
\label{tab:software}
\begin{tabular}{|p{70mm}|p{70mm}|}
    \hline
	\textbf{Area} & \textbf{Specification} \\
	\hline
    Operating System (Macbook) & Ventura 13.6.9 \\
    Operating System (AWS) & Amazon Linux 2013 \\
    Environment management (Macbook) & conda 23.7.4 \\
    Environment management (AWS) & venv \\
    Code management & github \\
    Coding IDE & Visual Studio Code  \\
    \hline
    Database (Macbook) & python-duckdb 1.0.0 \\
    Database (AWS) & Amazon MySQL RDS \\
    \hline
    Coding  & python 3.11.5 (unless otherwise specified)\\
    \hline
    Uniprot fasta parse & biopython 1.83 (SeqIO package) \\
    Breaking up extra.xml file & C++ 11, g++ compiler on Mac \\
    XML parsing to extract disorder information from extra.xml & xml.etree.ElementTree \\
    \hline
    Word2Vec implementation & gensim 4.3.2 \\
    Pairwise distance calculation of w2v vectors & sklearn.metrics \\
    Distance matrix creation & scikit-bio 0.6.2 \\
    Pearson and Spearman correlations & scikit-bio 0.6.2 \\
    KMeans clustering & scikit-learn 1.5.1 \\
    \hline
    Graphing libraries  & seaborn 0.13.2 , matplotlib 3.8.4 \\
    \hline
\end{tabular}
\caption{Software \& platform specifications}
\end{table}


\subsection{Infrastructure}
Apart from model creation, all tasks were undertaken locally on a Macbook. For model creation, it was decided to make use of AWS cloud infrastructure to allow execution to continue in the background. Specifically the following AWS services were provisioned using the \textbf{Terraform} Infrastructure as Code tool:

\begin{itemize}
    \item \textbf{EC2}: EC2, or Elastic Cloud Compute, is the service for traditional 'compute' (infrastructure servers) with a cpu and operating system. 4 separate AWS EC2 instances were used for model creation, each of type \textbf{t3.2xlarge}
    \item \textbf{EBS}: Elastic Block Store is analagous to a traditional disk drive. Each EC2 instance had a separate \textbf{150GB st1} volume attached.
\end{itemize}



%\subsubsection{Observations - Amazon Web Services (AWS)}
%\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
%As a side note, the following observations were made about the use of AWS

%\begin{itemize}
%    \item Overall this is a great service to use if jobs need to be run in parallel or simply behind the scenes. With some existing knowledge of infrastructure, it's relatively easy to learn the basics - there are plenty of online tutorials provided by AWS themselves. However, it is easy to get lost in the vast array of services and terminology. 
%    \item Once a server is running it's dedicated. There's no guesswork about availability and no queuing for a slot. Although most services are charged, there are free tiers available if performance is not a major concern. The EC2 instances for creating the word2vec models cost \$0.36 per hour.
%    \item The AWS Control Panel online console is not the easiest interface for provisioning services - it's much easier to create and destroy infrastructure using an \textbf{Infrastructure as Code} configuration tool such as \textbf{Terraform}. One hour of learning is sufficient to learn Terraform to allow the basic AWS services to be configured and provisioned. Services can then be created and destroyed from a single shell script command rather than clicking through multiple online forms.
%    \item The costs of AWS services are not always transparent and the novice user can easily run up larger than expected bills, simply by forgetting to stop one of the services. The use of Terraform helps to prevent this, as it's very easy to destroy infrastructure nightly, when not used. EBS instances can be backed up (snapshot'ed) each evening and destroyed, then recreated from the snapshot the next time they are needed
%    \item In the time available, some of the serverless services were investigated (AWS lambda and Amazon Glue). The benefit of these services is that they're billed only according to how much they're actually used. However, the costing was not very transparent and it was difficult to get the configuration correct to get them to perform. Thus the more traditional services (EC2, EBS) were used instead.
%\end{itemize}

%\textit{Note: All the Terraform code is available in the github repository in the code/terraform directory}.

%\end{tcolorbox}


% ----------------------------------------------------------------
%             Chapter 3 - Experiments
% ----------------------------------------------------------------
\chapter{Experiments}
This dissertation creates a wide variety of word2vec models, effectively using a Grid Search of hyperparameters to investigate which combination of hyperparameters produce embeddings and clusters of embeddings that contain biological meaningful information.

%find the 'best' set of embeddings. It makes the assumption that the model whose distance matrix is 'closest to' (has the highest correlation with) the provided \textbf{representative pfam distance matrix}, is more likely to have word vectors that contain some biologically meaningful information. 
%\paragraph{} For the purposes of this dissertation, '\textbf{biologically meaningful}' is determined by how successfully a clustering algorithm, when provided with the word2vec embeddings from the 'best' model, finds clusters of Pfam domain words that also align with real Pfam clans.\\

\paragraph{}To investigate this, the following experiments were conducted.

\begin{enumerate}
    \item Using an initial corpus many word2vec models were created, each using different combinations of hyperparameters and architectures (cbow v skipgram). For each set of embeddings, a distance matrix was created containing pairwise distances between each word embedding and comparing that with the provided representative pfam distance matrix. The model with the closest correlation was considered the best model for clustering (using Pearson and Spearman measures).
    \item Running the \textbf{KMeans} algorithm with those embeddings and comparing the output K clusters to the Pfam clan groups using Jaccard similarity to find overlaps
    \item Analyse the output to motivate the creation of other models or corpi with different configurations and repeat 1 and 2 to see which combinations produce the best results
\end{enumerate}






\newpage
\section{Experiments I - W2V Hyperparameter search and KMeans clusters}

\subsection{Word2Vec Models}
The initial experiments were conducted with the model configurations shown in table \ref{gap1hyperparams}, resulting in 240 different word2vec models. 

\begin{table}[hbt!]
\centering

\begin{tabular}{|l|l|}
	\hline
	metric & values  \\
	\hline
    corpus GAP size * & 1  \\
    model architecture & cbow and skipgram   \\
    model minimum word count & 1, 3, 5, 8   \\
    model window size & 3, 5, 8, 13, 21, 44   \\
    model vector size & 5, 10, 25, 50, 100   \\
	\hline
\end{tabular}
\caption{Model configurations for first round of experiments}
\label{gap1hyperparams}
\end{table}

\subsection{Distance matrix comparison results}
The word vectors for each of the 240 w2v models were extracted and pairwise distances were calculated using both Cosine and Euclidean formula; the resulting distances were normalised and each matrix (2 for each model) was correlated with the representative pfam distance matrix using both a pearson and spearman metrics. 

\subsubsection{Results}
\paragraph{}The top 10 results for all models using the Pearson and Spearman correlations are shown in table \ref{table_dist_results_pearson_g1} below. Note that the Mantel test metric was also calculated but this produced the same results as either Pearson or Spearman (depending upon which metric was used in the Mantel calculation), thus these results are not shown. All metrics had very low p-values indicating that the metrics are confident of the low correlations in the results, this is also not shown\\

\begin{table}[hbt!]
\centering
\label{table_dist_results_pearson_g1}
\begin{tabular}{|l|c|c|c|c|c|}
	\hline
	model type & min word count & window size & vector size & distance type & pearson \\
	\hline
	cbow & 8 & 13 & 5 & euc  & 0.0841 \\
    cbow & 8 & 44 & 5 & euc  & 0.0827 \\
    cbow & 8 & 21 & 5 & euc  & 0.0824 \\
    cbow & 5 & 44 & 5 & euc  & 0.0823 \\
    cbow & 8 & 8 & 5 & euc  & 0.0817 \\
    cbow & 5 & 8 & 5 & euc  & 0.0816 \\
    cbow & 5 & 21 & 5 & euc  & 0.0813 \\
    cbow & 8 & 5 & 5 & euc  & 0.0807 \\
    cbow & 3 & 13 & 5 & euc  & 0.0787 \\
	\hline
\end{tabular}
\caption{Initial experiments - Top 10 Distance Matrix correlations using Pearson}
\end{table}


\begin{table}[hbt!]
\centering
\label{table_dist_results_pearson g1}
\begin{tabular}{|l|c|c|c|c|c|}
	\hline
	model type & min word count & window size & vector size & distance type & spearman \\
	\hline
	skip & 8 & 44 & 25 & euc  & 0.0914 \\
    skip & 5 & 21 & 25 & euc  & 0.0894 \\
    skip & 8 & 21 & 25 & euc  & 0.0893 \\
    skip & 3 & 44 & 25 & euc  & 0.088 \\
    cbow & 8 & 13 & 5 & euc  & 0.088 \\
    skip & 8 & 13 & 25 & euc & 0.087 \\
    cbow & 8 & 21 & 5 & euc  & 0.087 \\
    cbow & 8 & 8 & 5 & euc  & 0.0865 \\
    cbow & 8 & 5 & 5 & euc  & 0.0863 \\
    cbow & 5 & 8 & 5 & euc  & 0.0862 \\
	\hline
\end{tabular}
\caption{Initial experiments - Top 10 Distance Matrix correlations using Pearson}
\end{table}
\pagebreak

These results are surprising - there is very little correlation between the two distance matrices with the maximum correlation being only \textbf{0.0914} using the spearman test and the skipgram model with the configuration : [min word count : 8, window size : 44, vector size :  25]). Reasons for this are discussed in the Conclusion.


\paragraph{}Some patterns do emerge from these experiments that motivated further experiments in the next iteration:
\begin{enumerate}
    \item There was no best model - with the exception of 1 model, all the test metrics were very close to each other - in the region between 8\% and 9\% correlation. The only model outside of that range had a statistic of 7.8\%, just under the others.
    \item None of the distance matrices in the top 10 were created using cosine distances
    \item The Pearson metric clearly preferred the cbow models whereas the spearman metric was a bit more mixed (although the top 4 were all skipgram models)
    \item Word2Vec models with a minimum word count of 8 dominate the results (12 instances), 5 instances in the top 20 have a word count of 5. None of the top models were created with a minimum word count of 1 - although that has the largest vocabulary (highest number of Pfam domains).
    \item Window sizes are more mixed, although the larger windows (13, 21, 44) feature more than the smaller window sizes (8, 5) in the top 5 results by each test statistic
    \item Regarding vector sizes, the Pearson metric clearly performed better on the smaller vector sizes, spearman on 5 or 25; no metric favoured the larger 50 or 100 vector sizes - even though 100 is usually the default for Word2Vec models.
\end{enumerate}


\begin{tcolorbox}[width=\linewidth, sharp corners=all, colback=white!95!black, colframe=white, boxrule=0.2mm, leftrule=0.2mm, rightrule=0.2mm]
The original expectation was that there would be one clear winner and that would be taken forward to identify clusters. Instead, with no obvious candidate, the next step of the experiments were conducted using a selection of the models to cover different hyperparameters and model types.
\end{tcolorbox}



% --------------- KMeans


\subsection{KMeans clustering results}
Given the inconclusive results from the distance matrix comparison, it was decided to use the embeddings from a range of models in KMeans clustering experiments. This covered the case where there might be a model that doesn't correlate well with the representative Pfams but which might still provide good clustering.


\begin{table}[hbt!]
\centering
\label{tab:modelsforkmeans}
\begin{tabular}{|lp{10cm}|lp{30cm}|}
	\hline
	\textbf{Reason} & \textbf{Model name (vectors to use for KMeans clustering)}\\
	\hline
	\textbf{Highest overall pearson} &  w2v\_20240911\_cbow\_mc8\_w13\_v5 \\
 \hline
    \textbf{Highest overall spearman} &  w2v\_20240910\_sg1\_mc8\_w44\_v25  \\
    \hline
   \textbf{Highest cbow model type}&  w2v\_20240911\_cbow\_mc8\_w13\_v5 	 \\
    \hline
    \textbf{Highest skipgram model type}& included above \\
    \hline
    \textbf{Highest of each min word count} &  w2v\_20240911\_sg1\_mc1\_w21\_v25,  w2v\_20240910\_sg1\_mc3\_w44\_v25, w2v\_20240911\_skip\_mc5\_w21\_v25, (mc8 already included)  \\
    \hline
    \textbf{Highest of each window size }&  w2v\_20240911\_cbow\_mc8\_w5\_v5, w2v\_20240911\_cbow\_mc8\_w8\_v5, (others already included) \\ 
    \hline
    \textbf{Highest of each vector size} &  w2v\_20240911\_cbow\_mc8\_w3\_v10,  w2v\_20240911\_skip\_mc5\_w44\_v50 ,  w2v\_20240910\_sg1\_mc8\_w44\_v100 \\
	\hline
\end{tabular}
\caption{Models whose embeddings were included in clustering experiments}
\end{table}


%\subsubsection{Selection of 'K' for K Means}
%One of the disadvantages of KMeans clustering, is that it must be provided with a value for 'K' - the number of clusters to find. Given the objective is to find clusters of clans and that we know how many clans there should actually be for a set of encoded Pfam domains, it was decided that, for each test, K would be set to equal the number of clans encompassed by the Pfam word vectors in the current set (this is not the same for each model due to the minimum count hyperparameter). 


\paragraph{} Each of the selected models was run through a KMeans clustering algorithm. Initially all Pfam embeddings were clustered as long as they belonged to clans with at least two Pfam members. 600 clans met that criteria and with K set to 600 the best result as measured by the Jaccard similarity was 0.005. Subsequent experiments were then executed changing the minimum clan sizes from 2 up to 50 with corresponding changes in the value for K. The top results per minimum clan size are shown in table \ref{kmeansg1results}.\\

\begin{table}[hbt!]
\centering

\begin{tabular}{|p{60mm}|c|c|p{45mm}|}
\hline
	model & min clan size & K & Average Jaccard similarity\\
\hline
w2v\_20240910\_skip\_mc3\_w44\_v25\_g1  &  2  & 600  & 0.0005  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v25\_g1  &  10  & 113  & 0.003  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v25\_g1  &  25  & 47  & 0.0083  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v25\_g1  &  50  & 20  & 0.0208  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v25\_g1  &  100  & 7  & 0.0749  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v100\_g1  &  200  & 4  & 0.1427  \\
\hline
w2v\_20240910\_skip\_mc8\_w44\_v100\_g1  &  250  & 2  & 0.3355 \\
\hline
\end{tabular}
\caption{Initial Experiments 1 - Jaccard similarities for clusters for different clan sizes}
\label{kmeansg1results}
\end{table}

As can be seen, as the minimum clan size criteria is increased from 2 to 250, the number of clans that meet that criteria reduces decrease from 600 to only 2. But even with only 2 clusters to compare, both with 250 or more pfams, the correlation is very low - at only 33\%.

\vspace{10mm}
\subsection{Experiments I - Analysis}
To analyse the results and try to explain correlations, visual analysis of the vector space was undertaken by plotting the first 3 Principal Components of different model configurations. These plots are colour coded according to the correct clan each word embedding actually belongs to.

\subsubsection{Analysis 1 - Principal Components}The plots in \ref{fig:0910g1kmbestc2} show all the vectors from the 2 'best' models (according to the distance matrix comparison). It is visually clear that all the vectors are very tightly packed in space, making it difficult for distance-based clustering algorithms to separate them.


\begin{figure} [hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/0910g1_km_best_c2.png}
    \caption{First 3 Principal components for vectors of the 2 'best' models}
    \label{fig:0910g1kmbestc2}
\end{figure}

\paragraph{}By plotting only larger clans as in figure \ref{fig:0910c150} it's easier to analyse visually how the embeddings are distributed in space. The pattern remains the same (only skipgram shown) - although there is some distribution of points, the centroids are each clustered together into one confined space making separation of one cluster from another very difficult.
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.75\linewidth]{images/0910g1_c150.png}
    \caption{Plotting only large clans}
   \label{fig:0910c150}
\end{figure}

%\begin{wrapfigure}{l}{8cm}

%\includegraphics[width=7cm]{images/0910g1_c150.png}
%\caption{Plotting only large clans}\label{wrap-fig:1}
%\end{wrapfigure} This is even clearer if we plot only those words that %belong to a small number of larger Pfam clan. This pattern is repeated %through all the top 10 models (not shown to save space). 
\vspace{50mm}

\subsubsection{Analysis 2 - cbow v skipgram}Side by side plots of the continuous bag of words and skipgram models also show similar results as per figure \ref{fig:0910g1cbowvskip}. However, the CBOW model does tend to  mass all points into an area that is slightly more densely populated than a skipgram model created with the same hyperparameters. This is more apparent in the second plot below where some of the clans have been removed to aid visual explanation.

\begin{figure} [hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/0910g1_cbow_v_skip.png}
    \caption{First 3 Principal components for cbow v skipgram - all clans}
    \label{fig:0910g1cbowvskip}
\end{figure}

\begin{figure} [hbt!]
    \centering
    \includegraphics[width=0.9\linewidth]{images/0910g1_km_v50_c150.png}
    \caption{First 3 Principal components for cbow v skipgram w/ clan size of 150}
    \label{fig:0910g1kmv50c150}
\end{figure}

\vspace{80mm}

\subsubsection{Analysis 3 - vector sizes}The next set of plots compare different vector sizes to see if higher dimensions might better separate the vectors. The plot below \ref{fig:0910g1v25v100} keeps all other inputs the same but the vector size is varied between 25 and 100. There is no major difference, although the higher vector size appears to spread the points over a slightly wider area.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.8\linewidth]{images/0910g1_v25_v_v100.png}
    \caption{First 3 Principal components for skipgram w/ different vector sizes}
    \label{fig:0910g1v25v100}
\end{figure}

\vspace{40mm}

\subsubsection{Analysis 4 - different K values} The last set of plots compare how different values of 'K' for the KMeans clustering algorithm influence the results. Initially K was set to the same number of clans whose pfam vectors were to be clustered. In a further set of experiments, the K value was changed to be 50\% and 75\% of the number of clans. This was to try and force the model to create better clusters. The results are plotted on a Heatmap 
showing the pairwise correlation (as measured by Jaccard) of different clusters identified by the KMeans algorithm (Y axis) with the different Pfam clans along the horizontal (labelled at the top).

\paragraph{}Within figure \ref{fig:changingk}, the plot on the left shows the heatmap with K set to the same size as the number of clans, the plot on the right when it is set to 75\% of the number of clans. Reading across the rows (each of which represent a cluster found by KMeans) there are stronger relationships between K clusters and clans with K set at 100\% compared to 75\% . Visually more rows also show a stronger correlation with only one clan. This suggests that performance is slightly improved when the K value is set to equal the number of clans.

\begin{figure} [hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/0910g1_jaccardmatrix.png}
    \caption{Heatmap showing the impact of different K values in finding clusters correlating to clans according to the Jaccard index. K=100\% on the left, 75\% on the right)}
    \label{fig:changingk}
\end{figure}

\vspace{100mm}

\subsection{Experiments I - Conclusions}
From the first set of experiments, it's clear that none of the models produced Pfam embeddings that cluster to mirror their associated Pfam clans. There was also no clear correlation between the 'best' model according to distance matrix comparisons and the ability to cluster.

That said some very slight differences emerge that are taken into account in the next set of experiments :

\begin{itemize}
    \item The cbow models appear to more densely pack their vectors
    \item A higher vector size produces slightly more separation of embeddings
    \item Keeping the value of K in KMeans equal to the clan size slightly improves results
\end{itemize}

\vspace{100mm}

\newpage

\section{Experiments II - Changed corpus and higher dimension embeddings}

\subsection{Configuration changes}
Based upon the outputs of the first set of experiments, a more fundamental change was made for the next set of experiments; the corpus was modified to reduce the number of GAP words. Also, larger vector sizes were provided as hyperparameters to the word2vec models. The new corpus was created using the same input data but with the condition that the word "GAP" would only be inserted into a sentence if the distance between the start and end of consecutive tokens (disorder or pfam domain) was greater than or equal to 50.\\

Reducing the number of GAP words should, in theory, ensure that Pfam word embeddings are less influenced by GAP words and fundamentally change the embeddings. Increasing the vector size to include dimensions of 250 and 500 may also create more 'space' for clusters to be found.

\paragraph{Model configurations}
The resulting model configurations used for the second set of experiments are shown below:
\vspace{10mm}
\begin{table}[hbt!]
\centering
\label{gap50hyperparams}
\begin{tabular}{|l|l|}
	\hline
	metric & values  \\
	\hline
    corpus GAP size & 50  \\
    model architecture & cbow and skipgram   \\
    model minimum word count & 3, 5, 8   \\
    model window size & 13, 21, 44   \\
    model vector size & 5, 25, 50, 100, 250, 500   \\
	\hline
\end{tabular}
\caption{Model configurations for Experiments II}
\end{table}


\vspace{100mm}
\subsection{Distance matrix comparison results}

When correlated against the representative pfam distance matrix, again using both spearman and person measures, the results of this configuration are only marginally better than the default models using a gap size of 1 - as per table \ref{table_dist_results_pearson_g50}.


\begin{table}[hbt!]
\centering

\begin{tabular}{|c|c|c|c|c|c|}
	\hline
	min word count & window size & vector size & pearson & spearman \\
	\hline
    8 & 44 & 25 & 0.0847 & 0.1092 \\
    8 & 21 & 25 & 0.082 & 0.1063  \\
    5 & 21 & 25 & 0.0812 & 0.1052  \\
    3 & 44 & 25 & 0.0817 & 0.1051  \\
    8 & 13 & 25 & 0.081 & 0.1051  \\
    5 & 44 & 25 & 0.0796 & 0.1033  \\
    3 & 13 & 25 & 0.0796 & 0.1032  \\
    3 & 21 & 25 & 0.0779 & 0.1018  \\
    5 & 13 & 25 & 0.0781 & 0.1017  \\
    8 & 44 & 50 & 0.0745 & 0.0984  \\
	\hline
\end{tabular}
\caption{Top 10 Distance Matrix correlations with minimum GAP distance in the corpus of 50 spaces}
\label{table_dist_results_pearson_g50}
\end{table}

One difference with the intial experiments is that the top 10 correlations via the pearson and spearman metrics are the same for all model configurations (and the top 10 are in the same order). Also, all the top 10 models are all of type skipgram, compared to the initial experiments where the top 10 correlations were distributed across a mixture of cbow and skipgram model architectures. Despite creating models with larger vector sizes (100, 250 and 500), it is notable that no models with these larger vector sizes feature in the top 10.

\subsection{KMeans clustering \& results}
Repeating the KMeans clustering with the embeddings from these 10 models and again varying the K hyperparameter according to different clan sizes, the results indicate that this new configuration did not change the ability of KMeans clustering to find correlations with Pfam clans.

\paragraph{} The results, listed in table \ref{tab:kmeansg50results} show the best correlations for different minimum clans sizes (ranging from 2 to 150). For comparison, models with larger vector sizes (50, 100, 250, 500) are also shown in the bottom half of the table. Even with only 7 clusters, the highest correlation with Pfam clans was 0.0741 (vector size of 50) compared to 0.0749 in the first set of experiments. As a visualisation, the Heatmap of the best correlation is again shown in \ref{fig:jaccardexp2}. More of the clusters overlap but the maximum similarity across the entire set of comparisons is still very low at c. 20\% (last two rows). 

\begin{table}[hbt!]
\centering

\begin{tabular}{|p{60mm}|c|c|p{45mm}|}
\hline
	model & min clan size & K & Average Jaccard similarity\\
\hline

w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  2  & 565  & 0.0005 \\
w2v\_20240923\_skip\_mc8\_w21\_v25\_g50  &  50  & 20  & 0.0217 \\
w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  10  & 108  & 0.0032 \\
w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  25  & 44  & 0.0089 \\
w2v\_20240923\_skip\_mc8\_w21\_v25\_g50  &  50  & 20  & 0.0217 \\
w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  100  & 7  & 0.0735 \\
w2v\_20240923\_skip\_mc8\_w44\_v25\_g50  &  150  & 5  & 0.1075 \\
\hline
\hline
w2v\_20240923\_skip\_mc8\_w44\_v50\_g50  &  100  & 7  & 0.0741 \\
w2v\_20240923\_skip\_mc8\_w44\_v50\_g50  &  150  & 5  & 0.103 \\
\hline
w2v\_20240923\_skip\_mc8\_w44\_v100\_g50  &  100  & 7  & 0.682 \\
w2v\_20240923\_skip\_mc8\_w44\_v100\_g50  &  150  & 5  & 0.0902 \\
\hline
w2v\_20240923\_skip\_mc8\_w44\_v250\_g50  &  100  & 7  & 0.0591 \\
w2v\_20240923\_skip\_mc8\_w44\_v250\_g50  &  150  & 5  & 0.0686 \\
\hline
w2v\_20240923\_skip\_mc8\_w44\_v500\_g50  &  100  & 7  & 0.0232 \\
w2v\_20240923\_skip\_mc8\_w44\_v500\_g50  &  150  & 5  & 0.0931 \\
\hline
\end{tabular}
\caption{Experiments 2 - KMeans clustering results for different clan sizes}
\label{tab:kmeansg50results}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/0920g50_jaccardheatmap.png}
    \caption{Experiments II - Jaccard similarity matrix - KMeans clusters v Pfam clans}
    \label{fig:jaccardexp2}
\end{figure}

\clearpage
\subsection{Experiments II - Analysis}
To complete the analysis of the second set of experiments, PCA plots again were created to visually compare different configurations. Representative plots are shown below.

\paragraph{cbow v skip} The higher dimensional vectors start to show a difference between the cbow and skipgram models - with the cbow models producing very tightly bound clusters.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.95\linewidth]{images/0920_g50_cbow_v_skip.png}
    \caption{First 3 Principal Components for cbow and skip with a corpus rule of 50 and vector size of 500}
    \label{fig:0920g50cbowvskip}
\end{figure}

\paragraph{vector size of 25 v 500} Comparing smaller and larger vector sizes \ref{fig:0920g50v25v500} the higher dimensions also result in a slightly broader distribution of embeddings. This is more apparent when the same data points are displayed for only the larger clans \ref{fig:0920g50v25v500_mcs150}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.95\linewidth]{images/0920_g50_v25_v_v500.png}
    \caption{First 3 Principal Components for skip with different vector sizes}
    \label{fig:0920g50v25v500}
\end{figure}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/0920_g50_v25_v_v500_mcs150.png}
    \caption{First 3 Principal Components for skip with different vector sizes - but only large clans shown}
    \label{fig:0920g50v25v500_mcs150}
\end{figure}

\subsection{Experiments II - Conclusions}
The results from the second set of experiments lead to the conclusion that changing the gap rule to include the word 'GAP' only if the gap is at least 50 amino acids long does \textbf{not} yield any better clustering results, even with higher dimensional vectors.



% --------------------------------------------------------



\clearpage
\section{Experiments III - Corpus with minimum gap size of 100}

\subsection{Configuration changes}
For this final set of experiments, the model architecture was limited to skipgram only and an extra vector size of 1,000 was included. The main modification was to change the corpus again so that the word 'GAP' would only be inserted in a sentence if the gap in the sequence between tokens was 100 amino acid characters or more. The following model configurations were used:

\begin{table}[h!]
\centering
\label{gap100hyperparams}
\begin{tabular}{|l|l|}
	\hline
	metric & values  \\
	\hline
    corpus GAP size & 100  \\
    model architecture & skipgram only  \\
    model minimum word count & 3, 5, 8   \\
    model window size & 13, 21, 44   \\
    model vector size & 5, 25, 50, 100, 250, 500 ,1000   \\
	\hline
\end{tabular}
\caption{Model configurations for the 3rd set of experiments}
\end{table}

\subsection{Distance matrix comparison results}
The results of the comparisons with the representative pfam distance matrix are shown in the table \ref{table_dist_results_pearsong100} below.
\begin{table}[hbt!]
\centering

\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
	min word count & window size & vector size & pearson & spearman \\
	\hline
	8 & 44 & 25 & 0.0896 & 0.1126 \\
    8 & 21 & 25 & 0.0879 & 0.1107 \\
    5 & 44 & 25 & 0.0867 & 0.1104 \\
    3 & 44 & 25 & 0.0864 & 0.1095 \\
    5 & 21 & 25 & 0.0857 & 0.1083 \\
    \hline
    3 & 13 & 25 & 0.0843 & 0.1068 \\
    8 & 13 & 25 & 0.0843 & 0.1062 \\
    5 & 13 & 25 & 0.0836 & 0.1054 \\
    3 & 21 & 25 & 0.0833 & 0.1059 \\
    8 & 44 & 5 &  0.0744 & 0.0853 \\
	\hline
\end{tabular}
\caption{Top 10 Distance Matrix correlations with minimum GAP distance in corpus of 100 spaces}
\label{table_dist_results_pearsong100}
\end{table}

The results of this configuration provide the best of the experiments - but only marginally - the best correlation being 11.26\%. As in the second set of experiments, the top 10 models using either the pearson or spearman metrics are the same and again, despite using larger vector sizes, the lower dimensional models perform better on this metric.

\subsection{KMeans clustering results}
For this clustering attempt only the top 5 models according to the spearman metric were used and lower-scoring models included with vector sizes of 100, 250, 500 and 1,000. The top correlations of the KMeans clusters with the actual Pfam clans are shown below.

\begin{table}[hbt!]
\centering

\begin{tabular}{|p{60mm}|c|c|p{45mm}|}
\hline
	model & min clan size & K & Average Jaccard similarity\\
\hline
w2v\_20240922\_skip\_mc8\_w44\_v25\_g100  &  2  & 557  & 0.0006 \\
w2v\_20240922\_skip\_mc8\_w21\_v25\_g100  &  10  & 106  & 0.0032 \\
w2v\_20240922\_skip\_mc8\_w44\_v25\_g100  &  25  & 43  & 0.0087 \\
w2v\_20240922\_skip\_mc8\_w44\_v25\_g100  &  50  & 20  & 0.0216 \\
w2v\_20240922\_skip\_mc5\_w44\_v25\_g100  &  100  & 7  & 0.0705 \\
w2v\_20240922\_skip\_mc8\_w21\_v25\_g100  &  150  & 5  & 0.1075 \\
\hline
w2v\_20240922\_skip\_mc8\_w44\_v150\_g50  &  150  & 5  & 0.1001 \\
w2v\_20240922\_skip\_mc8\_w44\_v250\_g50  &  150  & 5  & 0.0984 \\
w2v\_20240922\_skip\_mc8\_w44\_v500\_g50  &  150  & 5  & 0.0868 \\
w2v\_20240922\_skip\_mc8\_w44\_v1000\_g50  &  150  & 5  & 0.042 \\
\hline
\end{tabular}
\caption{Experiments 3 - KMeans clustering results for different clan sizes (thus different values of K}
\label{kmeansg100results}
\end{table}

\paragraph{}Again, the results in table \ref{kmeansg100results} do not show any strong correlations between the clusters from KMeans and the real clusters of clans. The top part of the table shows the best correlations across the different minimum clan sizes, the bottom shows the same for larger vector sizes (150, 250, 500 and 1,000) for comparison. The results are slightly worse than the second set of experiments with the maximum similarity being only 0.1001.

\subsection{Analysis}
The 3 Principal Components of a number of models were plotted to see if the higher vector sizes with this different corpus changed the distribution of embeddings \ref{fig:0922g100kmc2}. The plots for vector sizes from 5 to 1,000 are shown. Again, the embeddings are all densely packed in a small area. Plotting smaller subsets of embeddings show the same patterns as the first sets of experiments. Only one of these is shown below for comparison \ref{fig:0922g100kmc150c}. 

%To make it clearer, plots \ref{fig:0922g100kmc150} and beyond show a smaller subset of vectors from the same model but only those whose clans have at least 150 pfams. These plots also show the differences across different vector sizes.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\linewidth]{images/0922_g100_km_c2.png}
    \caption{3 Principal Components for vector sizes ranging from 5 to 1000}
    \label{fig:0922g100kmc2}
\end{figure}


%\begin{figure}
%    \centering
%    \includegraphics[width=1\linewidth]{images/0922g100_km_1_c150.png}
%    \caption{Minimum cluster size of 150, vector sizes 5 and 25}
%    \label{fig:0922g100kmc150}
%\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=1\linewidth]{images/0922g100_km_2_c150.png}
%%    \caption{Minimum cluster size of 150, vector sizes 100 and 250}
%    \label{fig:0922g100kmc150b}
%\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/0922_g100_km_3_c150.png}
    \caption{3 Principal Components for vector sizes of 500 and 1000 with a smaller set of plots}
    \label{fig:0922g100kmc150c}
\end{figure}


% ----------------------------------------------------------------
%             Chapter 4 - Analysis and Conclusiona
% ----------------------------------------------------------------
\chapter{Conclusion}

\section{Discussion}
In this study, a wide range of word2vec models have been successfully evaluated to investigate whether biological meaningful information is contained within their embeddings. A number of discussion points on results and method are presented below.

\subsubsection{Clustering performance}
Despite over 240 models being created and assessed, very little correlation could be found between the clusters found within the embeddings of Pfam domains and Pfam clans - the maximum observed was 10\% overlap. There could be many reasons for this.

\paragraph{Corpus considerations}Firstly, considering the \textbf{corpus}, with all vectors being positioned within the same small region, the word2vec algorithm has concluded that almost all domain tokens are somehow related semantically and must therefore appear close to each other in many sentences in the corpus. Changing the gap sizes should in theory have changed this distribution but the results for the gap sizes of 1, 50 and 100 were not markedly different. In retrospect, it may have been a mistake to include outlier sentences consisting of many domains with few other regions, this would be an interesting alternative to investigate. However, the reasons for including them were sound as they are contain biologically meaningful information.

\paragraph{}One might also consider the \textbf{window size}, as that impacts  the number of iterations of training the model executes - each resulting in a slight modification to the embeddings. However, many different combinations were tried with no discernible difference. The same is true of varying the \textbf{minimum word} count hyperparameter.

\paragraph{}Other studies have also used a similar domain-based representation of proteins to create word2vec embeddings and have reported better results. Specifically \textbf{Melidis et al.} (2020) \cite{llmdom2vec} showed that their embeddings outperformed sequence embeddings for toxin and enzymatic function prediction and was comparable for cellular location prediction. 

\paragraph{}They used gap sizes of 30 and also reduced sentence length by keeping only one domain in an overlapping region. The main difference with their approach was to use a much wider set of domain annotations - they used all available InterPro domain annotations including super-family, family, single domains and functional sites compared to this study's use of family and disorder regions only. They also developed sentences for all proteins rather than just eukaryotic proteins alone. Although its not clear from their paper, the use of a larger corpus and vocabulary may have resulted in embeddings that are distributed across a wider space.

\paragraph{Choice of clustering algorithm}Another possible explanation for the inability to produce meaningful clusters, may be the choice of \textbf{KMeans} algorithm to create them. An alternative may have been to use a \textbf{Gaussian Mixture Model} for clustering. These are probabilistic models that assume that all data points are generated from a mixture of a set number of Gaussian distributions with unknown parameters. They can be thought of as a generalised K-Means algorithm that incorporates information about the underlying covariance and means of the data in each cluster. This approach was considered at one point, but these models also depend upon a value being provided for the number of clusters to produce are more computationally intensive and less transparent. Also, when analysing the PCA plots, it is difficult to forsee that this model would be able to produce better clusters.


\subsubsection{Lack of distance correlation}
The lack of correlation observed between the distance matrices from word2vec embeddings and the supplied representative pfam distance matrix deserves some discussion. Both were ultimately derived from protein sequences, thus some correlation might have been expected. The following factors may have had some bearing on the poor results.
\paragraph{} Firstly, the word2vec distance matrices are created by encoding protein domains whereas the representative Pfam matrix was derived directly from alignments of the protein amino sequences. Although the protein domain tokens used in sentences were pfam ids and that pfam ids are also derived from sequence alignments - perhaps the final distance outputs from these two approaches are too far removed from the underlying sequence to allow comparison?
\paragraph{} Furthermore, the representative Pfam matrix was created by selecting a single protein at random from each Pfam family - it's possible that this introduced some noise or even that another random selection might provide different results.

\subsubsection{Method and choice of architecture}
With respect to the choice of architecture, two decisions were taken that may have resulted in additional overhead without some prior knowledge of these technologies.

\paragraph{} Firstly, Amazon Web Services (\textbf{AWS}) infrastructure was used to provide extra compute power when needed. This required some investment in time, although previous familiarity and tutorials limited this. Combined with the use of the \textbf{Terraform} tool to specify and provision AWS hardware, instances were extremely easy to create and destroy. That said, the AWS console is difficult to navigate, with too many services and too many hardware and operating system choices to be made. Pricing can be opaque and without prior experience it is easy to incur significant cost. However, when creating the number of models that were necessary, the choice of AWS was a good one as it allowed these to be created in the background and with some certainty on completion times. However, AWS may not be suitable for someone who is brand new to the technology.

\paragraph{} Another decision was the use of a local database (\textbf{DuckDB}) to store protein and domain data ahead of corpus creation. This approach was used because the volume of data required to form a corpus was not being parsed and searched quickly enough using basic python or unix command line tools on a local Macbook. Given the relational nature of the dataset (with protein ids acting as a foreign key for domains and disorder regions) a database was installed locally and proved extremely quick at ingesting tab delimited data and combining tokens. With indexes applied, the database was able to combine tokens from 10M proteins through a table join in only 30 seconds. The installation of DuckDB was simple and only a few SQL commands were needed to perform the data manipulations required. This approach would be recommended in future.


\section{Recommendations for future work}
Staying within the parameters of a word2vec model, recommendations from this study to take forward for further investigation include:

\paragraph{}Firstly, more domain annotations could be used in constructing the corpus as per Melidis at al. \cite{llmdom2vec}. They achieved some success with this vocabulary and it would be interesting to repeat the experiments of this study with that alternative corpus.

\paragraph{}Secondly, experimenting with a corpus that excludes outlier sentences is worthwhile investigating. This means excluding those sentences that have a high concentration of pfam domains compared to other tokens. Including these may have caused the embeddings in this study to converge in space.

\paragraph{}Beyond word2vec a BERT language model may be worth investigating. This architecture reads text from both directions in parallel which, in theory gives a more complete understanding of context. However, without changing the underlying corpus it is difficult to see whether this might indeed change anything.

\paragraph{}Finally, a custom embedding could be created. Although this study would advise designing such a model in close conjunction with experienced Biochemists with a deeper understanding of the biological and evolutionary relationships between protein sequences. For example, different weights might be applied to different tokens depending upon their position along the length of a protein and their particular function.

\section{Conclusion}
Using multiple word2vec models with protein domains as sentences, this study investigated whether meaningful biological relationships could be found by clustering domain embeddings. Despite trying over 240 model configurations and 3 different corpus constructions, the highest correlation achieved when comparing clusters with Pfam clans was only 10\%.




\appendix


\begin{thebibliography}{Bibliography }


% --------------- domain - sequence and structre --------------- 


\bibitem{domain1}Moore, A.D., Bjorklund,  A.K., Ekman, D., Bornberg\-Bauer, E., Elofsson, A.: Arrangements in the modular evolution of proteins. Trends in Biochemical Sciences 33(9), 444–451 (2008)

\bibitem{domain2}Forslund, S.K., Kaduk, M., Sonnhammer, E.L.: Evolution of protein domain architectures, 469–504 (2019)


% --------------- domain - sequence and structre --------------- 
\bibitem{pfam0}Sonnhammer, Erik LL and Eddy, Sean R and Durbin, Richard. Pfam: a comprehensive database of protein domain families based on seed alignments (1997). Proteins: Structure, Function, and Bioinformatics, vol. 28 No. 3, pp 405-420


\bibitem{pfam1}Punta M, Coggill PC, Eberhardt RY, Mistry J, Tate J, Boursnell C, Pang N, Forslund K, Ceric G, Clements J, Heger A, Holm L, Sonnhammer EL, Eddy SR, Bateman A, Finn RD. The Pfam protein families database. Nucleic Acids Res. 2012 Jan;40(Database issue):D290-301. doi: 10.1093/nar/gkr1065. Epub 2011 Nov 29. PMID: 22127870; PMCID: PMC3245129.


\bibitem{pfam2} Finn, Robert D and Bateman, Alex and Clements, Jody and Coggill, Penelope and Eberhardt, Ruth Y and Eddy, Sean R and Heger, Andreas and Hetherington, Kirstie and Holm, Liisa and Mistry, Jaina and others  (2014). "Pfam: the protein families database." Nucleic Acids Research, 42(D1), D222-D230. DOI: 10.1093/nar/gkr1065


\bibitem{pfamclan}Finn RD, Mistry J, Schuster-Böckler B, Griffiths-Jones S, Hollich V, Lassmann T, Moxon S, Marshall M, Khanna A, Durbin R, Eddy SR, Sonnhammer EL, Bateman A. Pfam: clans, web tools and services. Nucleic Acids Res. 2006 Jan 1;34(Database issue):D247-51. doi: 10.1093/nar/gkj149. PMID: 16381856; PMCID: PMC1347511.

% An introduction to Hidden Markov Models used in Pfam, which are central to identifying clans
\bibitem{pfamhmm} Finn, R. D., Clements, J., \& Eddy, S. R. (2011). "HMMER web server: interactive sequence similarity searching." Nucleic Acids Research, 39(suppl\_2), W29-W37. DOI: 10.1093\/nar\/gkr367






% --------------- Domains --------------- 

\bibitem{introprotdomain1}Moore, A.D., Bjorklund,  A.K., Ekman, D., Bornberg\-Bauer, E., Elofsson, A.: Arrangements in the modular evolution of proteins. Trends in Biochemical Sciences 33(9), 444–451 (2008)

\bibitem{introprotdomain2}Forslund, S.K., Kaduk, M., Sonnhammer, E.L.: Evolution of protein domain architectures, 469–504 (2019)

\bibitem{introprotdomain3}Das, S. and C.A. Orengo, Protein function annotation using protein domain family resources. Methods, 2015.

\bibitem{introprotdomain4}Nepomnyachiy, S., N. Ben-Tal, and R. Kolodny, Complex evolutionary footprints revealed in an analysis of reused protein segments of diverse lengths. Proc Natl Acad Sci U S A, 2017. 114(44): p. 11703-11708.

% --------------- Disordered regions --------------- 
\bibitem{introdisordered}Romero, P. et al. (1998) Thousands of proteins likely to have long disordered regions. Pac. Symp. Biocomput. 1998, 437–448




% ------------- NGrams  ------------- 


\bibitem{firth}Firth, J. R. (1957). A synopsis of linguistic theory 1930 - 1950, Studies in linguistic analysis, 1--32

\bibitem{osgood}Osgood, Charles E. et al. (1957). The Measurement of Meaning


\bibitem{markov}Markov, A. A. (1913). "An example of statistical study on the text of "Eugene Onegin" illustrating the linking of events in a chain"

\bibitem{shannon}Shannon, Claude E. (1948). "A mathematical theory of communication."

\bibitem{jelinek}Jelinek, F. (1975). "Continuous speech regocntion by statistical methods". Proceedings of the IEEE, vol. 64, pp 532 - 556.


\bibitem{ganap}Ganapathiraju, M., Weisser, D., Rosenfeld, R., Carbonell, J. G., Reddy, R., \& Klein-Seetharaman, J. (2002). Comparative ngram analysis of whole-genome sequences.


\bibitem{vriesngram2008}
Vries, G. D., Witteveen, C., \& Katrenko, S. (2008). Subfamily-specific conservation profiles for proteins based on n-gram patterns. *Bioinformatics, 24*(13), 2767-2773.

\bibitem{vriesngram2017}
De Vries, G., \& Tsivtsivadze, E. (2017). Learning n-gram patterns for protein sequence classification with an alignment-free sparse representation. *Bioinformatics, 33*(3), 926-933.






% ------------- Word2Vec  ------------- 


% word2vec mikolov
\bibitem{word2vecoriginal}Mikolov, T., Chen, K., Corrado, G.S., \& Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. International Conference on Learning Representations.

% doc2vec
\bibitem{doc2vec}Le Q., Mikolov T. (2014) Distributed representations of sentences and documents. Int. Conf. Mach. Learn. ICML 2014, 32, 1188–1196.

% transformer
\bibitem{transformer} Vaswani A. et al. Attention Is All You Need (2013) %https://arxiv.org/abs/1706.03762


% ------------- PLMs  General ------------- 

\bibitem{plmrives2019}Rives, A., et al. (2019). “Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences.”


\bibitem{plmtransfer} Heinzinger M, Elnaggar A, Wang Y, Dallago C, Nechaev D, Matthes F, Rost B. (2019) "Modeling aspects of the language of life through transfer-learning protein sequences". BMC Bioinformatics. 2019 Dec 17;20(1):723. doi: 10.1186/s12859-019-3220-8. PMID: 31847804; PMCID: PMC6918593.

\bibitem{plmheinzingerElmo2019} Michael Heinzinger, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes, Burkhard Rost (2019). "Modeling the language of life – Deep Learning Protein Sequences"


% ------------- PLMs Key models ------------- 

\bibitem{plmtape} Rao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, P., Canny, J., Abbeel, P., \& Song, Y. (2019). Evaluating Protein Transfer Learning with TAPE. Advances in Neural Information Processing Systems, 32.

\bibitem{plmesm}Rives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, Guo D, Ott M, Zitnick CL, Ma J, Fergus R. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc Natl Acad Sci U S A. 2021 Apr 13;118(15):e2016239118. doi: 10.1073/pnas.2016239118. PMID: 33876751; PMCID: PMC8053943.

% Prot Trans
\bibitem{plmprottrans} Elnaggar, A. et al. (2022) ProtTrans: towards cracking the lan- guage of lifes code through self-supervised deep learning and high performance computing. IEEE Trans. Pattern Anal. Mach. Intell. 44, 7112–7127

\bibitem{bert} Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2019). BERT: Pre\-training of Deep Bidirectional Transformers for Language Understanding. \textit{arXiv preprint arXiv:1810.04805}.

\bibitem{t5} Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... \& Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. \textit{Journal of Machine Learning Research}, 21(140), 1-67.


\bibitem{albert} Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., \& Soricut, R. (2020). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. \textit{arXiv preprint arXiv:1909.11942}.

\bibitem{electra} Clark, K., Luong, M. T., Le, Q. V., \& Manning, C. D. (2020). ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. \textit{arXiv preprint arXiv:2003.10555}.


% good summary of various techniques
\bibitem{llmfuncprot}Unsal, S., Atas, H., Albayrak, M. et al. Learning functional properties of proteins with language models. Nat Mach Intell 4, 227–245 (2022).

% Prot Bert
\bibitem{protbert}Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial, ProteinBERT: a universal deep-learning model of protein sequence and function, Bioinformatics, Volume 38, Issue 8, March 2022, Pages 2102–2110

\bibitem{transformerxl} Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., \& Salakhutdinov, R. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.

% ---------------- latest thinking

% alphafold
\bibitem{alphafold} Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, 0. \& Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. \textit{Nature}, 596(7873), 583-589. doi:10.1038/s41586-021-03819-2.


\bibitem{alhpafoldurl} https://deepmind.google/technologies/alphafold



% from db - domain to vec
\bibitem{llmdom2vec}Damianos P. Melidis, Brandon Malone, Wolfgang Nejdi, (2020) "dom2vec: Capturing domain structure and function using self-supervision on protein domain architectures"

\bibitem{llmword2vec}Buchan, D.W., Jones, D.T. (2020) "Learning a functional grammar of protein domains using natural language word embedding techniques". Proteins: Structure, Function, and Bioinformatics 88(4), 616-624 (2020)

\bibitem{yang}Yang, K.K., Wu, Z., Bedbrook, C.N., Arnold, F.H. (2018) "Learned protein embeddings for machine learning". Bioinformatics 34(15), 2642–2648

\bibitem{bebler}Bepler, T., Berger, B. (2019) "Learning protein sequence embeddings using information from structure". In: Proceedings of the 7th International Conference on Learning Representations

\bibitem{asgari}Asgari E, McHardy AC, Mofrad MRK. Probabilistic variable-length segmentation of protein sequences for discriminative motif discovery (DiMotif) and sequence embedding (ProtVecX). Sci Rep. 2019 Mar 5;9(1):3577. doi: 10.1038/s41598-019-38746-w. PMID: 30837494; PMCID: PMC640108

\bibitem{kimothi} Heinzinger, M., Elnaggar, A., Wang, Y., Dallago, C., Nechaev, D., Matthes, F., \& Rost, B. (2019). Modeling aspects of the language of life through transfer-learning protein sequences. *BMC Bioinformatics, 20*(1),


\bibitem{mazzaferro} Mazzaferro, S., Mylonas, R., Zhang, J., \& Shen, J. (2017). Predicting protein-ligand binding affinities with a novel machine-learning approach. *Journal of Chemical Information and Modeling, 57*(12), 3144-3155.



% ---------------- stats
\bibitem{statmantel} Mantel, N. (1967). "The detection of disease clustering and a generalized regression approach." Cancer Research, 27(2), 209-220.

\bibitem{statpearson} Pearson, K. (1895). "Note on regression and inheritance in the case of two parents." Proceedings of the Royal Society of London, 58, 240-242.

\bibitem{statspearman} Spearman, C. (1904). "The proof and measurement of association between two things." The American Journal of Psychology, 15(1), 72-101.



% --- Method

\bibitem{dbuchanreppfam}D.Buchan, Personal communication.

\bibitem{nw}Needleman, S. Wunsch, D. (1970) "A general method applicable to the search for similarities in the amino acid sequence of two proteins", Journal of molecular biology volume 48




% -------- References to databases and pfam etc



\end{thebibliography}

\chapter{Github code - location and organisation}
The code for this project is freely available on \href{https://github.com/greyneuron/COMP_0158_MSC_PROJECT/tree/main}{github}. It is organised as follows at the top level:

\begin{itemize}
    \item \textbf{code} contains all python, C++ code, and shell scripts organised by functional area
    \item \textbf{data} contains some sample data, restricted due to github limitations on sizes
    \item \textbf{logs} contains outputs logs from the various
    \item \textbf{database} contains the duckdb local database to hold the output of the data preparation exercises (not uploaded due to github restrictions)
\end{itemize}

%The folders beneath \textbf{code} and \textbf{data} have a mirrored structure and are (hopefully) self-explanatory

\begin{itemize}
    \item \textbf{data\_prep} contains all code used to download and parse raw data into tab delimited files
    \item \textbf{corpus} contains the code to create the corpus files
    \item \textbf{model} contains the code to create the models themselvves and run through the various hyperparameter combinations
    \item \textbf{distance} contains the code to compare the word2vec distances with the rand\_rep distance matrices
    \item \textbf{clustering} contains the code to run and analyse the outputs of KMeans cllustering algorithms
    \item \textbf{terraform} contains Terraform ccritps to create environments on AWS including networks, securoty groups, EC2 compute instances, EBS storage, and database instances
\end{itemize}

Note that within the various sub directories of the code folder, there will often be some Jupyter notebooks - called ***\_helper.ipynb. These were used as sandbox areas to quickly try out code. These are useful for testing, but once working, the code was transferred into regular python files within the same directory. 





% ---------------- my document ----------------

\end{document}