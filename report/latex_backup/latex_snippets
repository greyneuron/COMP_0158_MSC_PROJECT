%Word2Vec is a shallow neural network with just 2 layers (excluding the input) %- a hidden layer and an output layer. The hidden layer has one neuron for each %dimension we wish to use to represent a word. The output layer is a softmax %classifier that provides the probabilities of what the next word might be %given the current input word - as such it has the same dimension as the number %of words in the vocabulary.\\

%Under the covers it trains a classifier on a binary prediction task - instead %of counting how many times the word 'student' appears near the word 'teacher', %it instead trains a classifier to establish how likely the word 'student' will %show up near the word 'teacher'. The outcome of this prediction task is not %actually important - but the classifier weights are - these are the word %embeddings.\\





%The dimensions of the matrix is thus N x D where N is the size of the vocab %and D is the vector dimension. The embedding for a word 'n' in the vocabulary %is thus the 'n'th row of the matrix.



%Word2vec also produces embeddings that are dense vectors - i.e. there aren't %many zero-counts. These dense embeddings have been shown to work better in NLP %tasks than sparse ones. They embeddings are also static - meaning that each %word has only one vector representation. This compares to %\textbf{\textit{BERT}}, for example, in which the embedding for each word can %be different in different contexts.\\


%\paragraph{Word2Vec Architecture}
%Word2Vec is a shallow neural network with just 2 layers (excluding the input) %- a hidden layer and an output layer. The hidden layer has one neuron for each %dimension we wish to use to represent a word. The output layer is a softmax %classifier that provides the probabilities of what the next word might be %given the current input word - as such it has the same dimension as the number %of words in the vocabulary.\\

%\begin{center}\textbf{The hidden weights of the model after training it on the %corpus are the word embeddings.}
%\end{center}

%The dimensions of the matrix is thus N x D where N is the size of the vocab %and D is the vector dimension. The embedding for a word 'n' in the vocabulary %is thus the 'n'th row of the matrix.


%\begin{itemize}
%    \item \textbf{Continuous Bag of Words (CBOW)}: Predicts the current word %given the surrounding context (i.e., the words around it).
%    \item \textbf{Skip-Gram}: Predicts the surrounding context words given the %current word.
%\end{itemize}


When training \textbf{CBOW}, the objective is to maximize the probability of the target word (center word) given the surrounding context words. Formally, it maximizes the conditional probability:

\[
P(w_{\text{target}} \mid w_{\text{context1}}, w_{\text{context2}}, \dots)
\]

For \textbf{Skip-Gram}, the objective is to maximize the probability of the context words given a target word. It maximizes:

\[
P(w_{\text{context1}}, w_{\text{context2}}, \dots \mid w_{\text{target}})
\]

In both cases, these probabilities are modeled using softmax functions over the dot product of word vectors.

\subsubsection{Word Vectors}
The neural network learns two types of vectors for each word: one for when the word is treated as a target and another for when it is treated as part of the context. These vectors are initialized randomly and optimized during training. After training, the embeddings from the hidden layer (for either CBOW or Skip-Gram) are taken as the word vectors.

\subsubsection{Negative Sampling and Hierarchical Softmax}
Directly computing the softmax over all words in the vocabulary is computationally expensive. Word2Vec addresses this using techniques like:

\begin{itemize}
    \item \textbf{Negative Sampling}: Instead of updating the weights for all words, the model updates the weights for a small number of sampled negative words (words not in the context).
    \item \textbf{Hierarchical Softmax}: An approximation to the softmax function that reduces the computational complexity by structuring the output layer as a binary tree.
\end{itemize}

\subsection{Word2Vec Properties and Applications}

\begin{itemize}
    \item \textbf{Semantic Similarity}: Word2Vec embeddings capture semantic relationships between words. Words that are contextually similar (like \textit{king} and \textit{queen}) have similar vector representations. This allows operations like vector arithmetic. For example:
    \[
    \text{king} - \text{man} + \text{woman} \approx \text{queen}
    \]
    \item \textbf{Dimensionality Reduction}: Word2Vec embeddings are lower-dimensional compared to traditional methods like one-hot encoding, making them more memory-efficient and faster for downstream tasks.
    \item \textbf{Transfer Learning}: Pretrained Word2Vec models can be used as input features for other machine learning models, making them powerful tools in natural language processing (NLP) for tasks like text classification, machine translation, and sentiment analysis.
\end{itemize}
