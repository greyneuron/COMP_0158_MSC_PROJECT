Sat July 20th
- Tried mysql locally but wsa taking ages
- At 4pm started a scrpt to create pre-corpus but it is taking 1hr to parse 1M - kept it running
- Went to Noel Gallagher concert

Sun July 21st
- Pre-corpus script painfully slow
- Loaded all dat files into S3 to see if I could use Glue (also loaded into EBS)
- Got frustrated with that after a while as too much to learn and not convinced it would be fast enough or cheap enough
- Resorted back to an RDS instance but had difficulty loading the data from my Terraform instance
- Resorted to creating an RDS instance from the console and after some tinkering managed to get it to load my local files into W2V_TOKEN and W2V_PROTEIN
- Loaded all sata into these tables and applied indices - showed that 1M proteins could be executed very quickly
- Close of play:
    # - New rds instance created and db created called W2V with user 'admin' and password 'w0rd2v3c'
    # - Created W2V_TOKEN table and uploaded all pfam and disorder tokens
    # - Created an index on that table
    # - Loaded all protein data into W2V_PROTEIN - byut there are duplicates



# --------------------------------------------------------------------------------
# ssh to ecs
export dns="ec2-63-32-44-188.eu-west-1.compute.amazonaws.com"
ssh -i "w2v_rsa" ec2-user@$dns

# after restart of ec2
lsblk
sudo mkdir /data
sudo mount /dev/nvme1n1 /data
cd /data/dev/ucl
. w2venv/bin/activate

sudo dnf update -y
sudo dnf install mariadb105

. w2venv/bin/activate

# to activate venv
cd /data/dev/ucl
. w2venv/bin/activate

# to execute code (activate venv first)
export dns=
scp -i "w2v_rsa" ~/dev/ucl/comp0158_mscproject/code/mysql_tools.py ec2-user@$dns:/data/dev/ucl/code
cd code

# mysql from ec2 ssh
export endpoint="w2v-dev-db.cligs4ak0dtg.eu-west-1.rds.amazonaws.com"
mysql -h $endpoint -P 3306 -u w2v -p

# --------------------------------------------------------------------------------
                    Week commencing Monday July 22nd
*       Friday Target: Have a distance matrix for each pfam entry
# --------------------------------------------------------------------------------

* Tasks
1. Create pre-corpus
    - For each protein query the database
2. Create corpus
3. Run word2vec on corpus
4. Get unique pfam entries
5. For each pfam entry get its vector representation from word2vec and store it
6. Determine distances between each pfam entry

day 1 target: Create corpus by end of day (or at least pre-corpus)



# --------------------------------------------------------------------------------
Monday 22 July

- Created new rds instance from snapshot on 21 July, made it publicly available
- Had to add security groups again via the console so that I could access from ec2

- Tried running from laptop to RDS:
    - Loaded mysql client into conda on laptop 0 but couldn;t connect via laptop
    - Also tried adding a new securoty group such as in this way: https://stackoverflow.com/questions/37212945/aws-cant-connect-to-rds-database-from-my-machine
      that didn't work

- Resorted to running python code from EC2
    - Develped code on laptop
    - SSH to EC2 and create a venv then pip3 install mysqlclient

        # created venv again on ec2
        python3 -m venv w2venv
        source w2venv/bin/activate
        pip3 install mysql-connector-python

    - SCP code from laptop to ec2
        cd to terraform/w2v_ec2 (thats where the key is)
        get dns of ec2 instance
        export dns=<dns of ec2>
        assumes already have setup ssh
        scp -i "w2v_rsa" ~/dev/ucl/comp0158_mscproject/code/mysql_tools.py ec2-user@$dns:/data/dev/ucl/code

    - Ran mysql_tools.py in batches of 500,000 - see pre_corpus_fill_log_20240722.log
    - Renamed those files 1 to 9 to have extension 01 to 09
    - Also ran query to find unique pfam entries (also in mysql_tools.py) [20,725 unique pfams]

    - Tar'd up the files
        - cd to precorpus
        - tar -czvf archive_name.tar.gz precorpus.tar.gz

    - Copied them and the unique pfam entires to s3
        - aws configure
        - aws s3 cp precorpus.tar.gz s3://w2v-bucket/corpus/precorpus.tar.gz

        - aws s3 cp unique_corpus_pfam_20240722.dat s3://w2v-bucket/pfam/unique_corpus_pfam_20240722.dat

    - Created RDS Snapshot RDS
    - Stopped EC2
    - Stopped RDS TODO

# --------------------------------------------------------------------------------
Tuesday 23 July

Noticed that I had left out the protein start and end lengths - took all day to fis due to weird behaviour of query

Attempt 1 - Direct connect from macbook to RDS
- Restarted RDS from console and made it publisly available - just selected all the defaults
- The added new route to vpc sec groups to 0.0.0.0/0
- Was then able to connect from laptop - but connection kept dropping

Attempt 2 - Go back to EC2 connecting to RDS
- Restarted again from snapshot
- Selected w2v-security-group but left all others as is
- From EC2 this worked: 
    % mysql -h $endpoint -P 3306 -u admin -p

Changed query to inculde protein start and end but it struggled on db.t3g.xlarge

- Restarted with a larger db, but honestly it amde no difference
- Eventually settled on a modified version of Mondasy's code (precorpus_v2()) but with the benefit that this combines everything into one line per protein
- Running with a chink size of 250,000 proteins it was taking at least 2mins per chunk = 8min for 1M = 10 hours for 78M!
- Will need 312 files of output

# ALTERNATIVE - USE DIRECT SQL OUTPUT AND THE TIDY IT UP WITH AWK
- SSH to ec2
- create a folder and give it write access
- the following will write 1M results to a separate file
mysql -h $endpoint -P 3306 -u admin W2V -p -e 'SELECT W2V_PROTEIN.*, W2V_TOKEN.* FROM ( SELECT UNIPROT_ID, START, END FROM W2V_PROTEIN W2V_PROTEIN ORDER BY UNIPROT_ID LIMIT 0, 1000000) AS W2V_PROTEIN INNER JOIN W2V_TOKEN AS W2V_TOKEN ON W2V_PROTEIN.UNIPROT_ID = W2V_TOKEN.UNIPROT_ID' > sql/sql_output_0_1M.txt

cat sql_output_0_1M.txt | awk '{FS ="\t"} {print $1 ":" $2 ":" $3 "|" $5 ":" $6 ":" $7 ":" $8}'

# Close of play
- Have left create_pre_corpus_v2 runnig this evening from about 1830 - taking 2.5min for 250k proteins - 10min per 1M proteins = 13 hours
- Used the following:
    
    chunk_size      = 250000
    num_iterations  = 320
    iteration       = 1
    result          = 0
    start           = 0

created a new file for each iteration

- 10M target = 40 x 2.5min = 1hr 3/4 = ideally by 8pm get to v2_40
    18:48:30    : v2_12
    20:26:30      : v2_44 = 1hr 40 for 32 iterations

- Stopped on 90
- Need to redo number 1

 # --------------------------------------------------------------------------------
Wed 24 July

Dublin


# --------------------------------------------------------------------------------
Thurs 25 July

- Tried to restart DB form snapshot with local access but couldn;t add 0.0.0.0/0 due to conflict
- There are loads of child subnets and securoty groups already
- decided to delete and recreate the vpc!
- done

# restore snapshot rds from console
- w2v-db-1

- db.t4g.medium (2 CPU 4 GiB)  (could this be too small - previoulsy used db.t4g.xlarge : 4 cpu 16Gb)

- Single instance
- vpc: used default vpc
- vpc sec group: use existing (which is default)
- make public
- eu-west-1a

- didn;t have to do this next bit for some reason as rule was already there
- once says 'backing up' go to db instance and select vpc-sec-group
- select the sec group add new inbound for 0.0.0.0/0 on 3306
- copy endpoint

- db slow - not enough RAM?

brew install mysql-client
/usr/local/opt/mysql-client/bin/mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c

echo endpoint=""
/usr/local/opt/mysql-client/bin/mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c


Was using t3.xlarge as opposed to 


db.t4g.xlarge is haning again - especially on 221000 proteins
runs fast even from local pythin up to that point
locally it did 100k proteins in 27s


start_pos=0
chunk_size=2

for i in {0..5}
do
    echo "starting at" $start_pos
    # works
    #/usr/local/opt/mysql-client/bin/mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c -e "SELECT * FROM W2V_PROTEIN ORDER BY UNIPROT_ID LIMIT ${start_pos}, ${chunk_size}"

    #Â also works (hooray)
    /usr/local/opt/mysql-client/bin/mysql -h $endpoint -P 3306 -u admin W2V -pw0rd2v3c -e "SELECT W2V_PROTEIN.*, W2V_TOKEN.* FROM ( SELECT UNIPROT_ID, START, END FROM W2V_PROTEIN W2V_PROTEIN ORDER BY UNIPROT_ID LIMIT ${start_pos}, ${chunk_size}) AS W2V_PROTEIN INNER JOIN W2V_TOKEN AS W2V_TOKEN ON W2V_PROTEIN.UNIPROT_ID = W2V_TOKEN.UNIPROT_ID"

    start_pos=$((start_pos + chunk_size))
done


going to revert to the shell script above, querying a db.t4g.large

1. Start the rds on db.t4g.large
2. Change the endpoint variable in extract_tokens_from_db.sh
3. Change the params in extract_tokens_from_db.sh
4. Run extract_tokens_from_db.sh
5. Convert tokens to a single line per protein

# --------------------------------------------------------------------------------
Fiday

CONTINUE WITH DB EXTRACT APPROACH

- Updated extract_tokens_from_db.sh to loop
- Restarted DB used db.m6 or somethinng > it was quite slow
- Restarted DB used db.t4g.2xlarge > not sure it really makes a difference

Have 0 - 10,000,000 from Thursday
Started at 12,000,000 in 250k chunks - but set iterator to 9 instead of 

Want to do batches of 10M in chunks of 250k - thus 40 iterations

Noticed that batch size of 250k takes 3min - but so does batch size of 500k

Finished 10M - 20M but there are different batch sizes so that are not 20 files

10:50 - Started 20M to 30M in batches of 500k over 0 .. 9 iterations : each is taking about 3min 30 +> 10M will take about 38min
