Sat July 20th
- Tried mysql locally but wsa taking ages
- At 4pm started a scrpt to create pre-corpus but it is taking 1hr to parse 1M - kept it running
- Went to Noel Gallagher concert

Sun July 21st
- Pre-corpus script painfully slow
- Loaded all dat files into S3 to see if I could use Glue (also loaded into EBS)
- Got frustrated with that after a while as too much to learn and not convinced it would be fast enough or cheap enough
- Resorted back to an RDS instance but had difficulty loading the data from my Terraform instance
- Resorted to creating an RDS instance from the console and after some tinkering managed to get it to load my local files into W2V_TOKEN and W2V_PROTEIN
- Loaded all sata into these tables and applied indices - showed that 1M proteins could be executed very quickly
- Close of play:
    # - New rds instance created and db created called W2V with user 'admin' and password 'w0rd2v3c'
    # - Created W2V_TOKEN table and uploaded all pfam and disorder tokens
    # - Created an index on that table
    # - Loaded all protein data into W2V_PROTEIN - byut there are duplicates

# --------------------------------------------------------------------------------
                    Week commencing Monday July 22nd
*       Friday Target: Have a distance matrix for each pfam entry
# --------------------------------------------------------------------------------

* Tasks
1. Create pre-corpus
    - For each protein query the database
2. Create corpus
3. Run word2vec on corpus
4. Get unique pfam entries
5. For each pfam entry get its vector representation from word2vec and store it
6. Determine distances between each pfam entry

day 1 target: Create corpus by end of day (or at least pre-corpus)


Monday 22 July
- Created new rds instance from snapshot on 21 July, made it publicly available
- Had to add security groups again via the console so that I could access from ec2

- Tried running from laptop to RDS:
    - Loaded mysql client into conda on laptop 0 but couldn;t connect via laptop
    - Also tried adding a new securoty group such as in this way: https://stackoverflow.com/questions/37212945/aws-cant-connect-to-rds-database-from-my-machine
      that didn't work

- Resorted to running python code from EC2
    - Develped code on laptop
    - SSH to EC2 and create a venv then pip3 install mysqlclient

        # created venv again on ec2
        python3 -m venv w2venv
        source w2venv/bin/activate
        pip3 install mysql-connector-python

    - SCP code from laptop to ec2
        export dns=<dns of ec2>
        assumes already have setup ssh
        scp -i "w2v_rsa" ~/dev/ucl/comp0158_mscproject/code/mysql_tools.py ec2-user@$dns:/data/dev/ucl/code

    - Ran mysql_tools.py in batches of 500,000 - see pre_corpus_fill_log_20240722.log
    - Renamed those files 1 to 9 to have extension 01 to 09
    - Also ran query to find unique pfam entries (also in mysql_tools.py) [20,725 unique pfams]

    - Tar'd up the files
        - cd to precorpus
        - tar -czvf archive_name.tar.gz precorpus.tar.gz

    - Copied them and the unique pfam entires to s3
        - aws configure
        - aws s3 cp precorpus.tar.gz s3://w2v-bucket/corpus/precorpus.tar.gz

        - aws s3 cp unique_corpus_pfam_20240722.dat s3://w2v-bucket/pfam/unique_corpus_pfam_20240722.dat

    - Created RDS Snapshot RDS
    - Stopped EC2
    - Stopped RDS TODO

