{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# file handles\n",
    "protein_fasta_file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_10M.fasta\";\n",
    "interpro_file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Examples\n",
    "\n",
    "#### Uniref100 Fasta\n",
    ">\\>UniRef100_Q197F3 Uncharacterized protein 007R n=1 Tax=Invertebrate iridescent virus 3 TaxID=345201 RepID=007R_IIV3\n",
    "MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKIVDDASTITITYHRV\n",
    "YFGISGPKPRQVADLGEYYDVNELLNYDTYTKTQEFAQKYNSLVKPTIDAKNWSGNELVL\n",
    "\n",
    "\n",
    "#### Protein2ipr.dat\n",
    ">A0A000\tIPR004839\tAminotransferase, class I/classII\tPF00155\t41\t381<br>\n",
    "A0A000\tIPR010961\tTetrapyrrole biosynthesis, 5-aminolevulinic acid synthase\tTIGR01821\t12\t391<br>\n",
    "A0A000\tIPR015421\tPyridoxal phosphate-dependent transferase, major domain\tG3DSA:3.40.640.10\t48\t288<br>\n",
    "A0A000\tIPR015422\tPyridoxal phosphate-dependent transferase, small domain\tG3DSA:3.90.1150.10\t36\t378<br>\n",
    "A0A000\tIPR015424\tPyridoxal phosphate-dependent transferase\tSSF53383\t9\t389<br>\n",
    "A0A000\tIPR050087\t8-amino-7-oxononanoate synthase class-II\tPTHR13693\t34\t382<br>\n",
    "A0A001\tIPR003439\tABC transporter-like, ATP-binding domain\tPF00005\t361\t503<br>\n",
    "\n",
    "#### Explanation\n",
    "A0A0 Means its uniprot but not yet verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "1. Extract pfam entries from protein2ipr.dat\n",
    "\n",
    "awk '{FS=\"\\t\"}{print $1, \" \", $2, \" \", $4}' /Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat | awk '$3 ~ /PF[0-9]./ {print $0}' > protein2ipr_pfam2.dat\n",
    "\n",
    "2. Disorder regions\n",
    "- Use parse_match_complete.py\n",
    "- This parses /Volumes/My Passport/downloads/match_complete.xml which is 500GB abd searches for MobiDb entries\n",
    "- This script took about 6 hours to run\n",
    "\n",
    "- BUT -\n",
    "\n",
    "- MobiDB does not appear as a dbname, thus use the below to find possible values:\n",
    "\n",
    "\n",
    "grep \"dbname=\" /Volumes/My\\ Passport/downloads/match_complete.xml | awk '{FS=\"dbname=\"}{print $2}' | awk '{print $1}' | sort |uniq -c\n",
    "\n",
    "NOTE THAT THIS GREP TOOK ABOUT 8 HRS TO RUN ON MY LAPTOP:\n",
    "\n",
    "Ouput: \n",
    "1 \"ANTIFAM\"\n",
    "254194862 \"CATHGENE3D\"\n",
    "100323516 \"CDD\"\n",
    "25444276 \"HAMAP\"\n",
    "   1 \"INTERPRO\"\n",
    "51495327 \"NCBIFAM\"\n",
    "167725277 \"PANTHER\"\n",
    "278089257 \"PFAM\"\n",
    "20659779 \"PIRSF\"\n",
    "38902426 \"PRINTS\"\n",
    "94353952 \"PROFILE\"\n",
    "49021231 \"PROSITE\"\n",
    "4722014 \"SFLD\"\n",
    "69964999 \"SMART\"\n",
    "209693119 \"SSF\"\n",
    "   1 dbname=\"SFLD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\";\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam2.dat\"; first pfam with text stripped out\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_new.dat\"; #new pfam with full line\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/A8KBH6_ipr_pfam.dat\";\n",
    "\n",
    "#\n",
    "# greps for an id in interpro dat file\n",
    "# For A8KBH6, this will match A0A01A8KBH6 and A8KBH6\n",
    "# Time to parse the full protein2ipr.dat for A8KBH6 : 22min 56s\n",
    "#\n",
    "def grep_interpro(id):\n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            #match_string = \"รง\"            # works\n",
    "            #match_string = \"[A0A0-9]*A8KBH6\"   # works\n",
    "            #match_string = \"^[A0A0-9]*\"+id     # works\n",
    "            match_string = \"^[A0A0-9]*\"+id\n",
    "            match = re.search(match_string, line)\n",
    "            if match:\n",
    "                print('Matched:', id, 'in line:', line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep_interpro('A8KBH6')\n",
    "grep_interpro('A0A1A8KBH6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protein Sentence Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Protein Word\n",
    "#\n",
    "class ProteinWord:\n",
    "    def __init__(self, type, text, start, end):\n",
    "        self.type = type\n",
    "        self.text = text\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def __str__(self):\n",
    "        return f' {self.type}, {self.text}, {self.start}, {self.end}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f' {self.type}, {self.text}, {self.start}, {self.end}'\n",
    "    \n",
    "#\n",
    "# Protein Sentence\n",
    "#\n",
    "class ProteinSentence:\n",
    "    def __init__(self, uniprot_id, word):\n",
    "        #print('Creating new sentence for', uniprot_id, ': ',  word.text)\n",
    "        self.uniprot_id = uniprot_id\n",
    "        self.words = [word]\n",
    "        self.text = word.text\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        #print('Adding new word to', self.uniprot_id, ':',  word.text)\n",
    "        self.words.append(word)\n",
    "        self.text = self.text + ',' + word.text\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f' {self.uniprot_id}: {self.text}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f' {self.uniprot_id}: {self.text}'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpro protein2ipr.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse Interpro protein2ipr >> PFAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10M lines 10000000 : 20s\n",
    "#MAX_LINES = 10000000\n",
    "\n",
    "limit       = True # if True, onLy parses Max_lines lines \n",
    "MAX_COUNT   = 25000000\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 26 June 2024\n",
    "# parses protein2ipr.dat for entries with'PFNNN' and outpts those lines to a separate file\n",
    "# protein2ipr.dat       : 98.7GB,   1,355,591,115 entries\n",
    "# protein2ipr_new.dat   : 20.73GB,  298,766,058 entries\n",
    "# parsing time          : 23mins\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "input = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\"\n",
    "output = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_new_25M.dat\"\n",
    "\n",
    "#\n",
    "# parse an Interpro file and grep out those with pfam domains\n",
    "# Parsing the full protein2ipr.dat file in python took: 23mins 40s\n",
    "#\n",
    "def create_pfam_interpro():\n",
    "    match_count  = 0\n",
    "    output_file = open(output, \"w\")\n",
    "\n",
    "    with open(input, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "\n",
    "            # just match for PF\n",
    "            match = re.search(\"PF[0-9]+\", line) \n",
    "            \n",
    "            if match:\n",
    "                match_count += 1\n",
    "                if match_count > MAX_COUNT:\n",
    "                    print(MAX_COUNT, 'limit reached, breaking.')\n",
    "                    break\n",
    "\n",
    "                output_file.write(line)\n",
    "    output_file.close()\n",
    "\n",
    "create_pfam_interpro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniRef100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse PFAM to create initial ProteinSentence classes with PFAM entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10M lines 10000000 : 20s\n",
    "\n",
    "limit = True # if true, obey the max lines\n",
    "MAX_LINES = 1000000\n",
    "\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\"; # full file\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_new.dat\"; # pfam only\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_new_25M.dat\"; # pfam only\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/A8KBH6_ipr_pfam.dat\"; # as above but only entries for A8KBH6 (pfam only)\n",
    "\n",
    "# to hold sentences for each sequence\n",
    "sentences = {}\n",
    "\n",
    "#\n",
    "# parse an Interpro file and create ProteinSentences and Words\n",
    "#\n",
    "# TODO: Confirm id starting A0A0\n",
    "def parse_interpro():\n",
    "    count = 0\n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            if (line_number % 100000) == 0:\n",
    "                count += 1\n",
    "                print(count * 100000, 'lines processed.....')\n",
    "            if(limit):\n",
    "                if line_number > MAX_LINES:\n",
    "                    break\n",
    "            \n",
    "            # NB: Make sure the file is tab delimited\n",
    "            \n",
    "            # note that the first line matches exactly, so loads iof the fasta formats with A0A* will not match\n",
    "            #match = re.search(\"^([a-zA-Z0-9]+)\\\\tIPR[0-9]+\\\\t.*(PF[0-9]+)\\\\t([0-9]+)\\\\t([0-9]+)\", line)\n",
    "            \n",
    "            #match = re.search(\"[A0A0-9]*[a-zA-Z0-9]+\\\\tIPR\", line) # matches in protein2ipr_new.dat\n",
    "            #match = re.search(\"([A0A0-9]*[a-zA-Z0-9]+)\\\\tIPR[0-9]+\\\\t.*\\\\t(PF[0-9]+)\", line) # matches in protein2ipr_new.dat\n",
    "            \n",
    "            #\n",
    "            # Testing A0A1A8KBH6 and A8KBH6\n",
    "            #\n",
    "            # Works for both in: A8KBH6_ipr_pfam.dat\n",
    "            match = re.search(\"([A0A0-9]*[a-zA-Z0-9]+)\\\\tIPR[0-9]+\\\\t.*\\\\t(PF[0-9]+)\\\\t([0-9]+)\\\\t([0-9]+)\", line)\n",
    "                      \n",
    "            if match is not None:\n",
    "                id = match.group(1)\n",
    "                pfam_word = match.group(2)\n",
    "                start = match.group(3)\n",
    "                end = match.group(4)\n",
    "                \n",
    "                #print('Match protein :', id, 'PF:', pfam_word)\n",
    "                \n",
    "                # create a new word item\n",
    "                word = ProteinWord('pfam', pfam_word, start, end)\n",
    "                \n",
    "                # check if already have a protein with this id\n",
    "                if (id in sentences.keys()):\n",
    "                    sentences[id].add_word(word)\n",
    "                else:\n",
    "                    sentence = ProteinSentence(id, word)\n",
    "                    sentences[id] = sentence\n",
    "            \n",
    "parse_interpro()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences.keys()))\n",
    "#print(sentences.keys())\n",
    "#print(sentences.keys()[0:10])\n",
    "#print(sentences['A0A002'])\n",
    "#print(sentences['A8KBH6'])\n",
    "#print(sentences['A0A1A8KBH6'])\n",
    "print(sentences['A0A009GV07'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse UniRef100 FASTA 1 > Check for ProteinSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500k is enough for matches to be found\n",
    "MAX_LINES = 10000000\n",
    "matched = []\n",
    "not_matched = []\n",
    "\n",
    "#\n",
    "# parse a fasta file to get protein ids\n",
    "# for uniref, these ids are the characters after UniRef100_\n",
    "# TODO: Check if ids are proteins or protein clusters\n",
    "#\n",
    "def parse_fasta():\n",
    "    with open(protein_fasta_file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            \n",
    "            if line_number > MAX_LINES:  # line_number starts at 0.\n",
    "                break\n",
    "            #print('Processing :', line)\n",
    "            # note that the raw interpro file is tab delimited between fields\n",
    "            match = re.search(\"UniRef100_([A-Z0-9]+) \", line)\n",
    "            if match is not None:\n",
    "                id = match.group(1)\n",
    "\n",
    "                if id in sentences.keys():\n",
    "                    #print(line_number, 'Found sentence for protein :', id, sentences[id].text)\n",
    "                    if id not in matched:\n",
    "                        matched.append(id)\n",
    "                else:\n",
    "                    not_matched.append(id)\n",
    "parse_fasta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse UniRef100 FASTA 2 > Extract id, length, taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500k is enough for matches to be found\n",
    "MAX_LINES = 1000\n",
    "matched = []\n",
    "not_matched = []\n",
    "\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_500M.fasta\"\n",
    "\n",
    "#\n",
    "# parse a fasta file to get protein ids\n",
    "# for uniref, these ids are the characters after UniRef100_\n",
    "# TODO: Check if ids are proteins or protein clusters\n",
    "#\n",
    "def parse_fasta_2():\n",
    "    matchline = \"\"\n",
    "    id = \"\"\n",
    "    match_inprogress = False\n",
    "    \n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            \n",
    "            if line_number > MAX_LINES:  # line_number starts at 0.\n",
    "                break\n",
    "\n",
    "            match = re.search(\">UniRef100_([A-Z0-9]+).*TaxID=([0-9]+).*\", line)\n",
    "            \n",
    "            #match = re.search(\">UniRef100_([A-Z0-9]+)\", line) # works\n",
    "            \n",
    "            if match is not None:\n",
    "                if(id != \"\"):\n",
    "                    print(id, '\\t', len(matchline), '\\t', tax_id, '\\t', matchline)\n",
    "                matchline = \"\"\n",
    "                id = match.group(1)\n",
    "                tax_id = match.group(2)\n",
    "                continue\n",
    "            else:\n",
    "                matchline += line.strip()\n",
    "parse_fasta_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(matched),'Matched proteins:\\n', matched)\n",
    "print(len(not_matched), 'Unmatched proteins:\\n',not_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse UniRef100 FASTA 3 > Modified parse_masked_regions from Daniel - outputs masked_regions.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import re\n",
    "\n",
    "# re_string = \"\\|(.+)\\|\" # original version\n",
    "re_string = \"UniRef100_([A-Z0-9]+)\" # modified for UniRef100\n",
    "\n",
    "input = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_10M.fasta\"\n",
    "output = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/masked_regions.dat\"\n",
    "\n",
    "def parse_file(path, dom_type):\n",
    "    output_file = open(output, \"w\")\n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        # record.name = UniRef100_Q6GZX3\n",
    "        # record.description = UniRef100_Q6GZX3 Putative transc....\n",
    "        \n",
    "        # this removes the name from the description removes commas\n",
    "        raw_desc = record.description.replace(record.name+\" \", \"\")\n",
    "        raw_desc = raw_desc.replace(\",\", \"\")\n",
    "        \n",
    "        # extracts the id from the name\n",
    "        result = re.search(re_string, record.name)\n",
    "        uniprot_id = result.group(1)\n",
    "        \n",
    "        # loops throgh the sequence 3 dots at a time - not sure why\n",
    "        # is this supposed to return a line for each collection of at least 3 sequence characters?\n",
    "        # sequence characters?\n",
    "        #for m in re.finditer(r'\\.{3,}', str(record.seq)):          # original version\n",
    "        for m in re.finditer(r'.{3,}', str(record.seq)):            # modified for UniRef100\n",
    "            output_file.write(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" +\n",
    "                    str(m.start()+1)+\"\\t\"+str(m.end()+1) + '\\n')\n",
    "        # return()\n",
    "    output_file.close()\n",
    "\n",
    "parse_file(input, \"LowComplexity\")\n",
    "#parse_file(file, \"CoiledCoil\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disordered Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TRIED .gz file directly but too complex\n",
    "# \n",
    "\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "input = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/disordered/extra.xml.gz\"\n",
    "MAX_LINES = 10\n",
    "\n",
    "def parse_disordered():\n",
    "    #pattern = \"dbname=\\\"MOBIDBLT\\\"\"\n",
    "    pattern = \".*\"\n",
    "    \n",
    "    regex = re.compile(pattern)\n",
    "    buffer_size = 3\n",
    "    count = 0\n",
    "\n",
    "    # Open the gzip file\n",
    "    with gzip.open(input, 'rt') as file:  # 'rt' mode opens the file in text mode\n",
    "        buffer = []\n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            protein_match = re.search(\"<protein id=\\\"([A-Z0-9]+).*>\", line)\n",
    "            if(protein_match is not None):\n",
    "                uniprot_id = protein_match.groups(1)\n",
    "                print('Protein :', uniprot_id, line.strip())\n",
    "            else:\n",
    "                db_match = re.search(\"dbname=\\\"MOBIDBLT\\\"([A-Z0-9]+).*>\", line)\n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            count +=1\n",
    "            if count >= MAX_LINES:\n",
    "                break\n",
    "           \n",
    "            buffer.append(line)\n",
    "            # Keep the buffer size within the specified limit\n",
    "            if len(buffer) > buffer_size:\n",
    "                buffer.pop(0)\n",
    "            \n",
    "            # Join the buffer lines into a single string for regex matching\n",
    "            combined_lines = ''.join(buffer)\n",
    "            \n",
    "            matches = re.search(\"(.*)\", line)\n",
    "            if matches:\n",
    "                print('Match :', matches.groups(1), line.strip())\n",
    "            '''\n",
    "parse_disordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0A001\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t312\t340\n",
      "A0A001\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t571\t591\n",
      "A0A003\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t321\t340\n",
      "A0A007\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t368\t395\n",
      "A0A008\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t37\n",
      "A0A008\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t7\t21\n",
      "A0A009GNL6\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t27\t47\n",
      "A0A009GNL6\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t33\t47\n",
      "A0A009GPD0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t138\t164\n",
      "A0A009GRX3\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t61\t118\n",
      "A0A009GS49\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t499\t537\n",
      "A0A009GS49\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t511\t531\n",
      "A0A009GT65\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t121\t138\n",
      "A0A009GT65\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t121\t144\n",
      "A0A009GTE2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t112\t143\n",
      "A0A009GTE2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t115\t129\n",
      "A0A009GU26\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t343\t362\n",
      "A0A009GUZ2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t209\t273\n",
      "A0A009GUZ2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t212\t254\n",
      "A0A009GUZ2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t255\t269\n",
      "A0A009GVC4\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t44\t63\n",
      "A0A009GVG5\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t612\t627\n",
      "A0A009GVG5\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t612\t635\n",
      "A0A009GVV9\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t252\t271\n",
      "A0A009GVW2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t178\t201\n",
      "A0A009GVW2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t184\t201\n",
      "A0A009GVY1\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t82\t114\n",
      "A0A009GVY1\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t91\t114\n",
      "A0A009GWA0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t32\n",
      "A0A009GWD0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t20\n",
      "A0A009GY70\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t23\t67\n",
      "A0A009GY70\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t32\t47\n",
      "A0A009GYF0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t189\t223\n",
      "A0A009GYY0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t103\t131\n",
      "A0A009GYY0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t105\t120\n",
      "A0A009GZ83\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t23\n",
      "A0A009GZ83\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t24\n",
      "A0A009GZC0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t24\t44\n",
      "A0A009GZC0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t28\t44\n",
      "A0A009GZC5\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t20\n",
      "A0A009GZC5\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t161\t178\n",
      "A0A009GZC5\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t161\t180\n",
      "A0A009GZF0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t29\t76\n",
      "A0A009GZF0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t59\t76\n",
      "A0A009GZN8\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t575\t611\n",
      "A0A009GZN8\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t595\t611\n",
      "A0A009H000\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t711\t792\n",
      "A0A009H000\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t733\t768\n",
      "A0A009H045\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t25\t147\n",
      "A0A009H045\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t66\t89\n",
      "A0A009H045\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t90\t104\n",
      "A0A009H045\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t106\t126\n",
      "A0A009H0H9\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t34\n",
      "A0A009H0H9\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t44\n",
      "A0A009H0R6\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t409\t453\n",
      "A0A009H0R6\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t415\t443\n",
      "A0A009H0S0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t224\t247\n",
      "A0A009H0S0\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t230\t244\n",
      "A0A009H0Z7\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t522\t571\n",
      "A0A009H0Z7\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t550\t571\n",
      "A0A009H0Z9\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t137\t157\n",
      "A0A009H105\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t52\t66\n",
      "A0A009H105\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t52\t88\n",
      "A0A009H178\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t150\t173\n",
      "A0A009H178\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t150\t174\n",
      "A0A009H1E9\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t381\t620\n",
      "A0A009H1E9\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t415\t620\n",
      "A0A009H1L3\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t42\t69\n",
      "A0A009H1Q3\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t22\n",
      "A0A009H2E7\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t39\t61\n",
      "A0A009H2F4\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t17\n",
      "A0A009H2F4\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t35\n",
      "A0A009H2N6\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t32\t63\n",
      "A0A009H2N6\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t46\t63\n",
      "A0A009H2Q2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t530\t569\n",
      "A0A009H2Q2\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t548\t569\n",
      "A0A009H2Q3\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t832\t872\n",
      "A0A009H2Q9\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t27\t46\n",
      "A0A009H300\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t32\n",
      "A0A009H300\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t10\t32\n",
      "A0A009H367\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t103\t123\n",
      "A0A009H371\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t21\t55\n",
      "A0A009H371\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t28\t45\n",
      "A0A009H3B1\tIPRXXXXXX\tdisorder_prediction\tmobidb-lite\t1\t20\n"
     ]
    },
    {
     "ename": "ParseError",
     "evalue": "no element found: line 10001, column 0 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/conda_ucl_base/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[22], line 13\u001b[0m\n    for event, protein in context:\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/conda_ucl_base/lib/python3.12/xml/etree/ElementTree.py:1242\u001b[0m in \u001b[1;35miterator\u001b[0m\n    root = pullparser._close_and_return_root()\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda3/envs/conda_ucl_base/lib/python3.12/xml/etree/ElementTree.py:1290\u001b[0;36m in \u001b[0;35m_close_and_return_root\u001b[0;36m\n\u001b[0;31m    root = self._parser.close()\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m no element found: line 10001, column 0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This works directly on the uncompressed .gz file\n",
    "# No space on laptop for fully extracted extra.xml\n",
    "# Used this command to extract first 10000 lines into a separate file:\n",
    "#\n",
    "#\n",
    "\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "# get an iterable\n",
    "context = ElementTree.iterparse('/Users/patrick/dev/ucl/comp0158_mscproject/data/disordered/extra.10000.xml', events=(\"start\", \"end\"))\n",
    "\n",
    "# turn it into an iterator\n",
    "context = iter(context)\n",
    "\n",
    "# get the root element\n",
    "event, root = next(context)\n",
    "\n",
    "for event, protein in context:\n",
    "    if event == \"end\" and protein.tag == \"protein\":\n",
    "        # print(elem.attrib['id'])\n",
    "        for match in protein:\n",
    "            if 'MOBIDBLT' in match.attrib['dbname']:\n",
    "                for coords in match:\n",
    "                    print(protein.attrib['id']+\"\\tIPRXXXXXX\\t\" +\n",
    "                          match.attrib['name']+\"\\t\"+match.attrib['id']+\"\\t\" +\n",
    "                          coords.attrib['start']+\"\\t\"+coords.attrib['end'])\n",
    "        # exit()\n",
    "        root.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
