{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "import csv\n",
    "import duckdb\n",
    "\n",
    "# internal representation\n",
    "from protein_metadata import ProteinSentence\n",
    "from protein_metadata import ProteinWord\n",
    "from protein_db import ProteinDB\n",
    "\n",
    "# file handles\n",
    "protein_fasta_file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_10M.fasta\";\n",
    "interpro_file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\";\n",
    "\n",
    "pdb = ProteinDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\";\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam.dat\"; # protein2ipr with only pfam entries\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam_20M.dat\"; # protein2ipr with only pfam entries 0 20M lines only\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/A8KBH6_ipr_pfam.dat\"; # protein2ipr with only pfam entries - only for one protein\n",
    "\n",
    "#\n",
    "# greps for an id in interpro dat file\n",
    "# For A8KBH6, this will match A0A01A8KBH6 and A8KBH6\n",
    "# Time to parse the full protein2ipr.dat for A8KBH6 : 22min 56s\n",
    "#\n",
    "def grep_interpro(id):\n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            #match_string = \"รง\"            # works\n",
    "            #match_string = \"[A0A0-9]*A8KBH6\"   # works\n",
    "            #match_string = \"^[A0A0-9]*\"+id     # works\n",
    "            match_string = \"^[A0A0-9]*\"+id\n",
    "            match = re.search(match_string, line)\n",
    "            if match:\n",
    "                print('Matched:', id, 'in line:', line.strip())\n",
    "                \n",
    "grep_interpro('A8KBH6')\n",
    "grep_interpro('A0A1A8KBH6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ABSTRACTION AND LOADING INTO DB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TAXONOMY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxonomy names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Note that the default names.dmp file is not formatted very well and didn't parse easily\n",
    "# I had to use Perl to reformat it (perl_taxonomy.pl) and then imported the result here\n",
    "# I could probably just as easily have used python\n",
    "#\n",
    "# 4,057,294 lines read into DB in 50mins \n",
    "#\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/taxonomy/names_fixed.dmp\"\n",
    "\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "#con.execute(\"DROP TABLE TAX_NAME\")\n",
    "\n",
    "con.execute(\" \\\n",
    "    CREATE TABLE TAX_NAME (\\\n",
    "                TAX_ID VARCHAR,\\\n",
    "                NAME VARCHAR,\\\n",
    "                UNIQUE_NAME VARCHAR,\\\n",
    "                NAME_CLASS VARCHAR)\" \\\n",
    "                )\n",
    "\n",
    "with open(file, mode='r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter='|')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        #print(f'{row[0]} : {row[1]} : {row[2]} :  {row[3]}')\n",
    "        id = row[0]\n",
    "        name = row[1]\n",
    "        unique_name = row[2]\n",
    "        name_class = row[3]\n",
    "        \n",
    "        con.execute(\"INSERT INTO TAX_NAME (TAX_ID, NAME, UNIQUE_NAME, NAME_CLASS) VALUES(?,?,?,?)\", (id, name, unique_name, name_class))\n",
    "        \n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad index after data load \n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "con.execute(\"CREATE INDEX tn_txid_idx ON TAX_NAME(TAX_ID)\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "count = con.execute(\"SELECT COUNT(*) FROM TAX_NAME\").fetchall()\n",
    "print('Number of rows', count)\n",
    "res = con.execute(\"SELECT * FROM TAX_NAME\").fetchall()\n",
    "for i in range(10):\n",
    "    print(res[i])\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop table\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "#con.execute(\"DROP TABLE TAX_CAT\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "con.execute(\" \\\n",
    "    CREATE TABLE TAX_CAT (\\\n",
    "                TYPE VARCHAR,\\\n",
    "                SPECIES_ID VARCHAR,\\\n",
    "                CAT_ID VARCHAR)\" \\\n",
    "                )\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load categories\n",
    "import csv\n",
    "\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/taxonomy/categories.dmp\"\n",
    "\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "\n",
    "with open(file, mode='r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        #print(f'{row[0]} : {row[1]} : {row[2]}')\n",
    "        type = row[0]\n",
    "        species = row[1]\n",
    "        cat = row[2]\n",
    "        \n",
    "        con.execute(\"INSERT INTO TAX_CAT (TYPE, SPECIES_ID, CAT_ID) VALUES(?,?,?)\", (type, species, cat))\n",
    "\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "    \n",
    "con.close()\n",
    "\n",
    "# This apears to work but doesn't for some reason\n",
    "#con.execute(f'''\n",
    "#    COPY TAX_CAT FROM '{file}' (AUTO_DETECT TRUE)\n",
    "#''')\n",
    "\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "con.execute(\"CREATE INDEX TCAT_CATID_IDX ON TAX_CAT(CAT_ID)\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "\n",
    "count = con.execute(\"SELECT COUNT(*) FROM TAX_CAT WHERE TYPE='E'\").fetchall()\n",
    "#result = con.execute(\"SELECT * FROM TAX_CAT where SPECIES_ID='2759'\").fetchall()\n",
    "print(count)\n",
    "#print(result)\n",
    "#for i in range(10):\n",
    "#    print(result[i])\n",
    "#con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UNIREF100 FASTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "#con.execute(\"DROP TABLE PROTEIN\")\n",
    "#con.execute(\"DROP INDEX PROT_ID_X PROTEIN\")\n",
    "pdb.create_protein_table()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read proteins into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Reads fasta files into db\n",
    "# 100k lines takes : 2min\n",
    "# 1M : 20min\n",
    "# 10M : 3hrs 20min ? (started 13:44)\n",
    "# \n",
    "# UniRef100.fasta has 1,355,591,115 lines for which there are \n",
    "# 12,814,583 entries created in 342m\n",
    "# without an index a query on select * and then selecting a particujlr item took 55.9s\n",
    "# index took 5.5s to apply\n",
    "# same query after index took \n",
    "#\n",
    "def parse_fasta_to_db(dom_type):\n",
    "    \n",
    "    path        = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_100M.fasta\"\n",
    "    \n",
    "    uniprot_re      = \"UniRef100_([A-Z0-9]+)\" # modified for UniRef100\n",
    "    tax_id_re       = \"TaxID=([0-9]+)\"\n",
    "    #tax_name_re     = \"Tax=([a-zA-Z0-9\\s]+)\\s\"\n",
    "    tax_name_re     = \"Tax=(.+)[\\s\\t]TaxID\"\n",
    "    name_string     = \"([a-zA-Z0-9\\s-]+)n=\"\n",
    "    rep_id_re       = \"RepID=(.*)\"\n",
    "    \n",
    "    PROCESS_LIMIT   = -1 # number of lines to process, set to -1 to ignore\n",
    "    OUTPUT_LIMIT    = 10000  # determines how often to print a progress message\n",
    "    \n",
    "    line_number = 0\n",
    "    count = 0\n",
    "    con = duckdb.connect(database=ProteinDB.db_string) \n",
    "    \n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        \n",
    "        # ------- extract data from name -----------\n",
    "        \n",
    "        # this removes the name from the description and removes commas\n",
    "        raw_desc = record.description.replace(record.name+\" \", \"\")\n",
    "        raw_desc = raw_desc.replace(\",\", \"\")\n",
    "        \n",
    "        # extracts the id from the name\n",
    "        result      = re.search(uniprot_re, record.name)\n",
    "        uniprot_id  = result.group(1)\n",
    "        \n",
    "        name_q = re.search(name_string, raw_desc)\n",
    "        short_desc = name_q.group(1) \n",
    "        \n",
    "        rep_id=\"\"\n",
    "        rep_id_res = re.search(rep_id_re, raw_desc)\n",
    "        if rep_id_res is not None:\n",
    "            rep_id = rep_id_res.group(1)\n",
    "        \n",
    "        # need to get tax id\n",
    "        tax_id_res = re.search(tax_id_re, raw_desc)\n",
    "        tax_id = tax_id_res.group(1)\n",
    "        \n",
    "        tax_nm_res = re.search(tax_name_re, raw_desc)\n",
    "        if tax_nm_res is not None:\n",
    "            tax_name = tax_nm_res.group(1)\n",
    "        else:\n",
    "            print ('No tax name :', raw_desc )\n",
    "        \n",
    "        \n",
    "        # -------- check for termination ------------\n",
    "        #\n",
    "        if ((line_number // OUTPUT_LIMIT > 0) and (line_number % OUTPUT_LIMIT) == 0):\n",
    "            count += 1\n",
    "            print(count * OUTPUT_LIMIT, 'lines processed.....')\n",
    "        if(PROCESS_LIMIT != -1):\n",
    "            if line_number == PROCESS_LIMIT:\n",
    "                print('Last entry:', uniprot_id)\n",
    "            if line_number >= PROCESS_LIMIT:\n",
    "                print('Processing limit reached %s stopping' % (PROCESS_LIMIT))\n",
    "                break\n",
    "        line_number += 1\n",
    "        # ------------------------------------\n",
    "        \n",
    "        \n",
    "        # -------- get length ------------\n",
    "        #\n",
    "        # loops throgh the sequence 3 dots at a time - not sure why? Is this supposed to \n",
    "        # return a line for each collection of at least 3 sequence characters?\n",
    "        #for m in re.finditer(r'\\.{3,}', str(record.seq)):          # original version\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for m in re.finditer(r'.{3,}', str(record.seq)):            # modified for UniRef100\n",
    "            #print(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" + str(m.start()+1)+\"\\t\"+str(m.end()+1))\n",
    "            #ipr=\"IPRXXXXXX\"\n",
    "            start = str(m.start()+1)\n",
    "            end = str(m.end()+1)\n",
    "            #con.execute(\"INSERT INTO PROTEIN_SENTENCE (UNIPROT_ID, IPR, DESCRIPTION, DOM_TYPE, START_POS, END_POS) VALUES(?,?,?,?,?,?,)\", (uniprot_id, ipr, raw_desc, dom_type, start, end))\n",
    "            #output_file.write(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" + str(m.start()+1)+\"\\t\"+str(m.end()+1) + '\\n')\n",
    "        # return()\n",
    "        \n",
    "        '''\n",
    "        #print('full name :', record.name)\n",
    "        print('uniprot_id :', uniprot_id)\n",
    "        print('full desc :', raw_desc)\n",
    "        print('short desc :', short_desc)\n",
    "        print('tax name :', tax_name)\n",
    "        print('tax id :', tax_id)\n",
    "        print('rep id :', rep_id)\n",
    "        print('start :', start)\n",
    "        print('end :', end, '\\n')\n",
    "        '''\n",
    "        \n",
    "        con.execute(\"INSERT INTO PROTEIN (UNIPROT_ID, SHORT_DESCRIPTION, TAX_NAME, TAX_ID, DOM_TYPE, REP_ID, START_POS, END_POS) VALUES(?,?,?,?,?,?,?,?)\", (uniprot_id, short_desc, tax_name, tax_id, dom_type, rep_id, start, end))\n",
    "        \n",
    "        \n",
    "    con.close()\n",
    "\n",
    "parse_fasta_to_db(\"LowComplexity\")\n",
    "#parse_file(file, \"CoiledCoil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eukaryotic_fasta(dom_type):\n",
    "    \n",
    "    path        = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_100M.fasta\"\n",
    "    \n",
    "    uniprot_re      = \"UniRef100_([A-Z0-9]+)\" # modified for UniRef100\n",
    "    tax_id_re       = \"TaxID=([0-9]+)\"\n",
    "\n",
    "    PROCESS_LIMIT   = 10000000 # number of lines to process, set to -1 to ignore\n",
    "    OUTPUT_LIMIT    = 1000000  # determines how often to print a progress message\n",
    "    \n",
    "    line_number = 0\n",
    "    count = 0\n",
    "    \n",
    "    #con = duckdb.connect(database=ProteinDB.db_string) \n",
    "    \n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        \n",
    "        # -------- check for termination ------------\n",
    "        #\n",
    "        if ((line_number // OUTPUT_LIMIT > 0) and (line_number % OUTPUT_LIMIT) == 0):\n",
    "            count += 1\n",
    "            print(count * OUTPUT_LIMIT, 'lines processed.....')\n",
    "        if(PROCESS_LIMIT != -1):\n",
    "            if line_number >= PROCESS_LIMIT:\n",
    "                print('Processing limit reached %s stopping' % (PROCESS_LIMIT))\n",
    "                break\n",
    "        line_number += 1\n",
    "        \n",
    "        \n",
    "        # ------- if eukaryotic.... -----------\n",
    "        \n",
    "        # need to get tax id\n",
    "        tax_id_res = re.search(tax_id_re, record.description)\n",
    "        tax_id = tax_id_res.group(1)\n",
    "        \n",
    "        if(tax_id == '2759'):\n",
    "            #print('Found eukaryotic protein...')\n",
    "        \n",
    "            # extracts the id from the name\n",
    "            uniprot_res = re.search(uniprot_re, record.name)\n",
    "            uniprot_id  = uniprot_res.group(1)\n",
    "    \n",
    "            # -------- get length ------------\n",
    "            # loops throgh the sequence 3 dots at a time - not sure why? Is this supposed to \n",
    "            # return a line for each collection of at least 3 sequence characters?\n",
    "            #for m in re.finditer(r'\\.{3,}', str(record.seq)):          # original version\n",
    "            start = 0\n",
    "            end = 0\n",
    "            for m in re.finditer(r'.{3,}', str(record.seq)):            # modified for UniRef100\n",
    "                start = str(m.start()+1)\n",
    "                end = str(m.end()+1)\n",
    "\n",
    "            #print(uniprot_id, tax_id, start, end)\n",
    "            print('id %s taxonomy %s start %s end %s ' % (uniprot_id, tax_id, start, end))\n",
    "\n",
    "        \n",
    "        #con.execute(\"INSERT INTO PROTEIN (UNIPROT_ID, SHORT_DESCRIPTION, TAX_NAME, TAX_ID, DOM_TYPE, REP_ID, START_POS, END_POS) VALUES##(?,?,?,?,?,?,?,?)\", (uniprot_id, short_desc, tax_name, tax_id, dom_type, rep_id, start, end))\n",
    "        \n",
    "    #con.close()\n",
    "\n",
    "parse_eukaryotic_fasta(\"LowComplexity\")\n",
    "#parse_file(file, \"CoiledCoil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb.create_protein_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the connection worked\n",
    "con = duckdb.connect(database=ProteinDB.db_string)           \n",
    "\n",
    "#count = con.execute(\"SELECT COUNT(*) FROM PROTEIN\").fetchall()\n",
    "#result = con.execute(\"SELECT * FROM PROTEIN\").fetchall()\n",
    "result = con.execute(\"SELECT * FROM PROTEIN WHERE PROTEIN.UNIPROT_ID = 'A0JP26'\").fetchall()\n",
    "print(result)\n",
    "\n",
    "#print(result[1500859])\n",
    "'''\n",
    "for i in range(10):\n",
    "    print(result[i])\n",
    "con.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TeEMBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# PARSE TREMBL FASTA \n",
    "#\n",
    "def parse_trembl_fasta(dom_type):\n",
    "    path        = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniprot_trembl_10M.fasta\"\n",
    "    uniprot_re  = \"tr\\|([A-Z0-9]+)\\|\" # modified for UniRef100\n",
    "    \n",
    "    con = duckdb.connect(database=ProteinDB.db_string) \n",
    "    \n",
    "    PROCESS_LIMIT = 10000\n",
    "    record_count = 0\n",
    "    \n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "            \n",
    "        # -------- check for termination ------------\n",
    "        #\n",
    "        if(PROCESS_LIMIT != -1):\n",
    "            if record_count >= PROCESS_LIMIT:\n",
    "                print('Last entry:', record.name)\n",
    "                print('Processing limit reached %s stopping' % (PROCESS_LIMIT))\n",
    "                break\n",
    "        record_count += 1\n",
    "        # ------------------------------------\n",
    "\n",
    "        result      = re.search(uniprot_re, record.name)\n",
    "        uniprot_id  = result.group(1)\n",
    "                \n",
    "        pfam_res = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID = ?\", [uniprot_id]).fetchall()\n",
    "        if pfam_res is not None:\n",
    "            if len(pfam_res) > 0:\n",
    "                print(uniprot_id)\n",
    "                print(pfam_res)\n",
    "        \n",
    "        #con.execute(\"INSERT INTO PROTEIN (UNIPROT_ID, SHORT_DESCRIPTION, TAX_NAME, TAX_ID, DOM_TYPE, REP_ID, START_POS, END_POS) VALUES##(?,?,?,?,?,?,?,?)\", (uniprot_id, short_desc, tax_name, tax_id, dom_type, rep_id, start, end))\n",
    "     \n",
    "    #con.close()\n",
    "\n",
    "parse_trembl_fasta(\"LowComplexity\")\n",
    "#parse_trembl_fasta(file, \"CoiledCoil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PFAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse protein2ipr.dat into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "con.execute(\"DROP TABLE PROTEIN_WORD\")\n",
    "#con.execute(\"DROP INDEX WORD_PROT_ID_X\")\n",
    "pdb.create_word_table()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10M lines 10000000 : 20s\n",
    "#MAX_LINES = 10000000\n",
    "\n",
    "limit       = True # if True, onLy parses Max_lines lines \n",
    "MAX_COUNT   = 10000000\n",
    "OUTPUT_LIMIT = 10000\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 26 June 2024 - creating a file\n",
    "# parses protein2ipr.dat for entries with'PFNNN' and outpts those lines to a separate file\n",
    "# protein2ipr.dat       : 98.7GB,   1,355,591,115 entries\n",
    "# protein2ipr_new.dat   : 20.73GB,  298,766,058 entries\n",
    "# parsing time          : 23mins\n",
    "#\n",
    "# 01 July - Laoding direct into DB Table (PROTEIN_WORD)\n",
    "# Parses protein2ipr.dat, extracts ky info and writes it to DB\n",
    "# 2M in 10mins\n",
    "# 7.5M 40min\n",
    "# 10M in 50min > only produces 2.2M rows\n",
    "#\n",
    "# last entry processed : A0A074Z763\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "input = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\"\n",
    "\n",
    "#\n",
    "# parse an Interpro file and grep out those with pfam domains\n",
    "# Parsing the full protein2ipr.dat file in python took: 23mins 40s\n",
    "#\n",
    "def create_pfam_word():\n",
    "    count  = 0\n",
    "    \n",
    "    con = duckdb.connect(database=ProteinDB.db_string) \n",
    "    \n",
    "    uniprot_id = \"\"\n",
    "    with open(input, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            \n",
    "            # -------- check for termination ------------\n",
    "            #\n",
    "            if ((line_number // OUTPUT_LIMIT > 0) and (line_number % OUTPUT_LIMIT) == 0):\n",
    "                count += 1\n",
    "                print(count * OUTPUT_LIMIT, 'lines processed.....')\n",
    "            if(MAX_COUNT != -1):\n",
    "                if line_number > MAX_COUNT:\n",
    "                    print('Processing limit reached %s stopping. Last entry was %s' % (MAX_COUNT, uniprot_id))\n",
    "                    break\n",
    "            # ------------------------------------  \n",
    "            \n",
    "\n",
    "            match = re.search(\"([A0A0-9]*[a-zA-Z0-9]+)\\\\tIPR[0-9]+\\\\t.*\\\\t(PF[0-9]+)\\\\t([0-9]+)\\\\t([0-9]+)\", line)\n",
    "\n",
    "            if match is not None:\n",
    "                uniprot_id  = match.group(1)\n",
    "                pfam_word   = match.group(2)\n",
    "                start       = match.group(3)\n",
    "                end         = match.group(4)\n",
    "                item_type   = \"PFAM\"\n",
    "                \n",
    "                #print(uniprot_id, '\\t', pfam_word, '\\t', start, '\\t', end)\n",
    "                \n",
    "                con.execute(\"INSERT INTO PROTEIN_WORD (UNIPROT_ID, WORD_TYPE, REF_ID, START_POS, END_POS) VALUES(?,?,?,?,?)\", (uniprot_id, item_type, pfam_word, start, end))\n",
    "    con.close()\n",
    "\n",
    "create_pfam_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb.create_pfam_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the connection worked\n",
    "con = duckdb.connect(database=ProteinDB.db_string)           \n",
    "#id = 'A0A059X392'\n",
    "#id = 'A0A009GV07' # this has multiple pfam entries\n",
    "#id = 'A0A009GMU8'\n",
    "#id = 'AA0A074Z763'\n",
    "#result = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID = ?\", [id]).fetchall()\n",
    "\n",
    "result = con.execute(\"SELECT COUNT(*) FROM PROTEIN_WORD\").fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query pfam entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qurery_pfam(uniprot_id):\n",
    "    con = duckdb.connect(database=ProteinDB.db_string)           \n",
    "    result = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID = ?\", [id]).fetchall()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DISORDERED REGIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS WILL ERROR IF YOU ONLY USE A PARTIAL FILE\n",
    "#\n",
    "\n",
    "# This works directly on the uncompressed .gz file\n",
    "# No space on laptop for fully extracted extra.xml\n",
    "#\n",
    "\n",
    "# Used this command to extract first 10000 lines into a separate file:\n",
    "#\n",
    "# zgrep . -m 10000 data/disordered/extra.xml.gz > data/disordered/extra.10000.xml\n",
    "#\n",
    "\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "file = '/Users/patrick/dev/ucl/comp0158_mscproject/data/disordered/extra.10000.xml'\n",
    "\n",
    "# get an iterable\n",
    "context = ElementTree.iterparse(file, events=(\"start\", \"end\"))\n",
    "\n",
    "# turn it into an iterator\n",
    "context = iter(context)\n",
    "\n",
    "# get the root element\n",
    "event, root = next(context)\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    " \n",
    "for event, protein in context:\n",
    "    if event == \"end\" and protein.tag == \"protein\":\n",
    "        # print(elem.attrib['id'])\n",
    "        for match in protein:\n",
    "            if 'MOBIDBLT' in match.attrib['dbname']:\n",
    "                for coords in match:\n",
    "                    uniprot_id = protein.attrib['id']\n",
    "                    word_type=\"DISORDER\"\n",
    "                    start_pos = coords.attrib['start']\n",
    "                    end_pos = coords.attrib['end']\n",
    "                    '''\n",
    "                    print(protein.attrib['id']+\"\\tIPRXXXXXX\\t\" +\n",
    "                          match.attrib['name']+\"\\t\"+match.attrib['id']+\"\\t\" +\n",
    "                          coords.attrib['start']+\"\\t\"+coords.attrib['end'])\n",
    "                    '''\n",
    "                    con.execute(\"INSERT INTO PROTEIN_WORD (UNIPROT_ID, WORD_TYPE, START_POS, END_POS) VALUES(?,?,?,?)\", (uniprot_id, word_type, start_pos, end_pos))\n",
    "        # exit()\n",
    "        root.clear()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the connection worked\n",
    "con = duckdb.connect(database=ProteinDB.db_string)           \n",
    "word_type=\"DISORDER\"\n",
    "count = con.execute(\"SELECT COUNT(*) FROM PROTEIN_WORD WHERE WORD_TYPE = ?\", [word_type]).fetchall()\n",
    "print(count)\n",
    "\n",
    "result = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE WORD_TYPE = ?\", [word_type]).fetchall()\n",
    "for i in range(10):\n",
    "    print(res[i])\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the connection worked\n",
    "con = duckdb.connect(database=ProteinDB.db_string)           \n",
    "word_type=\"DISORDER\"\n",
    "result = con.execute(\"DELETE FROM PROTEIN_WORD WHERE WORD_TYPE = ?\", [word_type])\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 font color=\"yellow\">WORD SENTENCES</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxonomy Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "\n",
    "cat_result = con.execute(\"SELECT * FROM TAX_CAT WHERE TYPE = 'E'\").fetchall()\n",
    "\n",
    "for i in range(10):\n",
    "    print(cat_result[i])\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxonomy Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "\n",
    "name_result = con.execute(\"SELECT * FROM TAX_NAME\").fetchall()\n",
    "\n",
    "for i in range(10):\n",
    "    print(name_result[i])\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxonomy Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(name_result[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "\n",
    "cat_result = con.execute(\"SELECT * FROM TAX_CAT WHERE TYPE='E'\").fetchall()\n",
    "for i in range(10):\n",
    "    print(cat_result[i])\n",
    "\n",
    "\n",
    "#item_id = '10492'\n",
    "item_id = '2777'\n",
    "\n",
    "name_result = con.execute(\"SELECT * FROM TAX_NAME WHERE TAX_ID = ?\", [item_id]).fetchall()\n",
    "\n",
    "print(name_result)\n",
    "\n",
    "cat_result = con.execute(\"SELECT * FROM TAX_CAT WHERE TAX_CAT.SPECIES_ID = ?\", [item_id]).fetchall()\n",
    "\n",
    "print(cat_result)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "#id = 'A0A059X392'\n",
    "id = 'A0A009GV07' # this has multiple pfam entries\n",
    "#id = 'A0A009GMU8'\n",
    "#id = 'AA0A074Z763'\n",
    "\n",
    "#pfam_result = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID = ?\", [id]).fetchall()\n",
    "\n",
    "\n",
    "pfam_result = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID IN ('A0A009GV07','Q6GZX4', 'Q6GZX3', 'Q197F8', 'Q197F7','Q6GZX2', 'Q6GZX1', 'Q197F5','Q6GZX0','Q91G88','Q6GZW8')\").fetchall()\n",
    "\n",
    "print(pfam_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "id = 'A0A059X392'\n",
    "#id = 'A0A009GV07' # this has multiple pfam entries\n",
    "#id = 'A0A009GMU8'\n",
    "#id = 'AA0A074Z763'\n",
    "prot_count= con.execute(\"SELECT COUNT(*) FROM PROTEIN\").fetchall()\n",
    "print('Num proteins in DB:', prot_count)\n",
    "\n",
    "print('First 10:\\n')\n",
    "prot_result = con.execute(\"SELECT * FROM PROTEIN\").fetchall()\n",
    "for i in range(10):\n",
    "    print(prot_result[i])\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query pfam entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "\n",
    "id = 'A0A009GV07' # this has multiple pfam entries\n",
    "#id = '09GV07' # this has multiple pfam entries\n",
    "\n",
    "\n",
    "pfam_result = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID = ?\", [id]).fetchall()\n",
    "\n",
    "#pfam_result = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID IN ('A0A009GV07','Q6GZX4', 'Q6GZX3', 'Q197F8', 'Q197F7','Q6GZX2', 'Q6GZX1', 'Q197F5','Q6GZX0','Q91G88','Q6GZW8')\").fetchall()\n",
    "\n",
    "print(pfam_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query disorder regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "\n",
    "disorder_result = con.execute(\"SELECT * FROM PROTEIN_WORD WHERE WORD_TYPE='DISORDER' \").fetchall()\n",
    "\n",
    "for i in range(10):\n",
    "    print(disorder_result[i])\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=ProteinDB.db_string)\n",
    "id = 'A0A059X392'\n",
    "#id = 'A0A009GV07' # this has multiple pfam entries\n",
    "#id = 'A0A009GMU8'\n",
    "#id = 'AA0A074Z763'\n",
    "prot_result = con.execute(\"SELECT * FROM PROTEIN WHERE UNIPROT_ID = ?\", [id]).fetchall()\n",
    "print(prot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------- OLD STUFF -----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD : UNIREF FASTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deprecated : Parse UniRef100 FASTA 1 > Check for ProteinSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 500k is enough for matches to be found\n",
    "MAX_LINES = 10000000\n",
    "matched = []\n",
    "not_matched = []\n",
    "\n",
    "#\n",
    "# parse a fasta file to get protein ids\n",
    "# for uniref, these ids are the characters after UniRef100_\n",
    "# TODO: Check if ids are proteins or protein clusters\n",
    "#\n",
    "def parse_fasta():\n",
    "    with open(protein_fasta_file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            \n",
    "            if line_number > MAX_LINES:  # line_number starts at 0.\n",
    "                break\n",
    "            #print('Processing :', line)\n",
    "            # note that the raw interpro file is tab delimited between fields\n",
    "            match = re.search(\"UniRef100_([A-Z0-9]+) \", line)\n",
    "            if match is not None:\n",
    "                id = match.group(1)\n",
    "\n",
    "                if id in sentences.keys():\n",
    "                    #print(line_number, 'Found sentence for protein :', id, sentences[id].text)\n",
    "                    if id not in matched:\n",
    "                        matched.append(id)\n",
    "                else:\n",
    "                    not_matched.append(id)\n",
    "parse_fasta()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deprecated : Parse UniRef100 FASTA 2 > Extract id, length, taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 500k is enough for matches to be found\n",
    "MAX_LINES = 1000\n",
    "matched = []\n",
    "not_matched = []\n",
    "\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_500M.fasta\"\n",
    "\n",
    "#\n",
    "# parse a fasta file to get protein ids\n",
    "# for uniref, these ids are the characters after UniRef100_\n",
    "# TODO: Check if ids are proteins or protein clusters\n",
    "#\n",
    "def parse_fasta_2():\n",
    "    matchline = \"\"\n",
    "    id = \"\"\n",
    "    match_inprogress = False\n",
    "    \n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            \n",
    "            if line_number > MAX_LINES:  # line_number starts at 0.\n",
    "                break\n",
    "\n",
    "            match = re.search(\">UniRef100_([A-Z0-9]+).*TaxID=([0-9]+).*\", line)\n",
    "            \n",
    "            #match = re.search(\">UniRef100_([A-Z0-9]+)\", line) # works\n",
    "            \n",
    "            if match is not None:\n",
    "                if(id != \"\"):\n",
    "                    print(id, '\\t', len(matchline), '\\t', tax_id, '\\t', matchline)\n",
    "                matchline = \"\"\n",
    "                id = match.group(1)\n",
    "                tax_id = match.group(2)\n",
    "                continue\n",
    "            else:\n",
    "                matchline += line.strip()\n",
    "parse_fasta_2()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(matched),'Matched proteins:\\n', matched)\n",
    "print(len(not_matched), 'Unmatched proteins:\\n',not_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deprecated : Parse UniRef100 FASTA 3 > Modified parse_masked_regions from Daniel - outputs masked_regions.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "\n",
    "# re_string = \"\\|(.+)\\|\" # original version\n",
    "re_string = \"UniRef100_([A-Z0-9]+)\" # modified for UniRef100\n",
    "\n",
    "input = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_10M.fasta\"\n",
    "output = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/masked_regions.dat\"\n",
    "\n",
    "def parse_file(path, dom_type):\n",
    "    output_file = open(output, \"w\")\n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        # record.name = UniRef100_Q6GZX3\n",
    "        # record.description = UniRef100_Q6GZX3 Putative transc....\n",
    "        \n",
    "        # this removes the name from the description removes commas\n",
    "        raw_desc = record.description.replace(record.name+\" \", \"\")\n",
    "        raw_desc = raw_desc.replace(\",\", \"\")\n",
    "        \n",
    "        # extracts the id from the name\n",
    "        result = re.search(re_string, record.name)\n",
    "        uniprot_id = result.group(1)\n",
    "        \n",
    "        # loops throgh the sequence 3 dots at a time - not sure why\n",
    "        # is this supposed to return a line for each collection of at least 3 sequence characters?\n",
    "        # sequence characters?\n",
    "        #for m in re.finditer(r'\\.{3,}', str(record.seq)):          # original version\n",
    "        for m in re.finditer(r'.{3,}', str(record.seq)):            # modified for UniRef100\n",
    "            print(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" + str(m.start()+1)+\"\\t\"+str(m.end()+1))\n",
    "            #output_file.write(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" + str(m.start()+1)+\"\\t\"+str(m.end()+1) + '\\n')\n",
    "        # return()\n",
    "    output_file.close()\n",
    "\n",
    "parse_file(input, \"LowComplexity\")\n",
    "#parse_file(file, \"CoiledCoil\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEPRECATED - Read UniRef proteins into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#\n",
    "def parse_fasta_to_db(dom_type):\n",
    "    \n",
    "    path = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_10M.fasta\"\n",
    "    re_string = \"UniRef100_([A-Z0-9]+)\" # modified for UniRef100\n",
    "    \n",
    "    PROCESS_LIMIT   = 1000 # number of lines to process, set to -1 to ignore\n",
    "    OUTPUT_LIMIT    = 100  # determines how often to print a progress message\n",
    "    \n",
    "    line_number = 0\n",
    "    count = 0\n",
    "    con = duckdb.connect(database=ProteinDB.db_string) \n",
    "    \n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        \n",
    "        # this removes the name from the description and removes commas\n",
    "        raw_desc = record.description.replace(record.name+\" \", \"\")\n",
    "        raw_desc = raw_desc.replace(\",\", \"\")\n",
    "        \n",
    "        # extracts the id from the name\n",
    "        result = re.search(re_string, record.name)\n",
    "        uniprot_id = result.group(1)\n",
    "        \n",
    "        # check if want to output progress\n",
    "        if ((line_number // OUTPUT_LIMIT > 0) and (line_number % OUTPUT_LIMIT) == 0):\n",
    "            count += 1\n",
    "            print(count * OUTPUT_LIMIT, 'lines processed.....')\n",
    "                \n",
    "        # check if we've hit the line limit\n",
    "        if(PROCESS_LIMIT != -1):\n",
    "            if line_number == PROCESS_LIMIT:\n",
    "                print('Last entry:', uniprot_id)\n",
    "            if line_number > PROCESS_LIMIT:\n",
    "                print('Processing limit reached %s stopping' % (PROCESS_LIMIT))\n",
    "                break\n",
    "        \n",
    "        line_number += 1\n",
    "        \n",
    "        # loops throgh the sequence 3 dots at a time - not sure why? Is this supposed to \n",
    "        # return a line for each collection of at least 3 sequence characters?\n",
    "        #for m in re.finditer(r'\\.{3,}', str(record.seq)):          # original version\n",
    "        for m in re.finditer(r'.{3,}', str(record.seq)):            # modified for UniRef100\n",
    "            #print(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" + str(m.start()+1)+\"\\t\"+str(m.end()+1))\n",
    "            ipr=\"IPRXXXXXX\"\n",
    "            start = str(m.start()+1)\n",
    "            end = str(m.end()+1)\n",
    "            con.execute(\"INSERT INTO PROTEIN_SENTENCE (UNIPROT_ID, IPR, DESCRIPTION, DOM_TYPE, START_POS, END_POS) VALUES(?,?,?,?,?,?,)\", (uniprot_id, ipr, raw_desc, dom_type, start, end))\n",
    "            #output_file.write(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" + str(m.start()+1)+\"\\t\"+str(m.end()+1) + '\\n')\n",
    "        # return()\n",
    "    con.close()\n",
    "\n",
    "parse_fasta_to_db(\"LowComplexity\")\n",
    "#parse_file(file, \"CoiledCoil\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta_to_db_tax(dom_type):\n",
    "    \n",
    "    path        = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_10M.fasta\"\n",
    "    \n",
    "    re_string       = \"UniRef100_([A-Z0-9]+)\" # modified for UniRef100\n",
    "    tax_id_string   = \"TaxID=([0-9]+)\"\n",
    "    tax_string      = \"Tax=([a-zA-Z0-9\\s]+)TaxID\"\n",
    "    name_string     = \"([a-zA-Z0-9\\s-]+)n=\"\n",
    "    \n",
    "    PROCESS_LIMIT   = 100 # number of lines to process, set to -1 to ignore\n",
    "    OUTPUT_LIMIT    = 100  # determines how often to print a progress message\n",
    "    \n",
    "    line_number = 0\n",
    "    count = 0\n",
    "    con = duckdb.connect(database=ProteinDB.db_string) \n",
    "    \n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        \n",
    "        # ------- extract data from name -----------\n",
    "        \n",
    "        # this removes the name from the description and removes commas\n",
    "        raw_desc = record.description.replace(record.name+\" \", \"\")\n",
    "        raw_desc = raw_desc.replace(\",\", \"\")\n",
    "        \n",
    "        # extracts the id from the name\n",
    "        result      = re.search(re_string, record.name)\n",
    "        uniprot_id  = result.group(1)\n",
    "        \n",
    "        name_q = re.search(name_string, raw_desc)\n",
    "        short_name = name_q.group(1) \n",
    "        \n",
    "        # need to get tax id\n",
    "        tax_id_q = re.search(tax_id_string, raw_desc)\n",
    "        tax_id = tax_id_q.group(1)\n",
    "        \n",
    "        tax_nm_res = re.search(tax_string, raw_desc)\n",
    "        if tax_nm_res is not None:\n",
    "            tax_name = tax_nm_res.group(1)\n",
    "        else:\n",
    "            print ('No tax name :', raw_desc )\n",
    "        \n",
    "        \n",
    "        # -------- check for termination ------------\n",
    "        #\n",
    "        if ((line_number // OUTPUT_LIMIT > 0) and (line_number % OUTPUT_LIMIT) == 0):\n",
    "            count += 1\n",
    "            print(count * OUTPUT_LIMIT, 'lines processed.....')\n",
    "        if(PROCESS_LIMIT != -1):\n",
    "            if line_number == PROCESS_LIMIT:\n",
    "                print('Last entry:', uniprot_id)\n",
    "            if line_number > PROCESS_LIMIT:\n",
    "                print('Processing limit reached %s stopping' % (PROCESS_LIMIT))\n",
    "                break\n",
    "        line_number += 1\n",
    "        # ------------------------------------\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # -------- get length ------------\n",
    "        #\n",
    "        # loops throgh the sequence 3 dots at a time - not sure why? Is this supposed to \n",
    "        # return a line for each collection of at least 3 sequence characters?\n",
    "        #for m in re.finditer(r'\\.{3,}', str(record.seq)):          # original version\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for m in re.finditer(r'.{3,}', str(record.seq)):            # modified for UniRef100\n",
    "            #print(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" + str(m.start()+1)+\"\\t\"+str(m.end()+1))\n",
    "            ipr=\"IPRXXXXXX\"\n",
    "            start = str(m.start()+1)\n",
    "            end = str(m.end()+1)\n",
    "            print('start :', start)\n",
    "            print('end :', end, '\\n')\n",
    "            #con.execute(\"INSERT INTO PROTEIN_SENTENCE (UNIPROT_ID, IPR, DESCRIPTION, DOM_TYPE, START_POS, END_POS) VALUES(?,?,?,?,?,?,)\", (uniprot_id, ipr, raw_desc, dom_type, start, end))\n",
    "            #output_file.write(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" + str(m.start()+1)+\"\\t\"+str(m.end()+1) + '\\n')\n",
    "        # return()\n",
    "        \n",
    "        print('full name :', record.name)\n",
    "        print('uniprot_id :', uniprot_id)\n",
    "        print('short name :', short_name)\n",
    "        print('desc :', raw_desc)\n",
    "        print('tax name :', tax_name)\n",
    "        print('tax id :', tax_id, '\\n')\n",
    "        print('start :', start)\n",
    "        print('end :', end, '\\n')\n",
    "        \n",
    "    con.close()\n",
    "\n",
    "parse_fasta_to_db_tax(\"LowComplexity\")\n",
    "#parse_file(file, \"CoiledCoil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the connection worked\n",
    "con = duckdb.connect(database=ProteinDB.db_string)           \n",
    "#id = 'A0A059X392'\n",
    "id = 'Q99L13' # this has multiple pfam entries\n",
    "result = con.execute(\"SELECT * FROM PROTEIN_SENTENCE WHERE UNIPROT_ID = ?\", [id]).fetchall()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
