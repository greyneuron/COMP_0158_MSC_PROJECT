{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "import csv\n",
    "import duckdb\n",
    "\n",
    "# internal representation\n",
    "from protein_metadata import ProteinSentence\n",
    "from protein_metadata import ProteinWord\n",
    "from protein_db import ProteinDB\n",
    "\n",
    "# file handles\n",
    "protein_fasta_file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_10M.fasta\";\n",
    "interpro_file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Examples\n",
    "\n",
    "<p>>Uniref100 Fasta\n",
    ">\\>UniRef100_Q197F3 Uncharacterized protein 007R n=1 Tax=Invertebrate iridescent virus 3 TaxID=345201 RepID=007R_IIV3\n",
    "MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKIVDDASTITITYHRV\n",
    "YFGISGPKPRQVADLGEYYDVNELLNYDTYTKTQEFAQKYNSLVKPTIDAKNWSGNELVL\n",
    "\n",
    "<p> Protein2ipr.dat\n",
    ">A0A000\tIPR004839\tAminotransferase, class I/classII\tPF00155\t41\t381<br>\n",
    "A0A000\tIPR010961\tTetrapyrrole biosynthesis, 5-aminolevulinic acid synthase\tTIGR01821\t12\t391<br>\n",
    "A0A000\tIPR015421\tPyridoxal phosphate-dependent transferase, major domain\tG3DSA:3.40.640.10\t48\t288<br>\n",
    "\n",
    "<p>> Explanation\n",
    "A0A0 Means its uniprot but not yet verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### grep protein id from interpro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\";\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam.dat\"; # protein2ipr with only pfam entries\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam_20M.dat\"; # protein2ipr with only pfam entries 0 20M lines only\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/A8KBH6_ipr_pfam.dat\"; # protein2ipr with only pfam entries - only for one protein\n",
    "\n",
    "#\n",
    "# greps for an id in interpro dat file\n",
    "# For A8KBH6, this will match A0A01A8KBH6 and A8KBH6\n",
    "# Time to parse the full protein2ipr.dat for A8KBH6 : 22min 56s\n",
    "#\n",
    "def grep_interpro(id):\n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            #match_string = \"ç\"            # works\n",
    "            #match_string = \"[A0A0-9]*A8KBH6\"   # works\n",
    "            #match_string = \"^[A0A0-9]*\"+id     # works\n",
    "            match_string = \"^[A0A0-9]*\"+id\n",
    "            match = re.search(match_string, line)\n",
    "            if match:\n",
    "                print('Matched:', id, 'in line:', line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DB Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProteinDB\n",
      "┌─────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n",
      "│   column_name   │ column_type │  null   │   key   │ default │  extra  │\n",
      "│     varchar     │   varchar   │ varchar │ varchar │ varchar │ varchar │\n",
      "├─────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ UNIPROT_ID      │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ DESCRIPTION     │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ NAME            │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ TAX_ID          │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ TAX_DESCRIPTION │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "└─────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘\n",
      "\n",
      "┌─────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n",
      "│ column_name │ column_type │  null   │   key   │ default │  extra  │\n",
      "│   varchar   │   varchar   │ varchar │ varchar │ varchar │ varchar │\n",
      "├─────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ UNIPROT_ID  │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ WORD_TYPE   │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ REF_ID      │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ REF_TXT     │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ START_POS   │ USMALLINT   │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ END_POS     │ USMALLINT   │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "└─────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = ProteinDB()\n",
    "db.create_tables()\n",
    "db.describe_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grep protein id from interpro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\";\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam2.dat\"; first pfam with text stripped out\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam.dat\"; #new pfam with full line\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/A8KBH6_ipr_pfam.dat\";\n",
    "\n",
    "#\n",
    "# greps for an id in interpro dat file\n",
    "# For A8KBH6, this will match A0A01A8KBH6 and A8KBH6\n",
    "# Time to parse the full protein2ipr.dat for A8KBH6 : 22min 56s\n",
    "#\n",
    "def grep_interpro(id):\n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            #match_string = \"ç\"            # works\n",
    "            #match_string = \"[A0A0-9]*A8KBH6\"   # works\n",
    "            #match_string = \"^[A0A0-9]*\"+id     # works\n",
    "            match_string = \"^[A0A0-9]*\"+id\n",
    "            match = re.search(match_string, line)\n",
    "            if match:\n",
    "                print('Matched:', id, 'in line:', line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: A8KBH6 in line: A0A1A8KBH6\tIPR001017\tDehydrogenase, E1 component\tPF00676\t108\t406\n",
      "Matched: A8KBH6 in line: A8KBH6\tIPR000008\tC2 domain\tPF00168\t169\t274\n",
      "Matched: A8KBH6 in line: A8KBH6\tIPR000719\tProtein kinase domain\tPF00069\t340\t583\n",
      "Matched: A8KBH6 in line: A8KBH6\tIPR002219\tProtein kinase C-like, phorbol ester/diacylglycerol-binding domain\tPF00130\t34\t84\n",
      "Matched: A8KBH6 in line: A8KBH6\tIPR002219\tProtein kinase C-like, phorbol ester/diacylglycerol-binding domain\tPF00130\t99\t150\n",
      "Matched: A8KBH6 in line: A8KBH6\tIPR017892\tProtein kinase, C-terminal\tPF00433\t624\t658\n",
      "Matched: A0A1A8KBH6 in line: A0A1A8KBH6\tIPR001017\tDehydrogenase, E1 component\tPF00676\t108\t406\n"
     ]
    }
   ],
   "source": [
    "grep_interpro('A8KBH6')\n",
    "grep_interpro('A0A1A8KBH6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep_interpro('A8KBH6')\n",
    "grep_interpro('A0A1A8KBH6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create extract file of PFAM entries from Interpro protein2ipr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10M lines 10000000 : 20s\n",
    "#MAX_LINES = 10000000\n",
    "\n",
    "limit       = True # if True, onLy parses Max_lines lines \n",
    "MAX_COUNT   = 25000000\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# 26 June 2024\n",
    "# parses protein2ipr.dat for entries with'PFNNN' and outpts those lines to a separate file\n",
    "# protein2ipr.dat       : 98.7GB,   1,355,591,115 entries\n",
    "# protein2ipr_new.dat   : 20.73GB,  298,766,058 entries\n",
    "# parsing time          : 23mins\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "input = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\"\n",
    "output = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam_25M.dat\"\n",
    "\n",
    "#\n",
    "# parse an Interpro file and grep out those with pfam domains\n",
    "# Parsing the full protein2ipr.dat file in python took: 23mins 40s\n",
    "#\n",
    "def create_pfam_interpro():\n",
    "    match_count  = 0\n",
    "    output_file = open(output, \"w\")\n",
    "\n",
    "    with open(input, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "\n",
    "            # just match for PF\n",
    "            match = re.search(\"PF[0-9]+\", line) \n",
    "            \n",
    "            if match:\n",
    "                match_count += 1\n",
    "                if match_count > MAX_COUNT:\n",
    "                    print(MAX_COUNT, 'limit reached, breaking.')\n",
    "                    break\n",
    "\n",
    "                output_file.write(line)\n",
    "    output_file.close()\n",
    "\n",
    "#create_pfam_interpro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse ProteinSentence items from PFAM entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10M lines 10000000 took 20s\n",
    "\n",
    "PROCESS_LIMIT   = 10000 # number of lines to process, set to -1 to ignore\n",
    "OUTPUT_LIMIT    = 1000  # determines how often to print a progress message\n",
    "\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr.dat\"; # full file\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_new.dat\"; # pfam only\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/protein2ipr_pfam_25M.dat\"; # pfam only\n",
    "#file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/A8KBH6_ipr_pfam.dat\"; # as above but only entries for A8KBH6 (pfam only)\n",
    "\n",
    "# holds a sentence for each sequence\n",
    "sentences = {}\n",
    "\n",
    "#\n",
    "# parse an Interpro file and create ProteinSentences and Words\n",
    "#\n",
    "def parse_interpro():\n",
    "    count = 0\n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            \n",
    "            # check if want to output progress\n",
    "            if ((line_number // OUTPUT_LIMIT > 0) and (line_number % OUTPUT_LIMIT) == 0):\n",
    "                count += 1\n",
    "                print(count * OUTPUT_LIMIT, 'lines processed.....')\n",
    "                 \n",
    "            # check if we've hit the line limit\n",
    "            if(PROCESS_LIMIT != -1):          \n",
    "                if line_number > PROCESS_LIMIT:\n",
    "                    print('Processing limit reached %s stopping' % (PROCESS_LIMIT))\n",
    "                    break\n",
    "        \n",
    "            # NB: Make sure the file is tab delimited\n",
    "\n",
    "            # Works for both in: A8KBH6_ipr_pfam.dat\n",
    "            match = re.search(\"([A0A0-9]*[a-zA-Z0-9]+)\\\\tIPR[0-9]+\\\\t.*\\\\t(PF[0-9]+)\\\\t([0-9]+)\\\\t([0-9]+)\", line)\n",
    "                      \n",
    "            if match is not None:\n",
    "                uniprot_id  = match.group(1)\n",
    "                pfam_word   = match.group(2)\n",
    "                start       = match.group(3)\n",
    "                end         = match.group(4)\n",
    "                \n",
    "                print(uniprot_id, '\\t', pfam_word, '\\t', start, '\\t', end)\n",
    "                \n",
    "                '''\n",
    "                # create a new word item\n",
    "                word = ProteinWord('pfam', pfam_word, start, end)\n",
    "                \n",
    "                # check if already have a protein with this id\n",
    "                if (id in sentences.keys()):\n",
    "                    sentences[id].add_word(word)\n",
    "                else:\n",
    "                    sentence = ProteinSentence(id, word)\n",
    "                    sentences[id] = sentence\n",
    "                '''\n",
    "                \n",
    "                # put into db....?\n",
    "                item_type=\"PFAM\"\n",
    "                con = duckdb.connect(database=':memory:')           \n",
    "                #duckdb.sql(\"INSERT INTO PROTEIN_WORD(UNIPROT_ID, WORD_TYPE, REF_ID, START, END)\\\n",
    "                #    VALUES(?,?,?,?,?)\", (uniprot_id, item_type, pfam_word, start, end))\n",
    "                \n",
    "                duckdb.execute(\"INSERT INTO PROTEIN_WORD (UNIPROT_ID, WORD_TYPE, REF_ID, START_POS, END_POS) VALUES(?,?,?,?,?)\", (uniprot_id, item_type, pfam_word, start, end))\n",
    "                con.close()\n",
    "                \n",
    "                \n",
    "parse_interpro()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidInputException",
     "evalue": "Invalid Input Error: Prepared parameters can only be passed as a list or a dictionary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidInputException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#duckdb.sql(\"INSERT INTO PROTEIN_WORD(UNIPROT_ID, WORD_TYPE, REF_ID, START, END)\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#    VALUES(?,?,?,?,?)\", (uniprot_id, item_type, pfam_word, start, end))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#result = duckdb.sql(\"SELECT * FROM PROTEIN_WORD\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA0A000\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mduckdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID = ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_ucl_base/lib/python3.12/site-packages/duckdb/__init__.py:225\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(query, parameters, multiple_parameter_sets, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     conn \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:default:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiple_parameter_sets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInvalidInputException\u001b[0m: Invalid Input Error: Prepared parameters can only be passed as a list or a dictionary"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=':memory:')           \n",
    "#duckdb.sql(\"INSERT INTO PROTEIN_WORD(UNIPROT_ID, WORD_TYPE, REF_ID, START, END)\\\n",
    "#    VALUES(?,?,?,?,?)\", (uniprot_id, item_type, pfam_word, start, end))\n",
    "\n",
    "#result = duckdb.sql(\"SELECT * FROM PROTEIN_WORD\")\n",
    "id = 'A0A000'\n",
    "result = duckdb.execute(\"SELECT * FROM PROTEIN_WORD WHERE UNIPROT_ID = ?\", (id))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences.keys()))\n",
    "#print(sentences.keys())\n",
    "#print(sentences.keys()[0:10])\n",
    "#print(sentences['A0A002'])\n",
    "#print(sentences['A8KBH6'])\n",
    "#print(sentences['A0A1A8KBH6'])\n",
    "print(sentences['A0A009GV07'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse UniRef100 FASTA 1 > Check for ProteinSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500k is enough for matches to be found\n",
    "MAX_LINES = 10000000\n",
    "matched = []\n",
    "not_matched = []\n",
    "\n",
    "#\n",
    "# parse a fasta file to get protein ids\n",
    "# for uniref, these ids are the characters after UniRef100_\n",
    "# TODO: Check if ids are proteins or protein clusters\n",
    "#\n",
    "def parse_fasta():\n",
    "    with open(protein_fasta_file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            \n",
    "            if line_number > MAX_LINES:  # line_number starts at 0.\n",
    "                break\n",
    "            #print('Processing :', line)\n",
    "            # note that the raw interpro file is tab delimited between fields\n",
    "            match = re.search(\"UniRef100_([A-Z0-9]+) \", line)\n",
    "            if match is not None:\n",
    "                id = match.group(1)\n",
    "\n",
    "                if id in sentences.keys():\n",
    "                    #print(line_number, 'Found sentence for protein :', id, sentences[id].text)\n",
    "                    if id not in matched:\n",
    "                        matched.append(id)\n",
    "                else:\n",
    "                    not_matched.append(id)\n",
    "parse_fasta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse UniRef100 FASTA 2 > Extract id, length, taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500k is enough for matches to be found\n",
    "MAX_LINES = 1000\n",
    "matched = []\n",
    "not_matched = []\n",
    "\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_500M.fasta\"\n",
    "\n",
    "#\n",
    "# parse a fasta file to get protein ids\n",
    "# for uniref, these ids are the characters after UniRef100_\n",
    "# TODO: Check if ids are proteins or protein clusters\n",
    "#\n",
    "def parse_fasta_2():\n",
    "    matchline = \"\"\n",
    "    id = \"\"\n",
    "    match_inprogress = False\n",
    "    \n",
    "    with open(file, 'r') as input_file:\n",
    "        for line_number, line in enumerate(input_file):\n",
    "            \n",
    "            if line_number > MAX_LINES:  # line_number starts at 0.\n",
    "                break\n",
    "\n",
    "            match = re.search(\">UniRef100_([A-Z0-9]+).*TaxID=([0-9]+).*\", line)\n",
    "            \n",
    "            #match = re.search(\">UniRef100_([A-Z0-9]+)\", line) # works\n",
    "            \n",
    "            if match is not None:\n",
    "                if(id != \"\"):\n",
    "                    print(id, '\\t', len(matchline), '\\t', tax_id, '\\t', matchline)\n",
    "                matchline = \"\"\n",
    "                id = match.group(1)\n",
    "                tax_id = match.group(2)\n",
    "                continue\n",
    "            else:\n",
    "                matchline += line.strip()\n",
    "parse_fasta_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(matched),'Matched proteins:\\n', matched)\n",
    "print(len(not_matched), 'Unmatched proteins:\\n',not_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse UniRef100 FASTA 3 > Modified parse_masked_regions from Daniel - outputs masked_regions.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import re\n",
    "\n",
    "# re_string = \"\\|(.+)\\|\" # original version\n",
    "re_string = \"UniRef100_([A-Z0-9]+)\" # modified for UniRef100\n",
    "\n",
    "input = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/uniref100_10M.fasta\"\n",
    "output = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/masked_regions.dat\"\n",
    "\n",
    "def parse_file(path, dom_type):\n",
    "    output_file = open(output, \"w\")\n",
    "    for record in SeqIO.parse(path, \"fasta\"):\n",
    "        # record.name = UniRef100_Q6GZX3\n",
    "        # record.description = UniRef100_Q6GZX3 Putative transc....\n",
    "        \n",
    "        # this removes the name from the description removes commas\n",
    "        raw_desc = record.description.replace(record.name+\" \", \"\")\n",
    "        raw_desc = raw_desc.replace(\",\", \"\")\n",
    "        \n",
    "        # extracts the id from the name\n",
    "        result = re.search(re_string, record.name)\n",
    "        uniprot_id = result.group(1)\n",
    "        \n",
    "        # loops throgh the sequence 3 dots at a time - not sure why\n",
    "        # is this supposed to return a line for each collection of at least 3 sequence characters?\n",
    "        # sequence characters?\n",
    "        #for m in re.finditer(r'\\.{3,}', str(record.seq)):          # original version\n",
    "        for m in re.finditer(r'.{3,}', str(record.seq)):            # modified for UniRef100\n",
    "            output_file.write(uniprot_id+\"\\tIPRXXXXXX\\t\"+raw_desc+\"\\t\"+dom_type+\"\\t\" +\n",
    "                    str(m.start()+1)+\"\\t\"+str(m.end()+1) + '\\n')\n",
    "        # return()\n",
    "    output_file.close()\n",
    "\n",
    "parse_file(input, \"LowComplexity\")\n",
    "#parse_file(file, \"CoiledCoil\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disordered Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TRIED .gz file directly but too complex\n",
    "# \n",
    "\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "input = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/disordered/extra.xml.gz\"\n",
    "MAX_LINES = 10\n",
    "\n",
    "def parse_disordered():\n",
    "    #pattern = \"dbname=\\\"MOBIDBLT\\\"\"\n",
    "    pattern = \".*\"\n",
    "    \n",
    "    regex = re.compile(pattern)\n",
    "    buffer_size = 3\n",
    "    count = 0\n",
    "\n",
    "    # Open the gzip file\n",
    "    with gzip.open(input, 'rt') as file:  # 'rt' mode opens the file in text mode\n",
    "        buffer = []\n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            protein_match = re.search(\"<protein id=\\\"([A-Z0-9]+).*>\", line)\n",
    "            if(protein_match is not None):\n",
    "                uniprot_id = protein_match.groups(1)\n",
    "                print('Protein :', uniprot_id, line.strip())\n",
    "            else:\n",
    "                db_match = re.search(\"dbname=\\\"MOBIDBLT\\\"([A-Z0-9]+).*>\", line)\n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            count +=1\n",
    "            if count >= MAX_LINES:\n",
    "                break\n",
    "           \n",
    "            buffer.append(line)\n",
    "            # Keep the buffer size within the specified limit\n",
    "            if len(buffer) > buffer_size:\n",
    "                buffer.pop(0)\n",
    "            \n",
    "            # Join the buffer lines into a single string for regex matching\n",
    "            combined_lines = ''.join(buffer)\n",
    "            \n",
    "            matches = re.search(\"(.*)\", line)\n",
    "            if matches:\n",
    "                print('Match :', matches.groups(1), line.strip())\n",
    "            '''\n",
    "parse_disordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This works directly on the uncompressed .gz file\n",
    "# No space on laptop for fully extracted extra.xml\n",
    "# Used this command to extract first 10000 lines into a separate file:\n",
    "#\n",
    "# zgrep . -m 10000 data/disordered/extra.xml.gz > data/disordered/extra.10000.xml\n",
    "#\n",
    "\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "file = '/Users/patrick/dev/ucl/comp0158_mscproject/data/disordered/extra.10000.xml'\n",
    "\n",
    "# get an iterable\n",
    "context = ElementTree.iterparse(file, events=(\"start\", \"end\"))\n",
    "\n",
    "# turn it into an iterator\n",
    "context = iter(context)\n",
    "\n",
    "# get the root element\n",
    "event, root = next(context)\n",
    "\n",
    "for event, protein in context:\n",
    "    if event == \"end\" and protein.tag == \"protein\":\n",
    "        # print(elem.attrib['id'])\n",
    "        for match in protein:\n",
    "            if 'MOBIDBLT' in match.attrib['dbname']:\n",
    "                for coords in match:\n",
    "                    print(protein.attrib['id']+\"\\tIPRXXXXXX\\t\" +\n",
    "                          match.attrib['name']+\"\\t\"+match.attrib['id']+\"\\t\" +\n",
    "                          coords.attrib['start']+\"\\t\"+coords.attrib['end'])\n",
    "        # exit()\n",
    "        root.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cats_file) \u001b[38;5;28;01mas\u001b[39;00m cf:\n\u001b[1;32m     33\u001b[0m     catreader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(cf, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcatreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# check if item at pos 1 in current row is in namelookup\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname_lookup\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkingdom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_ucl_base/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_ucl_base/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_ucl_base/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_ucl_base/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "# parse protein2ipr to output just the pfam domains\n",
    "\n",
    "#\n",
    "# get these by extracting taxdump.tar.gz and taxcat.tar.gz\n",
    "#\n",
    "names_file = '/Users/patrick/dev/ucl/comp0158_mscproject/data/taxonomy/names.dmp'\n",
    "cats_file = '/Users/patrick/dev/ucl/comp0158_mscproject/data/taxonomy/categories.dmp'\n",
    "\n",
    "name_lookup = {}\n",
    "\n",
    "# print(\"Reading TAXA names\")\n",
    "with open(names_file) as nf:\n",
    "    namesreader = csv.reader(nf, delimiter='|', quotechar='\\'')\n",
    "    # each row has 4 cols, but col names have \\t - need to remove via rstrip and lstrip\n",
    "    # cleanrow becomes, fir example : [ 1, root, scientific name, ''] \n",
    "    # cleanrow[0] refers to the entry on the first column of the current row\n",
    "    for row in namesreader:\n",
    "        clean_row = [x.rstrip().lstrip() for x in row]\n",
    "        if 'scientific name' in clean_row[3]:\n",
    "            # print(clean_row)\n",
    "            clean_row[1] = clean_row[1].replace(',', '')\n",
    "            \n",
    "            name_lookup[clean_row[0]] = {}\n",
    "            name_lookup[clean_row[0]]['name'] = clean_row[1]\n",
    "            name_lookup[clean_row[0]]['kingdom'] = 'unknown'\n",
    "            \n",
    "            \n",
    "# print(\"Reading TAXA categories\")\n",
    "with open(cats_file) as cf:\n",
    "    catreader = csv.reader(cf, delimiter='\\t', quotechar='\\'')\n",
    "    for row in catreader:\n",
    "        # check if item at pos 1 in current row is in namelookup\n",
    "        if row[1] in name_lookup:\n",
    "            name_lookup[row[1]]['kingdom'] = row[0]\n",
    "        if row[2] in name_lookup:\n",
    "            name_lookup[row[1]]['kingdom'] = row[0]\n",
    "            \n",
    "            \n",
    "'''\n",
    "\n",
    "# print(name_lookup)\n",
    "uniprot_lookup = {}\n",
    "# print(\"Annotating UNIPROT\")\n",
    "with open('/scratch1/NOT_BACKED_UP/dbuchan/uniprot/idmapping_selected.tab') as mapping:\n",
    "    mappingreader = csv.reader(mapping, delimiter='\\t', quotechar='\\'')\n",
    "    for row in mappingreader:\n",
    "        if row[12] in name_lookup:\n",
    "            # print(row)\n",
    "            uniprot_lookup[row[0]] = name_lookup[row[12]]\n",
    "            uniprot_lookup[row[0]]['taxaid'] = row[12]\n",
    "            # break\n",
    "\n",
    "# print(\"Annotating UNIPROT PFam\")\n",
    "\n",
    "# print(uniprot_lookup)\n",
    "with open('/scratch1/NOT_BACKED_UP/dbuchan/interpro/derived/protein2ipr_pfam.dat') as pfam:\n",
    "# with open('/scratch1/NOT_BACKED_UP/dbuchan/interpro/masked_regions.dat') as pfam:\n",
    "#with open('/scratch1/NOT_BACKED_UP/dbuchan/interpro/disorder_regions.dat') as pfam:\n",
    "    pfamreader = csv.reader(pfam, delimiter=',', quotechar='\\'')\n",
    "    for row in pfamreader:\n",
    "        if row[0] in uniprot_lookup:\n",
    "            new_line = [row[0], uniprot_lookup[row[0]]['taxaid'],\n",
    "                        uniprot_lookup[row[0]]['kingdom'],\n",
    "                        uniprot_lookup[row[0]]['name']] + row[1:]\n",
    "            print(\",\".join(new_line))\n",
    "            sys.stdout.flush()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
