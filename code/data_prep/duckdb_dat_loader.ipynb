{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Tested again on July 31st and August 1st\n",
    "\n",
    "\n",
    "This notebook contains various scripts to load data into tables on a local DuckDB database. <br>\n",
    "- proteins are loaded into W2V_PROTEIN\n",
    "- pfam entries are loaded into W2V_TOKEN\n",
    "- disorder regions are also loaded into W2V_TOKEN\n",
    "\n",
    "The tables are created at the time the data is loaded - so see the appropariate cells for the table definition.\n",
    "\n",
    "Indexes are applied after the data is loaded.\n",
    "\n",
    "\n",
    "DuckDB is very easy to install on a mac and can load tab-delimited files extremely quickly.\n",
    "To recreate this environment, you just need to install DuckDB and then set the db_string at the top of this file\n",
    "to the location where you wish the database file to be stored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "#\n",
    "# TODO - SET THIS STRING TO WHERE YOU WANT THE DB TO STORE ITS DATA\n",
    "#\n",
    "db_string = \"/Users/patrick/dev/ucl/comp0158_mscproject/database/w2v_20240731_test.db\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the DB works OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A TABLE\n",
    "#con = duckdb.connect(database=':memory:')\n",
    "con = duckdb.connect(database=db_string)  \n",
    "duckdb.sql(\"\\\n",
    "    CREATE TABLE TEST (\\\n",
    "        ID VARCHAR,\\\n",
    "    )\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n",
      "│ column_name │ column_type │  null   │   key   │ default │  extra  │\n",
      "│   varchar   │   varchar   │ varchar │ varchar │ varchar │ varchar │\n",
      "├─────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ ID          │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "└─────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DESCRIBE\n",
    "con = duckdb.connect(database=db_string)\n",
    "res = duckdb.sql(\"DESCRIBE TEST\")\n",
    "print(res)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP\n",
    "con = duckdb.connect(database=db_string)  \n",
    "duckdb.sql(\"DROP TABLE TEST\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PROTEIN TrEMBL : INTO W2V_PROTEIN\n",
    "\n",
    "I initialy used TrEMBL to create the corpus as the UniRef100 extract was too large and kept breaking my Macbook!\n",
    "I subsequently found that it's possible to download only the eukaryortic UniRef100 proteins from the uniprot website. For this, it's necessary to filter on tax id 2759 and also select 100% completion(?). Note that it takes between 12 and 16 hours for Uniprot to prepare the extract for download, but it's worth it as it contains the taxonomy details as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load protein file into protein table\n",
    "# 20 July 2024 - This took 12.9s to load uniprotkb-2759_78494531.dat (78M proteins)\n",
    "# 31 July testing again to check code works\n",
    "con = duckdb.connect(database=db_string)           \n",
    "con.execute(\"CREATE TABLE W2V_PROTEIN AS SELECT * FROM read_csv_auto('/Volumes/My Passport/data/protein/dat/uniprotkb-2759_78494531.dat', columns={'uniprot_id' :'VARCHAR', 'start': 'USMALLINT', 'end': 'USMALLINT'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(78494529,)]\n"
     ]
    }
   ],
   "source": [
    "# This should output that there are 78,494,529 items\n",
    "con = duckdb.connect(database=db_string)           \n",
    "protein_count = con.execute(\"SELECT COUNT(*) FROM W2V_PROTEIN\").fetchall()\n",
    "print(protein_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=db_string)           \n",
    "#con.execute(\"DROP TABLE W2V_PROTEIN\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created\n"
     ]
    }
   ],
   "source": [
    "# create an index (after loading the data)\n",
    "con = duckdb.connect(database=db_string)   \n",
    "con.execute(\"CREATE INDEX UNIP_IDX ON W2V_PROTEIN(UNIPROT_ID)\")\n",
    "print('index created')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A0A010PZU8', 1, 1389)]\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=db_string)      \n",
    "\n",
    "# SELECT FROM LIST OF IDS - REALLY SLOW\n",
    "#list = ['A0A010R6E0', 'A0A010RP22']\n",
    "#entries = con.execute(\"SELECT * FROM PFAM_TOKEN WHERE column0 IN (SELECT UNNEST(?))\", [list]).fetchall()\n",
    "\n",
    "res = con.execute(\"SELECT * FROM W2V_PROTEIN WHERE UNIPROT_ID = (?)\", ['A0A010PZU8']).fetchall()\n",
    "\n",
    "print(res)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PROTEIN - UNIREF :\n",
    "\n",
    "UniRef100 - All Eukaryotic Proteins - including Taxonomy Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load protein file into protein table\n",
    "# 05 Aug 2024 - Took 1min 4.5s to load 95,272,305 items\n",
    "#\n",
    "# [counter, uniprot_id, len, start, end, n_members, tax_id, tax_name]\n",
    "#\n",
    "con = duckdb.connect(database=db_string)           \n",
    "con.execute(\"CREATE TABLE W2V_PROTEIN_UREF100_E AS SELECT * FROM read_csv_auto('/Users/patrick/dev/ucl/comp0158_mscproject/data/protein/uniref100only_2759-95272305_20240805.dat', columns={'counter' : UINTEGER, 'uniprot_id' :VARCHAR, 'length': USMALLINT, 'start': USMALLINT, 'end': USMALLINT, 'n_members': USMALLINT, 'tax_id' :UINTEGER, 'tax_name' : VARCHAR})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This should output that there are 95,272,305 items\n",
    "con = duckdb.connect(database=db_string)           \n",
    "#res = con.execute(\"DROP TABLE W2V_PROTEIN_UREF100_E\").fetchall()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95272305,)\n"
     ]
    }
   ],
   "source": [
    "# This should output that there are 95,272,305 items\n",
    "con = duckdb.connect(database=db_string)           \n",
    "count = con.execute(\"SELECT COUNT(*) FROM W2V_PROTEIN_UREF100_E\").fetchall()\n",
    "print(count[0])\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index (after loading the data) - initially ran out of memory on Macbook with all proteins\n",
    "# Took 25s with eukaryotic only\n",
    "#\n",
    "# Going to create 2 indices as have added a counter column and want an index\n",
    "#\n",
    "con = duckdb.connect(database=db_string)   \n",
    "con.execute(\"CREATE INDEX UNIREF100_IDX ON W2V_PROTEIN_UREF100_E(UNIPROT_ID)\")\n",
    "con.execute(\"CREATE INDEX COUNTER_IDX ON W2V_PROTEIN_UREF100_E(COUNTER)\")\n",
    "print('indices created')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000000, 'A0A6M2CIG0', 372, 1, 373, 2, 6941, 'Rhipicephalus microplus')\n",
      "(30000001, 'A0A6M2CIG2', 436, 1, 437, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000002, 'A0A6M2CIG3', 503, 1, 504, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000003, 'A0A6M2CIG5', 512, 1, 513, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000004, 'A0A6M2CIG7', 519, 1, 520, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000005, 'A0A6M2CIG9', 84, 1, 85, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000006, 'A0A6M2CIH1', 518, 1, 519, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000007, 'A0A6M2CIH2', 215, 1, 216, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000008, 'A0A6M2CIH3', 546, 1, 547, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000009, 'A0A6M2CIH5', 204, 1, 205, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000010, 'A0A6M2CIH6', 226, 1, 227, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000011, 'A0A6M2CIH8', 96, 1, 97, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000012, 'A0A6M2CII0', 561, 1, 562, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000013, 'A0A6M2CII1', 269, 1, 270, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000014, 'A0A6M2CII2', 372, 1, 373, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000015, 'A0A6M2CII3', 319, 1, 320, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000016, 'A0A6M2CII4', 540, 1, 541, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000017, 'A0A6M2CII5', 480, 1, 481, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000018, 'A0A6M2CII6', 279, 1, 280, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000019, 'A0A6M2CII7', 302, 1, 303, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000020, 'A0A6M2CII8', 159, 1, 160, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000021, 'A0A6M2CII9', 331, 1, 332, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000022, 'A0A6M2CIJ1', 142, 1, 143, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000023, 'A0A6M2CIJ2', 163, 1, 164, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000024, 'A0A6M2CIJ3', 360, 1, 361, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000025, 'A0A6M2CIJ4', 170, 1, 171, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000026, 'A0A6M2CIJ5', 514, 1, 515, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000027, 'A0A6M2CIJ6', 505, 1, 506, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000028, 'A0A6M2CIJ8', 246, 1, 247, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000029, 'A0A6M2CIJ9', 205, 1, 206, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000030, 'A0A6M2CIK0', 344, 1, 345, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000031, 'A0A6M2CIK1', 227, 1, 228, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000032, 'A0A6M2CIK3', 1456, 1, 1457, 2, 6941, 'Rhipicephalus microplus')\n",
      "(30000033, 'A0A6M2CIK4', 247, 1, 248, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000034, 'A0A6M2CIK5', 419, 1, 420, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000035, 'A0A6M2CIK6', 181, 1, 182, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000036, 'A0A6M2CIK7', 226, 1, 227, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000037, 'A0A6M2CIK8', 367, 1, 368, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000038, 'A0A6M2CIK9', 75, 1, 76, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000039, 'A0A6M2CIL0', 223, 1, 224, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000040, 'A0A6M2CIL1', 319, 1, 320, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000041, 'A0A6M2CIL2', 399, 1, 400, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000042, 'A0A6M2CIL3', 198, 1, 199, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000043, 'A0A6M2CIL4', 79, 1, 80, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000044, 'A0A6M2CIL5', 305, 1, 306, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000045, 'A0A6M2CIL6', 161, 1, 162, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000046, 'A0A6M2CIL7', 125, 1, 126, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000047, 'A0A6M2CIL8', 361, 1, 362, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000048, 'A0A6M2CIL9', 151, 1, 152, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000049, 'A0A6M2CIM0', 142, 1, 143, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000050, 'A0A6M2CIM1', 482, 1, 483, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000051, 'A0A6M2CIM2', 439, 1, 440, 2, 6941, 'Rhipicephalus microplus')\n",
      "(30000052, 'A0A6M2CIM3', 290, 1, 291, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000053, 'A0A6M2CIM4', 187, 1, 188, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000054, 'A0A6M2CIM7', 107, 1, 108, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000055, 'A0A6M2CIM8', 596, 1, 597, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000056, 'A0A6M2CIM9', 95, 1, 96, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000057, 'A0A6M2CIN0', 373, 1, 374, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000058, 'A0A6M2CIN1', 224, 1, 225, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000059, 'A0A6M2CIN2', 413, 1, 414, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000060, 'A0A6M2CIN3', 87, 1, 88, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000061, 'A0A6M2CIN4', 275, 1, 276, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000062, 'A0A6M2CIN5', 292, 1, 293, 2, 6941, 'Rhipicephalus microplus')\n",
      "(30000063, 'A0A6M2CIN6', 484, 1, 485, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000064, 'A0A6M2CIN8', 374, 1, 375, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000065, 'A0A6M2CIN9', 527, 1, 528, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000066, 'A0A6M2CIP0', 601, 1, 602, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000067, 'A0A6M2CIP1', 218, 1, 219, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000068, 'A0A6M2CIP2', 248, 1, 249, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000069, 'A0A6M2CIP3', 168, 1, 169, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000070, 'A0A6M2CIP4', 375, 1, 376, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000071, 'A0A6M2CIP5', 420, 1, 421, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000072, 'A0A6M2CIP6', 519, 1, 520, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000073, 'A0A6M2CIP7', 277, 1, 278, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000074, 'A0A6M2CIP9', 164, 1, 165, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000075, 'A0A6M2CIQ0', 325, 1, 326, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000076, 'A0A6M2CIQ1', 98, 1, 99, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000077, 'A0A6M2CIQ3', 189, 1, 190, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000078, 'A0A6M2CIQ4', 189, 1, 190, 2, 6941, 'Rhipicephalus microplus')\n",
      "(30000079, 'A0A6M2CIQ5', 420, 1, 421, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000080, 'A0A6M2CIQ6', 139, 1, 140, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000081, 'A0A6M2CIQ7', 525, 1, 526, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000082, 'A0A6M2CIQ8', 215, 1, 216, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000083, 'A0A6M2CIQ9', 149, 1, 150, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000084, 'A0A6M2CIR0', 390, 1, 391, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000085, 'A0A6M2CIR1', 93, 1, 94, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000086, 'A0A6M2CIR2', 270, 1, 271, 3, 6941, 'Rhipicephalus microplus')\n",
      "(30000087, 'A0A6M2CIR3', 250, 1, 251, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000088, 'A0A6M2CIR4', 119, 1, 120, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000089, 'A0A6M2CIR5', 376, 1, 377, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000090, 'A0A6M2CIR6', 258, 1, 259, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000091, 'A0A6M2CIR7', 711, 1, 712, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000092, 'A0A6M2CIR8', 717, 1, 718, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000093, 'A0A6M2CIR9', 218, 1, 219, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000094, 'A0A6M2CIS0', 555, 1, 556, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000095, 'A0A6M2CIS1', 517, 1, 518, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000096, 'A0A6M2CIS2', 258, 1, 259, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000097, 'A0A6M2CIS3', 579, 1, 580, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000098, 'A0A6M2CIS4', 246, 1, 247, 1, 6941, 'Rhipicephalus microplus')\n",
      "(30000099, 'A0A6M2CIS5', 229, 1, 230, 1, 6941, 'Rhipicephalus microplus')\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=db_string)   \n",
    "results = con.execute(\"SELECT * FROM W2V_PROTEIN_UREF100_E WHERE COUNTER >= 30000000 AND COUNTER <30000100 \").fetchall()\n",
    "for res in results:\n",
    "    print (res)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PFAM TOKENS INTO W2V_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# July 20 2024 - Took 1m 55s to load 296,017,815 entries from a directory on a macbook\n",
    "# July 31 2024 - Restest took 3m 10s from an external drive attached to macbook\n",
    "con = duckdb.connect(database=db_string)\n",
    "\n",
    "con.execute(\"CREATE TABLE W2V_TOKEN AS SELECT * FROM read_csv_auto('/Volumes/My Passport/data/pfam/protein2ipr_pfam_20240715.dat', columns={'uniprot_id' :'VARCHAR', 'type' : 'VARCHAR', 'token' : 'VARCHAR', 'start': 'USMALLINT', 'end': 'USMALLINT'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(377274915,)]\n"
     ]
    }
   ],
   "source": [
    "# with pfam only this shows 296,017,815 entries\n",
    "# after loading disorder as well this shows 377,274,915 (81,257,100 disorder entries)\n",
    "con = duckdb.connect(database=db_string)           \n",
    "protein_count = con.execute(\"SELECT COUNT(*) FROM W2V_TOKEN\").fetchall()\n",
    "print(protein_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index (after loading data)\n",
    "con = duckdb.connect(database=db_string)  \n",
    "res = con.execute(\"CREATE INDEX PF_TKN_IDX ON W2V_TOKEN(UNIPROT_ID)\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 614 entries for PF20200\n",
      "Found 607 unique proteins containing PF20200\n",
      "Found 0 eukaryotic proteins with pfam entry PF20200\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "con = duckdb.connect(database=db_string)\n",
    "#token = 'PF19782' # Has 0 eukaryotic proteins\n",
    "#token = 'PF20176' # Has 0 eukaryotic proteins\n",
    "token = 'PF20200' # Has 0 eukaryotic proteins\n",
    "#token = 'PF14033' # Has about 4852 eukaryotic proteins\n",
    "\n",
    "# get number of times this token is in W2V_TOKEN\n",
    "token_count = con.execute(\"SELECT COUNT(*) FROM W2V_TOKEN WHERE TOKEN=(?)\", [token]).fetchall()\n",
    "print(f\"found {token_count[0][0]} entries for {token}\")\n",
    "\n",
    "# get the protein id for each token\n",
    "results = con.execute(\"SELECT DISTINCT UNIPROT_ID FROM W2V_TOKEN WHERE TOKEN=(?)\", [token]).fetchall()\n",
    "\n",
    "print(f\"Found {len(results)} unique proteins containing {token}\")\n",
    "\n",
    "\n",
    "# need to check if that protein is actually eukaryotic\n",
    "eukaryotic_count = 0\n",
    "for protein_res in results:\n",
    "    #print(res)\n",
    "    protein_id = protein_res[0]\n",
    "    \n",
    "    # check for protein_id\n",
    "    protein_count = con.execute(\"SELECT COUNT(*) FROM W2V_PROTEIN WHERE UNIPROT_ID=(?)\", [protein_id]).fetchall()\n",
    "    \n",
    "    count = protein_count[0][0]\n",
    "    \n",
    "    if(count >0):\n",
    "        eukaryotic_count +=1\n",
    "        \n",
    "print(f\"Found {eukaryotic_count} eukaryotic proteins with pfam entry {token}\") \n",
    "    \n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### LOAD DISORDER ITEMS INTO W2V_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load disorder entries\n",
    "# First run : July 19\n",
    "# Retest    : August 1st (on Macbook - took 2mn 25s)\n",
    "con = duckdb.connect(database=db_string) \n",
    "con.execute(\"INSERT INTO W2V_TOKEN SELECT * FROM read_csv_auto('/Volumes/My Passport/data/disorder/dat/disordered_tokens_20240719.dat')\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(377274915,)]\n"
     ]
    }
   ],
   "source": [
    "# with pfam only this shows 296,017,815 entries\n",
    "# after loading disorder as well this shows 377,274,915\n",
    "con = duckdb.connect(database=db_string)           \n",
    "token_count = con.execute(\"SELECT COUNT(*) FROM W2V_TOKEN\").fetchall()\n",
    "print(token_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A0A010PZU8', 'PFAM', 'PF00400', 865, 900), ('A0A010PZU8', 'PFAM', 'PF00400', 928, 955), ('A0A010PZU8', 'PFAM', 'PF00400', 960, 998), ('A0A010PZU8', 'PFAM', 'PF00400', 1017, 1040), ('A0A010PZU8', 'PFAM', 'PF00400', 1078, 1108), ('A0A010PZU8', 'PFAM', 'PF00400', 1233, 1260), ('A0A010PZU8', 'PFAM', 'PF05729', 358, 479), ('A0A010PZU8', 'PFAM', 'PF17100', 152, 254), ('A0A010PZU8', 'DISORDER', 'Consensus Disorder Prediction', 1, 30)]\n"
     ]
    }
   ],
   "source": [
    "# test that W2V_TOKEN has all pfam and disorder entries\n",
    "con = duckdb.connect(database=db_string)           \n",
    "tokens = con.execute(\"SELECT * FROM W2V_TOKEN WHERE UNIPROT_ID=(?)\", ['A0A010PZU8']).fetchall()\n",
    "print(tokens)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TAXONOMY INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the data-preparation folder for a shell script that produces the .dat file loaded here\n",
    "con = duckdb.connect(database=db_string)\n",
    "con.execute(\"CREATE TABLE W2V_TAX_NAME AS SELECT * FROM read_csv_auto('/Volumes/My Passport/data/taxonomy/dat/scientific_names_20240802.dat', columns={'tax_id' :'VARCHAR', 'name' : 'VARCHAR'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2588170,)]\n"
     ]
    }
   ],
   "source": [
    "# count  - should have 2,588,170 entries\n",
    "con = duckdb.connect(database=db_string)           \n",
    "token_count = con.execute(\"SELECT COUNT(*) FROM W2V_TAX_NAME\").fetchall()\n",
    "print(token_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index (after loading data)\n",
    "con = duckdb.connect(database=db_string)  \n",
    "res = con.execute(\"CREATE INDEX TAX_NM_IDX ON W2V_TAX_NAME(TAX_ID)\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the data-preparation folder for a shell script that produces the .dat file loaded here\n",
    "con = duckdb.connect(database=db_string)\n",
    "con.execute(\"CREATE TABLE W2V_TAX_CAT AS SELECT * FROM read_csv_auto('/Volumes/My Passport/data/taxonomy/dat/categories_20240802.dat', columns={'type' : 'VARCHAR', 'parent_id' :'VARCHAR', 'id' : 'VARCHAR'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1567316,)]\n"
     ]
    }
   ],
   "source": [
    "# count  - should have 1,567,316 entries\n",
    "con = duckdb.connect(database=db_string)           \n",
    "token_count = con.execute(\"SELECT COUNT(*) FROM W2V_TAX_CAT\").fetchall()\n",
    "print(token_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index (after loading data)\n",
    "con = duckdb.connect(database=db_string)  \n",
    "res = con.execute(\"CREATE INDEX TAX_CT_IDX ON W2V_TAX_CAT(ID)\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1310608', 'Acinetobacter sp. 1295259')]\n"
     ]
    }
   ],
   "source": [
    "# test that W2V_TOKEN has all pfam and disorder entries\n",
    "# 1445577   : Colletotrichum fioriniae PJ7\n",
    "# 10116     : Rattus norvegicus\n",
    "con = duckdb.connect(database=db_string)           \n",
    "tokens = con.execute(\"SELECT * FROM W2V_TAX_NAME WHERE TAX_ID=(?)\", ['1310608']).fetchall()\n",
    "print(tokens)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODING AND ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIND UNIQUE PFAM FOR EUKARYOTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 required.\n",
      "iteration 0 from 0 to 500000. query took 5.040165185928345s, overall took 5.111280202865601s\n",
      "iteration 1 from 500000 to 1000000. query took 5.5530312061309814s, overall took 5.624272108078003s\n",
      "iteration 2 from 1000000 to 1500000. query took 5.292984962463379s, overall took 5.377043962478638s\n",
      "iteration 3 from 1500000 to 2000000. query took 5.45654821395874s, overall took 5.533218145370483s\n",
      "iteration 4 from 2000000 to 2500000. query took 5.277507781982422s, overall took 5.312421798706055s\n",
      "iteration 5 from 2500000 to 3000000. query took 5.170738220214844s, overall took 5.219146013259888s\n",
      "iteration 6 from 3000000 to 3500000. query took 5.81486177444458s, overall took 5.8929338455200195s\n",
      "iteration 7 from 3500000 to 4000000. query took 5.861753940582275s, overall took 5.918999910354614s\n",
      "iteration 8 from 4000000 to 4500000. query took 5.312227725982666s, overall took 5.3758320808410645s\n",
      "iteration 9 from 4500000 to 5000000. query took 5.828030109405518s, overall took 5.8947670459747314s\n",
      "iteration 10 from 5000000 to 5500000. query took 6.114027976989746s, overall took 6.191333055496216s\n",
      "iteration 11 from 5500000 to 6000000. query took 5.648717641830444s, overall took 5.7217066287994385s\n",
      "iteration 12 from 6000000 to 6500000. query took 6.665011882781982s, overall took 6.7480528354644775s\n",
      "iteration 13 from 6500000 to 7000000. query took 5.7649171352386475s, overall took 5.826034069061279s\n",
      "iteration 14 from 7000000 to 7500000. query took 5.457338094711304s, overall took 5.520350217819214s\n",
      "iteration 15 from 7500000 to 8000000. query took 5.657837152481079s, overall took 5.744754314422607s\n",
      "iteration 16 from 8000000 to 8500000. query took 5.81337308883667s, overall took 5.882138967514038s\n",
      "iteration 17 from 8500000 to 9000000. query took 5.609989881515503s, overall took 5.68217396736145s\n",
      "iteration 18 from 9000000 to 9500000. query took 5.504445791244507s, overall took 5.5734031200408936s\n",
      "iteration 19 from 9500000 to 10000000. query took 5.388667345046997s, overall took 5.458146095275879s\n",
      "iteration 20 from 10000000 to 10500000. query took 5.812897205352783s, overall took 5.915337324142456s\n",
      "iteration 21 from 10500000 to 11000000. query took 5.9460999965667725s, overall took 6.032092809677124s\n",
      "iteration 22 from 11000000 to 11500000. query took 5.98786187171936s, overall took 6.056574821472168s\n",
      "iteration 23 from 11500000 to 12000000. query took 5.5468690395355225s, overall took 5.622529029846191s\n",
      "iteration 24 from 12000000 to 12500000. query took 5.397963047027588s, overall took 5.474071979522705s\n",
      "iteration 25 from 12500000 to 13000000. query took 5.300381898880005s, overall took 5.3629820346832275s\n",
      "iteration 26 from 13000000 to 13500000. query took 5.601617813110352s, overall took 5.6914238929748535s\n",
      "iteration 27 from 13500000 to 14000000. query took 5.606385231018066s, overall took 5.719731092453003s\n",
      "iteration 28 from 14000000 to 14500000. query took 5.6758809089660645s, overall took 5.730866193771362s\n",
      "iteration 29 from 14500000 to 15000000. query took 5.1743810176849365s, overall took 5.245467901229858s\n",
      "iteration 30 from 15000000 to 15500000. query took 5.325756311416626s, overall took 5.396174192428589s\n",
      "iteration 31 from 15500000 to 16000000. query took 5.649058103561401s, overall took 5.733150005340576s\n",
      "iteration 32 from 16000000 to 16500000. query took 5.510801792144775s, overall took 5.585223913192749s\n",
      "iteration 33 from 16500000 to 17000000. query took 5.262126684188843s, overall took 5.317554950714111s\n",
      "iteration 34 from 17000000 to 17500000. query took 5.318727970123291s, overall took 5.42971396446228s\n",
      "iteration 35 from 17500000 to 18000000. query took 5.850088357925415s, overall took 5.9273552894592285s\n",
      "iteration 36 from 18000000 to 18500000. query took 5.415824890136719s, overall took 5.4706871509552s\n",
      "iteration 37 from 18500000 to 19000000. query took 5.584745168685913s, overall took 5.744458198547363s\n",
      "iteration 38 from 19000000 to 19500000. query took 6.123266935348511s, overall took 6.228822231292725s\n",
      "iteration 39 from 19500000 to 20000000. query took 5.521755218505859s, overall took 5.58414626121521s\n",
      "iteration 40 from 20000000 to 20500000. query took 5.406562089920044s, overall took 5.473851203918457s\n",
      "iteration 41 from 20500000 to 21000000. query took 5.472402811050415s, overall took 5.569985866546631s\n",
      "iteration 42 from 21000000 to 21500000. query took 5.313629865646362s, overall took 5.374581575393677s\n",
      "iteration 43 from 21500000 to 22000000. query took 5.301592111587524s, overall took 5.365411043167114s\n",
      "iteration 44 from 22000000 to 22500000. query took 5.696295976638794s, overall took 5.782448768615723s\n",
      "iteration 45 from 22500000 to 23000000. query took 5.332273960113525s, overall took 5.419224262237549s\n",
      "iteration 46 from 23000000 to 23500000. query took 5.553530216217041s, overall took 5.614794015884399s\n",
      "iteration 47 from 23500000 to 24000000. query took 5.239769220352173s, overall took 5.30860710144043s\n",
      "iteration 48 from 24000000 to 24500000. query took 5.263850212097168s, overall took 5.329637050628662s\n",
      "iteration 49 from 24500000 to 25000000. query took 5.790328741073608s, overall took 5.871736764907837s\n",
      "iteration 50 from 25000000 to 25500000. query took 5.598312854766846s, overall took 5.671000003814697s\n",
      "iteration 51 from 25500000 to 26000000. query took 5.785121917724609s, overall took 5.872541189193726s\n",
      "iteration 52 from 26000000 to 26500000. query took 6.101886987686157s, overall took 6.260528802871704s\n",
      "iteration 53 from 26500000 to 27000000. query took 6.110169887542725s, overall took 6.264711856842041s\n",
      "iteration 54 from 27000000 to 27500000. query took 8.331618070602417s, overall took 8.379316091537476s\n",
      "iteration 55 from 27500000 to 28000000. query took 5.634086847305298s, overall took 5.698912858963013s\n",
      "iteration 56 from 28000000 to 28500000. query took 5.492632627487183s, overall took 5.566701650619507s\n",
      "iteration 57 from 28500000 to 29000000. query took 5.934625864028931s, overall took 6.017469882965088s\n",
      "iteration 58 from 29000000 to 29500000. query took 5.65509295463562s, overall took 5.774521112442017s\n",
      "iteration 59 from 29500000 to 30000000. query took 5.890164852142334s, overall took 6.003816843032837s\n",
      "iteration 60 from 30000000 to 30500000. query took 5.70838189125061s, overall took 5.817672967910767s\n",
      "iteration 61 from 30500000 to 31000000. query took 5.874181270599365s, overall took 6.016273260116577s\n",
      "iteration 62 from 31000000 to 31500000. query took 5.703934907913208s, overall took 5.787015914916992s\n",
      "iteration 63 from 31500000 to 32000000. query took 5.8905720710754395s, overall took 5.971188068389893s\n",
      "iteration 64 from 32000000 to 32500000. query took 5.526849985122681s, overall took 5.609086990356445s\n",
      "iteration 65 from 32500000 to 33000000. query took 6.796650171279907s, overall took 6.890078067779541s\n",
      "iteration 66 from 33000000 to 33500000. query took 5.632051944732666s, overall took 5.749861001968384s\n",
      "iteration 67 from 33500000 to 34000000. query took 5.774212837219238s, overall took 5.888742923736572s\n",
      "iteration 68 from 34000000 to 34500000. query took 5.817417144775391s, overall took 5.932908058166504s\n",
      "iteration 69 from 34500000 to 35000000. query took 6.156193017959595s, overall took 6.261896848678589s\n",
      "iteration 70 from 35000000 to 35500000. query took 5.730557918548584s, overall took 5.839894771575928s\n",
      "iteration 71 from 35500000 to 36000000. query took 6.361891746520996s, overall took 6.457456827163696s\n",
      "iteration 72 from 36000000 to 36500000. query took 5.679829120635986s, overall took 5.717336177825928s\n",
      "iteration 73 from 36500000 to 37000000. query took 5.014204740524292s, overall took 5.04816198348999s\n",
      "iteration 74 from 37000000 to 37500000. query took 5.152298927307129s, overall took 5.20105504989624s\n",
      "iteration 75 from 37500000 to 38000000. query took 6.03942084312439s, overall took 6.108107805252075s\n",
      "iteration 76 from 38000000 to 38500000. query took 5.243863105773926s, overall took 5.311841011047363s\n",
      "iteration 77 from 38500000 to 39000000. query took 5.3616039752960205s, overall took 5.4288010597229s\n",
      "iteration 78 from 39000000 to 39500000. query took 5.994816780090332s, overall took 6.07723593711853s\n",
      "iteration 79 from 39500000 to 40000000. query took 5.370830774307251s, overall took 5.452043056488037s\n",
      "iteration 80 from 40000000 to 40500000. query took 5.952004909515381s, overall took 6.046228885650635s\n",
      "iteration 81 from 40500000 to 41000000. query took 6.519928932189941s, overall took 6.625221014022827s\n",
      "iteration 82 from 41000000 to 41500000. query took 5.926766633987427s, overall took 6.069753646850586s\n",
      "iteration 83 from 41500000 to 42000000. query took 6.36069393157959s, overall took 6.519433975219727s\n",
      "iteration 84 from 42000000 to 42500000. query took 6.118249893188477s, overall took 6.274156808853149s\n",
      "iteration 85 from 42500000 to 43000000. query took 5.869178056716919s, overall took 6.020174980163574s\n",
      "iteration 86 from 43000000 to 43500000. query took 5.873318910598755s, overall took 6.024786949157715s\n",
      "iteration 87 from 43500000 to 44000000. query took 6.132025957107544s, overall took 6.284291982650757s\n",
      "iteration 88 from 44000000 to 44500000. query took 5.865691184997559s, overall took 6.0178611278533936s\n",
      "iteration 89 from 44500000 to 45000000. query took 5.593027114868164s, overall took 5.661622047424316s\n",
      "iteration 90 from 45000000 to 45500000. query took 10.15321969985962s, overall took 10.217694997787476s\n",
      "iteration 91 from 45500000 to 46000000. query took 5.298935890197754s, overall took 5.362051963806152s\n",
      "iteration 92 from 46000000 to 46500000. query took 6.248634099960327s, overall took 6.333672285079956s\n",
      "iteration 93 from 46500000 to 47000000. query took 5.513162851333618s, overall took 5.584207773208618s\n",
      "iteration 94 from 47000000 to 47500000. query took 5.805202960968018s, overall took 5.891185998916626s\n",
      "iteration 95 from 47500000 to 48000000. query took 5.766768217086792s, overall took 5.865427017211914s\n",
      "iteration 96 from 48000000 to 48500000. query took 6.885131120681763s, overall took 6.948960781097412s\n",
      "iteration 97 from 48500000 to 49000000. query took 5.749946117401123s, overall took 5.817334175109863s\n",
      "iteration 98 from 49000000 to 49500000. query took 5.627484321594238s, overall took 5.714720964431763s\n",
      "iteration 99 from 49500000 to 50000000. query took 6.326647043228149s, overall took 6.39854884147644s\n",
      "iteration 100 from 50000000 to 50500000. query took 5.345139026641846s, overall took 5.414474010467529s\n",
      "iteration 101 from 50500000 to 51000000. query took 5.603348970413208s, overall took 5.692634105682373s\n",
      "iteration 102 from 51000000 to 51500000. query took 5.4886250495910645s, overall took 5.54630184173584s\n",
      "iteration 103 from 51500000 to 52000000. query took 6.9306769371032715s, overall took 7.024245023727417s\n",
      "iteration 104 from 52000000 to 52500000. query took 8.171138048171997s, overall took 8.230718851089478s\n",
      "iteration 105 from 52500000 to 53000000. query took 5.902271032333374s, overall took 5.968536138534546s\n",
      "iteration 106 from 53000000 to 53500000. query took 5.5070390701293945s, overall took 5.566655874252319s\n",
      "iteration 107 from 53500000 to 54000000. query took 5.334735870361328s, overall took 5.397665023803711s\n",
      "iteration 108 from 54000000 to 54500000. query took 6.374461889266968s, overall took 6.442692041397095s\n",
      "iteration 109 from 54500000 to 55000000. query took 5.3681840896606445s, overall took 5.4285101890563965s\n",
      "iteration 110 from 55000000 to 55500000. query took 6.08446192741394s, overall took 6.191961050033569s\n",
      "iteration 111 from 55500000 to 56000000. query took 6.28400993347168s, overall took 6.351668834686279s\n",
      "iteration 112 from 56000000 to 56500000. query took 5.629034042358398s, overall took 5.69284725189209s\n",
      "iteration 113 from 56500000 to 57000000. query took 5.54422402381897s, overall took 5.623770713806152s\n",
      "iteration 114 from 57000000 to 57500000. query took 5.561318874359131s, overall took 5.630791902542114s\n",
      "iteration 115 from 57500000 to 58000000. query took 5.976158857345581s, overall took 6.048212051391602s\n",
      "iteration 116 from 58000000 to 58500000. query took 5.89574670791626s, overall took 5.975306034088135s\n",
      "iteration 117 from 58500000 to 59000000. query took 5.65830397605896s, overall took 5.730449914932251s\n",
      "iteration 118 from 59000000 to 59500000. query took 5.638567209243774s, overall took 5.694957971572876s\n",
      "iteration 119 from 59500000 to 60000000. query took 5.744671106338501s, overall took 5.820353984832764s\n",
      "iteration 120 from 60000000 to 60500000. query took 5.446968078613281s, overall took 5.5106260776519775s\n",
      "iteration 121 from 60500000 to 61000000. query took 5.623298168182373s, overall took 5.695050001144409s\n",
      "iteration 122 from 61000000 to 61500000. query took 5.110122203826904s, overall took 5.1126182079315186s\n",
      "iteration 123 from 61500000 to 62000000. query took 5.080652952194214s, overall took 5.1456849575042725s\n",
      "iteration 124 from 62000000 to 62500000. query took 5.740499973297119s, overall took 5.813913822174072s\n",
      "iteration 125 from 62500000 to 63000000. query took 5.58410906791687s, overall took 5.6567559242248535s\n",
      "iteration 126 from 63000000 to 63500000. query took 5.835978984832764s, overall took 5.9046430587768555s\n",
      "iteration 127 from 63500000 to 64000000. query took 5.765514850616455s, overall took 5.847491025924683s\n",
      "iteration 128 from 64000000 to 64500000. query took 6.132451057434082s, overall took 6.223701000213623s\n",
      "iteration 129 from 64500000 to 65000000. query took 5.734445095062256s, overall took 5.827852010726929s\n",
      "iteration 130 from 65000000 to 65500000. query took 5.385334014892578s, overall took 5.454135179519653s\n",
      "iteration 131 from 65500000 to 66000000. query took 5.39490818977356s, overall took 5.4702770709991455s\n",
      "iteration 132 from 66000000 to 66500000. query took 5.778995990753174s, overall took 5.862410068511963s\n",
      "iteration 133 from 66500000 to 67000000. query took 5.852276086807251s, overall took 5.931419849395752s\n",
      "iteration 134 from 67000000 to 67500000. query took 5.605967998504639s, overall took 5.683461904525757s\n",
      "iteration 135 from 67500000 to 68000000. query took 5.466624021530151s, overall took 5.540919065475464s\n",
      "iteration 136 from 68000000 to 68500000. query took 5.073085069656372s, overall took 5.076413869857788s\n",
      "iteration 137 from 68500000 to 69000000. query took 4.5665810108184814s, overall took 4.566582918167114s\n",
      "iteration 138 from 69000000 to 69500000. query took 4.684656858444214s, overall took 4.684659957885742s\n",
      "iteration 139 from 69500000 to 70000000. query took 4.549876928329468s, overall took 4.54987907409668s\n",
      "iteration 140 from 70000000 to 70500000. query took 4.55745005607605s, overall took 4.557452201843262s\n",
      "iteration 141 from 70500000 to 71000000. query took 4.570852041244507s, overall took 4.5708537101745605s\n",
      "iteration 142 from 71000000 to 71500000. query took 4.539026975631714s, overall took 4.539030075073242s\n",
      "iteration 143 from 71500000 to 72000000. query took 4.664011716842651s, overall took 4.664013862609863s\n",
      "iteration 144 from 72000000 to 72500000. query took 4.455291032791138s, overall took 4.4552929401397705s\n",
      "iteration 145 from 72500000 to 73000000. query took 4.594467878341675s, overall took 4.594470739364624s\n",
      "iteration 146 from 73000000 to 73500000. query took 4.566625118255615s, overall took 4.5666282176971436s\n",
      "iteration 147 from 73500000 to 74000000. query took 4.57419490814209s, overall took 4.574198007583618s\n",
      "iteration 148 from 74000000 to 74500000. query took 4.583066940307617s, overall took 4.583069086074829s\n",
      "iteration 149 from 74500000 to 75000000. query took 4.473283052444458s, overall took 4.47328519821167s\n",
      "iteration 150 from 75000000 to 75500000. query took 4.441143989562988s, overall took 4.441145896911621s\n",
      "iteration 151 from 75500000 to 76000000. query took 4.521886348724365s, overall took 4.521888017654419s\n",
      "iteration 152 from 76000000 to 76500000. query took 4.6353089809417725s, overall took 4.635311126708984s\n",
      "iteration 153 from 76500000 to 77000000. query took 4.433835983276367s, overall took 4.433838129043579s\n",
      "iteration 154 from 77000000 to 77500000. query took 4.4629247188568115s, overall took 4.462926864624023s\n",
      "iteration 155 from 77500000 to 78000000. query took 4.434348106384277s, overall took 4.43435001373291s\n",
      "iteration 156 from 78000000 to 78500000. query took 4.529793739318848s, overall took 4.52979588508606s\n",
      "iteration 157 from 78500000 to 79000000. query took 4.4656360149383545s, overall took 4.465637922286987s\n",
      "iteration 158 from 79000000 to 79500000. query took 4.409681081771851s, overall took 4.409682989120483s\n",
      "iteration 159 from 79500000 to 80000000. query took 4.396827936172485s, overall took 4.396829843521118s\n",
      "iteration 160 from 80000000 to 80500000. query took 4.469097852706909s, overall took 4.469100713729858s\n",
      "iteration 161 from 80500000 to 81000000. query took 4.514865159988403s, overall took 4.5148680210113525s\n",
      "iteration 162 from 81000000 to 81500000. query took 4.560933828353882s, overall took 4.560935735702515s\n",
      "iteration 163 from 81500000 to 82000000. query took 4.434186935424805s, overall took 4.4341888427734375s\n",
      "iteration 164 from 82000000 to 82500000. query took 4.410090208053589s, overall took 4.410092115402222s\n",
      "iteration 165 from 82500000 to 83000000. query took 4.487610101699829s, overall took 4.487613201141357s\n",
      "iteration 166 from 83000000 to 83500000. query took 4.399835109710693s, overall took 4.399836778640747s\n",
      "iteration 167 from 83500000 to 84000000. query took 4.67997407913208s, overall took 4.679976940155029s\n",
      "iteration 168 from 84000000 to 84500000. query took 4.3959033489227295s, overall took 4.395906209945679s\n",
      "iteration 169 from 84500000 to 85000000. query took 4.32454514503479s, overall took 4.324547052383423s\n",
      "iteration 170 from 85000000 to 85500000. query took 4.478860139846802s, overall took 4.478863000869751s\n",
      "iteration 171 from 85500000 to 86000000. query took 4.513427019119263s, overall took 4.513429164886475s\n",
      "iteration 172 from 86000000 to 86500000. query took 4.444139003753662s, overall took 4.444141149520874s\n",
      "iteration 173 from 86500000 to 87000000. query took 4.409636735916138s, overall took 4.409639835357666s\n",
      "iteration 174 from 87000000 to 87500000. query took 4.41119384765625s, overall took 4.411195993423462s\n",
      "iteration 175 from 87500000 to 88000000. query took 4.6047680377960205s, overall took 4.604769945144653s\n",
      "iteration 176 from 88000000 to 88500000. query took 4.60027003288269s, overall took 4.600273132324219s\n",
      "iteration 177 from 88500000 to 89000000. query took 4.381880044937134s, overall took 4.381881952285767s\n",
      "iteration 178 from 89000000 to 89500000. query took 4.446381092071533s, overall took 4.446383953094482s\n",
      "iteration 179 from 89500000 to 90000000. query took 4.454540967941284s, overall took 4.4545440673828125s\n",
      "iteration 180 from 90000000 to 90500000. query took 4.420276165008545s, overall took 4.420279026031494s\n",
      "iteration 181 from 90500000 to 91000000. query took 4.539085149765015s, overall took 4.539087295532227s\n",
      "iteration 182 from 91000000 to 91500000. query took 4.428329944610596s, overall took 4.428332805633545s\n",
      "iteration 183 from 91500000 to 92000000. query took 4.405994176864624s, overall took 4.405996084213257s\n",
      "iteration 184 from 92000000 to 92500000. query took 4.416383266448975s, overall took 4.416384935379028s\n",
      "iteration 185 from 92500000 to 93000000. query took 4.502187013626099s, overall took 4.5022149085998535s\n",
      "iteration 186 from 93000000 to 93500000. query took 4.406663179397583s, overall took 4.406664848327637s\n",
      "iteration 187 from 93500000 to 94000000. query took 4.427849054336548s, overall took 4.427850961685181s\n",
      "iteration 188 from 94000000 to 94500000. query took 4.701767921447754s, overall took 4.735903024673462s\n",
      "iteration 189 from 94500000 to 95000000. query took 5.096492052078247s, overall took 5.166707992553711s\n",
      "iteration 190 from 95000000 to 95500000. query took 4.767068147659302s, overall took 4.799823045730591s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_file_root = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/pfam/tmp/eukaryotic_pfam_smart_not_unique_\"\n",
    "\n",
    "#\n",
    "# IMPORTANT - WHY WE NEED THIS\n",
    "#\n",
    "# W2V_TOKEN was created from protein2ipr.dat and thus has loads of non-eukaryiotic entries\n",
    "# that are not relevant. This script creates a list of pfam entries for eukaryotic proteins \n",
    "# only by joining across W2V_PROTEIN_UREF100_E and W2V_TOKEN by UNIPROT_ID\n",
    "#\n",
    "# W2V_PROTEIN_UREF100_E only has eukaryotic proteins in it\n",
    "#\n",
    "# This script outputs to a number of files all protein-pfam couples, there are many repeated elements\n",
    "# A new file is created for each 'chunk'.These can be combined into one and then sorted into a unique\n",
    "# list - see 'combine_pfam_list.sh'\n",
    "#\n",
    "#\n",
    "# *** VERY IMPORTANT ****\n",
    "# Having a counter on W2V_PROTEIN_UREF100_E and moving through that table in chunks\n",
    "# is SIGNIFICANTLY faster that doing a join using LIMIT and OFFSET\n",
    "#\n",
    "def get_eukaryotic_pfams(start_pos, end_pos, iteration):\n",
    "    #print(f\"iteration {iteration} from {start_pos} to {end_pos}.\")\n",
    "    s = time.time()\n",
    "    \n",
    "    output_file = output_file_root+ str(iteration) + \".txt\"\n",
    "    # create long life/expensive objects\n",
    "    of  = open(output_file, \"w\")\n",
    "    con = duckdb.connect(database=db_string)\n",
    "    \n",
    "    try:\n",
    "        #con = duckdb.connect(database=db_string)  \n",
    "        #results = con.execute(f\"SELECT W2V_PROTEIN_UREF100_E.UNIPROT_ID, W2V_TOKEN.TOKEN FROM ( SELECT UNIPROT_ID FROM W2V_PROTEIN_UREF100_E ORDER BY UNIPROT_ID LIMIT {limit} OFFSET {offset}) AS W2V_PROTEIN_UREF100_E INNER JOIN W2V_TOKEN AS W2V_TOKEN ON W2V_PROTEIN_UREF100_E.UNIPROT_ID = W2V_TOKEN.UNIPROT_ID WHERE W2V_TOKEN.TYPE = 'PFAM' \").fetchall()\n",
    "        results = con.execute(f\"SELECT W2V_PROTEIN_UREF100_E.UNIPROT_ID, W2V_TOKEN.TOKEN FROM ( SELECT UNIPROT_ID FROM W2V_PROTEIN_UREF100_E WHERE COUNTER >= {start_pos} and COUNTER < {end_pos}) AS W2V_PROTEIN_UREF100_E INNER JOIN W2V_TOKEN AS W2V_TOKEN ON W2V_PROTEIN_UREF100_E.UNIPROT_ID = W2V_TOKEN.UNIPROT_ID WHERE W2V_TOKEN.TYPE = 'PFAM' \").fetchall()\n",
    "    except Exception as e:\n",
    "        print(f\"Error on iteration {iteration}, {e}, closing finr {output_file}\")\n",
    "        of.close()\n",
    "        con.close()\n",
    "        return\n",
    "    e1 = time.time()\n",
    "\n",
    "    for res in results:\n",
    "        #print(res[1])\n",
    "        of.write(res[1] +'\\n')        \n",
    "    e2 = time.time()\n",
    "\n",
    "    print(f\"iteration {iteration} from {start_pos} to {end_pos}. query took {e1-s}s, overall took {e2-s}s\")\n",
    "\n",
    "    of.close()\n",
    "    con.close()\n",
    "\n",
    "num_eukaryotic = 95272305\n",
    "start_pos       = 0    # start point\n",
    "chunk_size      = 500000    # how many rows to return\n",
    "end_pos         = chunk_size\n",
    "iterations      = (num_eukaryotic // chunk_size) + 1\n",
    "\n",
    "# not starting from 0 as alredy did 60\n",
    "#start_iteration = iterations - 61\n",
    "\n",
    "print(iterations, 'required.')\n",
    "\n",
    "for i in range(iterations):\n",
    "    get_eukaryotic_pfams(start_pos, end_pos, i)\n",
    "    start_pos += chunk_size\n",
    "    end_pos += chunk_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table of pfam ids without the 'PF' in front!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=db_string)\n",
    "\n",
    "output_file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/pfam/unique_eukaryotic_pfam_ids.txt\"          \n",
    "con.execute(\"CREATE TABLE W2V_PFAM_E AS SELECT * FROM read_csv_auto('/Users/patrick/dev/ucl/comp0158_mscproject/data/pfam/unique_eukaryotic_pfam_ids.txt', columns={'COUNTER' :'USMALLINT', 'STRIPPED_PFAM_ID': 'USMALLINT', 'PFAM_ID': 'VARCHAR'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15577,)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=db_string)           \n",
    "count = con.execute(\"SELECT COUNT(*) FROM W2V_PFAM_E\").fetchall()\n",
    "print(count[0])\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for PFAM and PROTEIN ENTRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V_PROTEIN [('A0A010PZJ8', 1, 494)]\n",
      "W2V_PROTEIN_UREF100_E [('UniRef100', 'A0A010PZJ8', 493, 1, 494, 1, 1445577, 'Colletotrichum fioriniae PJ7')]\n"
     ]
    }
   ],
   "source": [
    "# test that W2V_TOKEN has all pfam and disorder entries\n",
    "# 1445577   : Colletotrichum fioriniae PJ7\n",
    "# 10116     : Rattus norvegicus\n",
    "con = duckdb.connect(database=db_string)\n",
    "\n",
    "# 1. Test - find a protein with pfam entries\n",
    "#    - Both of these work\n",
    "#protein_id = \"A0A009GYB3\" # this is prob not eukaryotic\n",
    "protein_id = \"A0A010PZJ8\"\n",
    "\n",
    "#tokens = con.execute(\"SELECT * FROM W2V_TOKEN WHERE UNIPROT_ID = 'A0A009GYB3'\").fetchall()\n",
    "#tokens = con.execute(\"SELECT * FROM W2V_TOKEN WHERE UNIPROT_ID = (?)\", [protein_id] ).fetchall()\n",
    "\n",
    "# 2. Find that same protein in W2V_PROTEIN\n",
    "# doesn't work - possibly because the pfam entries are from all proteins whereas W2V_PROTEIN only\n",
    "# has TrEMBL Eukaryotic proteins\n",
    "#tokens = con.execute(\"SELECT * FROM W2V_PROTEIN WHERE UNIPROT_ID = 'A0A009GYB3'\").fetchall()\n",
    "tokens = con.execute(\"SELECT * FROM W2V_PROTEIN WHERE UNIPROT_ID = (?)\", [protein_id]).fetchall()\n",
    "print('W2V_PROTEIN', tokens)\n",
    "\n",
    "# doesn't work\n",
    "#tokens = con.execute(\"SELECT * FROM W2V_TOKEN WHERE UNIPROT_ID = (?)\", ['protein_id']).fetchall()\n",
    "\n",
    "# none of these work - is the protein A0A009GYB3 in UniRef??\n",
    "# tokens = con.execute(\"SELECT * FROM W2V_PROTEIN_UNIREF_100_ALL_TAX WHERE UNIPROT_ID = 'A0A009GYB3'\").fetchall()\n",
    "tokens = con.execute(\"SELECT * FROM W2V_PROTEIN_UREF100_E WHERE UNIPROT_ID = (?)\", [protein_id]).fetchall()\n",
    "# tokens = con.execute(\"SELECT * FROM W2V_PROTEIN_UNIREF_100_ALL_TAX WHERE UNIPROT_ID = (?)\", [protein_id]).fetchall()\n",
    "# grep \"A0A009GYB3\" uniref100_tax_20240801.dat > returns nothing\n",
    "\n",
    "print('W2V_PROTEIN_UREF100_E', tokens)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('E', '710243', '1445577')]\n",
      "[('1445577', 'Colletotrichum fioriniae PJ7')]\n"
     ]
    }
   ],
   "source": [
    "# test that W2V_TOKEN has all pfam and disorder entries\n",
    "# 1445577   : Colletotrichum fioriniae PJ7\n",
    "# 10116     : Rattus norvegicus\n",
    "con = duckdb.connect(database=db_string)           \n",
    "tokens = con.execute(\"SELECT * FROM W2V_TAX_CAT WHERE ID=(?)\", ['1445577']).fetchall()\n",
    "print(tokens)\n",
    "tokens = con.execute(\"SELECT * FROM W2V_TAX_NAME WHERE TAX_ID=(?)\", ['1445577']).fetchall()\n",
    "print(tokens)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=db_string)           \n",
    "con.execute(\"DROP TABLE PROTEIN\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlock database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# this doesn;t seem to work....\n",
    "def is_locked():\n",
    "    lock_file = f'{db_string}.lock'\n",
    "    return os.path.exists(lock_file)\n",
    "\n",
    "is_locked()\n",
    "\n",
    "# ... but this does from a command prompt\n",
    "#fuser database/proteins.db\n",
    "\n",
    "fuser /Users/patrick/dev/ucl/comp0158_mscproject/database/w2v_20240731_test.db\n",
    "\n",
    "# then kill -9 <id if there is one list>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
