{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Tested again on July 31st and August 1st\n",
    "\n",
    "\n",
    "This notebook contains various scripts to load data into tables on a local DuckDB database. <br>\n",
    "- proteins are loaded into W2V_PROTEIN\n",
    "- pfam entries are loaded into W2V_TOKEN\n",
    "- disorder regions are also loaded into W2V_TOKEN\n",
    "\n",
    "The tables are created at the time the data is loaded - so see the appropariate cells for the table definition.\n",
    "\n",
    "Indexes are applied after the data is loaded.\n",
    "\n",
    "\n",
    "DuckDB is very easy to install on a mac and can load tab-delimited files extremely quickly.\n",
    "To recreate this environment, you just need to install DuckDB and then set the db_string at the top of this file\n",
    "to the location where you wish the database file to be stored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "#\n",
    "# TODO - SET THIS STRING TO WHERE YOU WANT THE DB TO STORE ITS DATA\n",
    "#\n",
    "db_string = \"/Users/patrick/dev/ucl/comp0158_mscproject/database/w2v_20240731_test.db\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the DB works OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A TABLE\n",
    "#con = duckdb.connect(database=':memory:')\n",
    "con = duckdb.connect(database=db_string)  \n",
    "duckdb.sql(\"\\\n",
    "    CREATE TABLE TEST (\\\n",
    "        ID VARCHAR,\\\n",
    "    )\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n",
      "│ column_name │ column_type │  null   │   key   │ default │  extra  │\n",
      "│   varchar   │   varchar   │ varchar │ varchar │ varchar │ varchar │\n",
      "├─────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ ID          │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "└─────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DESCRIBE\n",
    "con = duckdb.connect(database=db_string)\n",
    "res = duckdb.sql(\"DESCRIBE TEST\")\n",
    "print(res)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP\n",
    "con = duckdb.connect(database=db_string)  \n",
    "duckdb.sql(\"DROP TABLE TEST\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION - CONVESION TO DAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PROTEIN TrEMBL : INTO W2V_PROTEIN\n",
    "\n",
    "I initialy used TrEMBL to create the corpus as the UniRef100 extract was too large and kept breaking my Macbook!\n",
    "I subsequently found that it's possible to download only the eukaryortic UniRef100 proteins from the uniprot website. For this, it's necessary to filter on tax id 2759 and also select 100% completion(?). Note that it takes between 12 and 16 hours for Uniprot to prepare the extract for download, but it's worth it as it contains the taxonomy details as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load protein file into protein table\n",
    "# 20 July 2024 - This took 12.9s to load uniprotkb-2759_78494531.dat (78M proteins)\n",
    "# 31 July testing again to check code works\n",
    "con = duckdb.connect(database=db_string)           \n",
    "con.execute(\"CREATE TABLE W2V_PROTEIN AS SELECT * FROM read_csv_auto('/Volumes/My Passport/data/protein/dat/uniprotkb-2759_78494531.dat', columns={'uniprot_id' :'VARCHAR', 'start': 'USMALLINT', 'end': 'USMALLINT'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(78494529,)]\n"
     ]
    }
   ],
   "source": [
    "# This should output that there are 78,494,529 items\n",
    "con = duckdb.connect(database=db_string)           \n",
    "protein_count = con.execute(\"SELECT COUNT(*) FROM W2V_PROTEIN\").fetchall()\n",
    "print(protein_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=db_string)           \n",
    "#con.execute(\"DROP TABLE W2V_PROTEIN\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created\n"
     ]
    }
   ],
   "source": [
    "# create an index (after loading the data)\n",
    "con = duckdb.connect(database=db_string)   \n",
    "con.execute(\"CREATE INDEX UNIP_IDX ON W2V_PROTEIN(UNIPROT_ID)\")\n",
    "print('index created')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A0A010PZU8', 1, 1389)]\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=db_string)      \n",
    "\n",
    "# SELECT FROM LIST OF IDS - REALLY SLOW\n",
    "#list = ['A0A010R6E0', 'A0A010RP22']\n",
    "#entries = con.execute(\"SELECT * FROM PFAM_TOKEN WHERE column0 IN (SELECT UNNEST(?))\", [list]).fetchall()\n",
    "\n",
    "res = con.execute(\"SELECT * FROM W2V_PROTEIN WHERE UNIPROT_ID = (?)\", ['A0A010PZU8']).fetchall()\n",
    "\n",
    "print(res)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PROTEIN - UNIREF :\n",
    "\n",
    "UniRef100 - All Eukaryotic Proteins - including Taxonomy Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load protein file into protein table\n",
    "# 05 Aug 2024 - Took 1min 4.5s to load 95,272,305 items\n",
    "#\n",
    "# [counter, uniprot_id, len, start, end, n_members, tax_id, tax_name]\n",
    "#\n",
    "con = duckdb.connect(database=db_string)           \n",
    "con.execute(\"CREATE TABLE W2V_PROTEIN_UREF100_E AS SELECT * FROM read_csv_auto('/Users/patrick/dev/ucl/comp0158_mscproject/data/protein/uniref100only_2759-95272305_20240805.dat', columns={'counter' : UINTEGER, 'uniprot_id' :VARCHAR, 'length': USMALLINT, 'start': USMALLINT, 'end': USMALLINT, 'n_members': USMALLINT, 'tax_id' :UINTEGER, 'tax_name' : VARCHAR})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This should output that there are 95,272,305 items\n",
    "con = duckdb.connect(database=db_string)           \n",
    "#res = con.execute(\"DROP TABLE W2V_PROTEIN_UREF100_E\").fetchall()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('counter', 'UINTEGER', 'YES', None, None, None), ('uniprot_id', 'VARCHAR', 'YES', None, None, None), ('length', 'USMALLINT', 'YES', None, None, None), ('start', 'USMALLINT', 'YES', None, None, None), ('end', 'USMALLINT', 'YES', None, None, None), ('n_members', 'USMALLINT', 'YES', None, None, None), ('tax_id', 'UINTEGER', 'YES', None, None, None), ('tax_name', 'VARCHAR', 'YES', None, None, None)]\n"
     ]
    }
   ],
   "source": [
    "# This should output that there are 95,272,305 items\n",
    "con = duckdb.connect(database=db_string)           \n",
    "count = con.execute(\"DESCRIBE W2V_PROTEIN_UREF100_E\").fetchall()\n",
    "print(count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95272305,)\n"
     ]
    }
   ],
   "source": [
    "# This should output that there are 95,272,305 items\n",
    "con = duckdb.connect(database=db_string)           \n",
    "count = con.execute(\"SELECT COUNT(*) FROM W2V_PROTEIN_UREF100_E\").fetchall()\n",
    "print(count[0])\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index (after loading the data) - initially ran out of memory on Macbook with all proteins\n",
    "# Took 25s with eukaryotic only\n",
    "#\n",
    "# Going to create 2 indices as have added a counter column and want an index\n",
    "#\n",
    "con = duckdb.connect(database=db_string)   \n",
    "con.execute(\"CREATE INDEX UNIREF100_IDX ON W2V_PROTEIN_UREF100_E(UNIPROT_ID)\")\n",
    "con.execute(\"CREATE INDEX COUNTER_IDX ON W2V_PROTEIN_UREF100_E(COUNTER)\")\n",
    "print('indices created')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=db_string)   \n",
    "results = con.execute(\"SELECT * FROM W2V_PROTEIN_UREF100_E WHERE COUNTER >= 30000000 AND COUNTER <30000100 \").fetchall()\n",
    "for res in results:\n",
    "    print (res)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PFAM TOKENS INTO W2V_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# July 20 2024 - Took 1m 55s to load 296,017,815 entries from a directory on a macbook\n",
    "# July 31 2024 - Restest took 3m 10s from an external drive attached to macbook\n",
    "con = duckdb.connect(database=db_string)\n",
    "\n",
    "con.execute(\"CREATE TABLE W2V_TOKEN AS SELECT * FROM read_csv_auto('/Volumes/My Passport/data/pfam/protein2ipr_pfam_20240715.dat', columns={'uniprot_id' :'VARCHAR', 'type' : 'VARCHAR', 'token' : 'VARCHAR', 'start': 'USMALLINT', 'end': 'USMALLINT'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(377274915,)]\n"
     ]
    }
   ],
   "source": [
    "# with pfam only this shows 296,017,815 entries\n",
    "# after loading disorder as well this shows 377,274,915 (81,257,100 disorder entries)\n",
    "con = duckdb.connect(database=db_string)           \n",
    "protein_count = con.execute(\"SELECT COUNT(*) FROM W2V_TOKEN\").fetchall()\n",
    "print(protein_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index (after loading data)\n",
    "con = duckdb.connect(database=db_string)  \n",
    "res = con.execute(\"CREATE INDEX PF_TKN_IDX ON W2V_TOKEN(UNIPROT_ID)\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 614 entries for PF20200\n",
      "Found 607 unique proteins containing PF20200\n",
      "Found 0 eukaryotic proteins with pfam entry PF20200\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "con = duckdb.connect(database=db_string)\n",
    "#token = 'PF19782' # Has 0 eukaryotic proteins\n",
    "#token = 'PF20176' # Has 0 eukaryotic proteins\n",
    "token = 'PF20200' # Has 0 eukaryotic proteins\n",
    "#token = 'PF14033' # Has about 4852 eukaryotic proteins\n",
    "\n",
    "# get number of times this token is in W2V_TOKEN\n",
    "token_count = con.execute(\"SELECT COUNT(*) FROM W2V_TOKEN WHERE TOKEN=(?)\", [token]).fetchall()\n",
    "print(f\"found {token_count[0][0]} entries for {token}\")\n",
    "\n",
    "# get the protein id for each token\n",
    "results = con.execute(\"SELECT DISTINCT UNIPROT_ID FROM W2V_TOKEN WHERE TOKEN=(?)\", [token]).fetchall()\n",
    "\n",
    "print(f\"Found {len(results)} unique proteins containing {token}\")\n",
    "\n",
    "\n",
    "# need to check if that protein is actually eukaryotic\n",
    "eukaryotic_count = 0\n",
    "for protein_res in results:\n",
    "    #print(res)\n",
    "    protein_id = protein_res[0]\n",
    "    \n",
    "    # check for protein_id\n",
    "    protein_count = con.execute(\"SELECT COUNT(*) FROM W2V_PROTEIN WHERE UNIPROT_ID=(?)\", [protein_id]).fetchall()\n",
    "    \n",
    "    count = protein_count[0][0]\n",
    "    \n",
    "    if(count >0):\n",
    "        eukaryotic_count +=1\n",
    "        \n",
    "print(f\"Found {eukaryotic_count} eukaryotic proteins with pfam entry {token}\") \n",
    "    \n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### LOAD DISORDER ITEMS INTO W2V_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load disorder entries\n",
    "# First run : July 19\n",
    "# Retest    : August 1st (on Macbook - took 2mn 25s)\n",
    "con = duckdb.connect(database=db_string) \n",
    "con.execute(\"INSERT INTO W2V_TOKEN SELECT * FROM read_csv_auto('/Volumes/My Passport/data/disorder/dat/disordered_tokens_20240719.dat')\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(81257100,)]\n"
     ]
    }
   ],
   "source": [
    "# with pfam only this shows 296,017,815 entries\n",
    "# after loading disorder as well this shows 377,274,915\n",
    "con = duckdb.connect(database=db_string)           \n",
    "token_count = con.execute(\"SELECT COUNT(*) FROM W2V_TOKEN WHERE TYPE='DISORDER'\").fetchall()\n",
    "print(token_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A0A010PZU8', 'PFAM', 'PF00400', 865, 900), ('A0A010PZU8', 'PFAM', 'PF00400', 928, 955), ('A0A010PZU8', 'PFAM', 'PF00400', 960, 998), ('A0A010PZU8', 'PFAM', 'PF00400', 1017, 1040), ('A0A010PZU8', 'PFAM', 'PF00400', 1078, 1108), ('A0A010PZU8', 'PFAM', 'PF00400', 1233, 1260), ('A0A010PZU8', 'PFAM', 'PF05729', 358, 479), ('A0A010PZU8', 'PFAM', 'PF17100', 152, 254), ('A0A010PZU8', 'DISORDER', 'Consensus Disorder Prediction', 1, 30)]\n"
     ]
    }
   ],
   "source": [
    "# test that W2V_TOKEN has all pfam and disorder entries\n",
    "con = duckdb.connect(database=db_string)           \n",
    "tokens = con.execute(\"SELECT * FROM W2V_TOKEN WHERE UNIPROT_ID=(?)\", ['A0A010PZU8']).fetchall()\n",
    "print(tokens)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TAXONOMY INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the data-preparation folder for a shell script that produces the .dat file loaded here\n",
    "con = duckdb.connect(database=db_string)\n",
    "con.execute(\"CREATE TABLE W2V_TAX_NAME AS SELECT * FROM read_csv_auto('/Volumes/My Passport/data/taxonomy/dat/scientific_names_20240802.dat', columns={'tax_id' :'VARCHAR', 'name' : 'VARCHAR'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2588170,)]\n"
     ]
    }
   ],
   "source": [
    "# count  - should have 2,588,170 entries\n",
    "con = duckdb.connect(database=db_string)           \n",
    "token_count = con.execute(\"SELECT COUNT(*) FROM W2V_TAX_NAME\").fetchall()\n",
    "print(token_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index (after loading data)\n",
    "con = duckdb.connect(database=db_string)  \n",
    "res = con.execute(\"CREATE INDEX TAX_NM_IDX ON W2V_TAX_NAME(TAX_ID)\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the data-preparation folder for a shell script that produces the .dat file loaded here\n",
    "con = duckdb.connect(database=db_string)\n",
    "con.execute(\"CREATE TABLE W2V_TAX_CAT AS SELECT * FROM read_csv_auto('/Volumes/My Passport/data/taxonomy/dat/categories_20240802.dat', columns={'type' : 'VARCHAR', 'parent_id' :'VARCHAR', 'id' : 'VARCHAR'})\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1567316,)]\n"
     ]
    }
   ],
   "source": [
    "# count  - should have 1,567,316 entries\n",
    "con = duckdb.connect(database=db_string)           \n",
    "token_count = con.execute(\"SELECT COUNT(*) FROM W2V_TAX_CAT\").fetchall()\n",
    "print(token_count)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index (after loading data)\n",
    "con = duckdb.connect(database=db_string)  \n",
    "res = con.execute(\"CREATE INDEX TAX_CT_IDX ON W2V_TAX_CAT(ID)\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1310608', 'Acinetobacter sp. 1295259')]\n"
     ]
    }
   ],
   "source": [
    "# test that W2V_TOKEN has all pfam and disorder entries\n",
    "# 1445577   : Colletotrichum fioriniae PJ7\n",
    "# 10116     : Rattus norvegicus\n",
    "con = duckdb.connect(database=db_string)           \n",
    "tokens = con.execute(\"SELECT * FROM W2V_TAX_NAME WHERE TAX_ID=(?)\", ['1310608']).fetchall()\n",
    "print(tokens)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION - GET TOKENS FOR E PROTEINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 querying from 0 to 1000000.\n",
      "iteration 0 querying from 0 to 1000000. query took 10.43s, overall took 12.18s\n",
      "iteration 1 querying from 1000000 to 2000000.\n",
      "iteration 1 querying from 1000000 to 2000000. query took 11.78s, overall took 13.87s\n",
      "iteration 2 querying from 2000000 to 3000000.\n",
      "iteration 2 querying from 2000000 to 3000000. query took 9.38s, overall took 10.63s\n",
      "iteration 3 querying from 3000000 to 4000000.\n",
      "iteration 3 querying from 3000000 to 4000000. query took 11.54s, overall took 13.47s\n",
      "iteration 4 querying from 4000000 to 5000000.\n",
      "iteration 4 querying from 4000000 to 5000000. query took 11.61s, overall took 13.36s\n",
      "iteration 5 querying from 5000000 to 6000000.\n",
      "iteration 5 querying from 5000000 to 6000000. query took 10.69s, overall took 12.62s\n",
      "iteration 6 querying from 6000000 to 7000000.\n",
      "iteration 6 querying from 6000000 to 7000000. query took 10.82s, overall took 12.81s\n",
      "iteration 7 querying from 7000000 to 8000000.\n",
      "iteration 7 querying from 7000000 to 8000000. query took 11.63s, overall took 13.76s\n",
      "iteration 8 querying from 8000000 to 9000000.\n",
      "iteration 8 querying from 8000000 to 9000000. query took 11.06s, overall took 13.02s\n",
      "iteration 9 querying from 9000000 to 10000000.\n",
      "iteration 9 querying from 9000000 to 10000000. query took 10.76s, overall took 12.67s\n",
      "iteration 10 querying from 10000000 to 11000000.\n",
      "iteration 10 querying from 10000000 to 11000000. query took 11.82s, overall took 14.08s\n",
      "iteration 11 querying from 11000000 to 12000000.\n",
      "iteration 11 querying from 11000000 to 12000000. query took 11.26s, overall took 13.11s\n",
      "iteration 12 querying from 12000000 to 13000000.\n",
      "iteration 12 querying from 12000000 to 13000000. query took 10.94s, overall took 12.84s\n",
      "iteration 13 querying from 13000000 to 14000000.\n",
      "iteration 13 querying from 13000000 to 14000000. query took 13.1s, overall took 15.67s\n",
      "iteration 14 querying from 14000000 to 15000000.\n",
      "iteration 14 querying from 14000000 to 15000000. query took 10.43s, overall took 12.1s\n",
      "iteration 15 querying from 15000000 to 16000000.\n",
      "iteration 15 querying from 15000000 to 16000000. query took 11.67s, overall took 13.83s\n",
      "iteration 16 querying from 16000000 to 17000000.\n",
      "iteration 16 querying from 16000000 to 17000000. query took 10.47s, overall took 12.18s\n",
      "iteration 17 querying from 17000000 to 18000000.\n",
      "iteration 17 querying from 17000000 to 18000000. query took 11.88s, overall took 14.32s\n",
      "iteration 18 querying from 18000000 to 19000000.\n",
      "iteration 18 querying from 18000000 to 19000000. query took 12.5s, overall took 15.0s\n",
      "iteration 19 querying from 19000000 to 20000000.\n",
      "iteration 19 querying from 19000000 to 20000000. query took 11.7s, overall took 13.92s\n",
      "iteration 20 querying from 20000000 to 21000000.\n",
      "iteration 20 querying from 20000000 to 21000000. query took 11.23s, overall took 13.19s\n",
      "iteration 21 querying from 21000000 to 22000000.\n",
      "iteration 21 querying from 21000000 to 22000000. query took 10.81s, overall took 12.72s\n",
      "iteration 22 querying from 22000000 to 23000000.\n",
      "iteration 22 querying from 22000000 to 23000000. query took 11.58s, overall took 13.85s\n",
      "iteration 23 querying from 23000000 to 24000000.\n",
      "iteration 23 querying from 23000000 to 24000000. query took 10.94s, overall took 13.11s\n",
      "iteration 24 querying from 24000000 to 25000000.\n",
      "iteration 24 querying from 24000000 to 25000000. query took 10.99s, overall took 13.09s\n",
      "iteration 25 querying from 25000000 to 26000000.\n",
      "iteration 25 querying from 25000000 to 26000000. query took 11.04s, overall took 13.21s\n",
      "iteration 26 querying from 26000000 to 27000000.\n",
      "iteration 26 querying from 26000000 to 27000000. query took 15.93s, overall took 19.29s\n",
      "iteration 27 querying from 27000000 to 28000000.\n",
      "iteration 27 querying from 27000000 to 28000000. query took 11.53s, overall took 13.33s\n",
      "iteration 28 querying from 28000000 to 29000000.\n",
      "iteration 28 querying from 28000000 to 29000000. query took 11.67s, overall took 13.48s\n",
      "iteration 29 querying from 29000000 to 30000000.\n",
      "iteration 29 querying from 29000000 to 30000000. query took 9.28s, overall took 10.51s\n",
      "iteration 30 querying from 30000000 to 31000000.\n",
      "iteration 30 querying from 30000000 to 31000000. query took 9.34s, overall took 10.65s\n",
      "iteration 31 querying from 31000000 to 32000000.\n",
      "iteration 31 querying from 31000000 to 32000000. query took 8.17s, overall took 9.0s\n",
      "iteration 32 querying from 32000000 to 33000000.\n",
      "iteration 32 querying from 32000000 to 33000000. query took 8.67s, overall took 9.62s\n",
      "iteration 33 querying from 33000000 to 34000000.\n",
      "iteration 33 querying from 33000000 to 34000000. query took 9.52s, overall took 10.77s\n",
      "iteration 34 querying from 34000000 to 35000000.\n",
      "iteration 34 querying from 34000000 to 35000000. query took 9.33s, overall took 10.54s\n",
      "iteration 35 querying from 35000000 to 36000000.\n",
      "iteration 35 querying from 35000000 to 36000000. query took 9.22s, overall took 10.51s\n",
      "iteration 36 querying from 36000000 to 37000000.\n",
      "iteration 36 querying from 36000000 to 37000000. query took 8.04s, overall took 8.61s\n",
      "iteration 37 querying from 37000000 to 38000000.\n",
      "iteration 37 querying from 37000000 to 38000000. query took 8.89s, overall took 9.74s\n",
      "iteration 38 querying from 38000000 to 39000000.\n",
      "iteration 38 querying from 38000000 to 39000000. query took 9.0s, overall took 9.71s\n",
      "iteration 39 querying from 39000000 to 40000000.\n",
      "iteration 39 querying from 39000000 to 40000000. query took 8.99s, overall took 9.85s\n",
      "iteration 40 querying from 40000000 to 41000000.\n",
      "iteration 40 querying from 40000000 to 41000000. query took 9.3s, overall took 10.6s\n",
      "iteration 41 querying from 41000000 to 42000000.\n",
      "iteration 41 querying from 41000000 to 42000000. query took 10.37s, overall took 12.1s\n",
      "iteration 42 querying from 42000000 to 43000000.\n",
      "iteration 42 querying from 42000000 to 43000000. query took 10.0s, overall took 11.44s\n",
      "iteration 43 querying from 43000000 to 44000000.\n",
      "iteration 43 querying from 43000000 to 44000000. query took 9.81s, overall took 11.23s\n",
      "iteration 44 querying from 44000000 to 45000000.\n",
      "iteration 44 querying from 44000000 to 45000000. query took 9.83s, overall took 11.39s\n",
      "iteration 45 querying from 45000000 to 46000000.\n",
      "iteration 45 querying from 45000000 to 46000000. query took 8.34s, overall took 9.22s\n",
      "iteration 46 querying from 46000000 to 47000000.\n",
      "iteration 46 querying from 46000000 to 47000000. query took 8.86s, overall took 9.92s\n",
      "iteration 47 querying from 47000000 to 48000000.\n",
      "iteration 47 querying from 47000000 to 48000000. query took 8.88s, overall took 9.95s\n",
      "iteration 48 querying from 48000000 to 49000000.\n",
      "iteration 48 querying from 48000000 to 49000000. query took 8.11s, overall took 8.86s\n",
      "iteration 49 querying from 49000000 to 50000000.\n",
      "iteration 49 querying from 49000000 to 50000000. query took 8.64s, overall took 9.66s\n",
      "iteration 50 querying from 50000000 to 51000000.\n",
      "iteration 50 querying from 50000000 to 51000000. query took 8.66s, overall took 9.61s\n",
      "iteration 51 querying from 51000000 to 52000000.\n",
      "iteration 51 querying from 51000000 to 52000000. query took 8.29s, overall took 9.19s\n",
      "iteration 52 querying from 52000000 to 53000000.\n",
      "iteration 52 querying from 52000000 to 53000000. query took 8.06s, overall took 8.84s\n",
      "iteration 53 querying from 53000000 to 54000000.\n",
      "iteration 53 querying from 53000000 to 54000000. query took 8.44s, overall took 9.2s\n",
      "iteration 54 querying from 54000000 to 55000000.\n",
      "iteration 54 querying from 54000000 to 55000000. query took 8.09s, overall took 8.82s\n",
      "iteration 55 querying from 55000000 to 56000000.\n",
      "iteration 55 querying from 55000000 to 56000000. query took 8.76s, overall took 9.81s\n",
      "iteration 56 querying from 56000000 to 57000000.\n",
      "iteration 56 querying from 56000000 to 57000000. query took 8.41s, overall took 9.3s\n",
      "iteration 57 querying from 57000000 to 58000000.\n",
      "iteration 57 querying from 57000000 to 58000000. query took 8.42s, overall took 9.24s\n",
      "iteration 58 querying from 58000000 to 59000000.\n",
      "iteration 58 querying from 58000000 to 59000000. query took 8.51s, overall took 9.45s\n",
      "iteration 59 querying from 59000000 to 60000000.\n",
      "iteration 59 querying from 59000000 to 60000000. query took 8.15s, overall took 8.89s\n",
      "iteration 60 querying from 60000000 to 61000000.\n",
      "iteration 60 querying from 60000000 to 61000000. query took 8.15s, overall took 8.94s\n",
      "iteration 61 querying from 61000000 to 62000000.\n",
      "iteration 61 querying from 61000000 to 62000000. query took 7.29s, overall took 7.8s\n",
      "iteration 62 querying from 62000000 to 63000000.\n",
      "iteration 62 querying from 62000000 to 63000000. query took 9.34s, overall took 10.25s\n",
      "iteration 63 querying from 63000000 to 64000000.\n",
      "iteration 63 querying from 63000000 to 64000000. query took 8.39s, overall took 9.26s\n",
      "iteration 64 querying from 64000000 to 65000000.\n",
      "iteration 64 querying from 64000000 to 65000000. query took 8.81s, overall took 9.94s\n",
      "iteration 65 querying from 65000000 to 66000000.\n",
      "iteration 65 querying from 65000000 to 66000000. query took 8.09s, overall took 8.95s\n",
      "iteration 66 querying from 66000000 to 67000000.\n",
      "iteration 66 querying from 66000000 to 67000000. query took 8.51s, overall took 9.48s\n",
      "iteration 67 querying from 67000000 to 68000000.\n",
      "iteration 67 querying from 67000000 to 68000000. query took 8.21s, overall took 9.08s\n",
      "iteration 68 querying from 68000000 to 69000000.\n",
      "iteration 68 querying from 68000000 to 69000000. query took 6.36s, overall took 6.38s\n",
      "iteration 69 querying from 69000000 to 70000000.\n",
      "iteration 69 from 69000000 to 70000000.... no results returned. finished?\n"
     ]
    }
   ],
   "source": [
    "output_file_root = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/tokens/uniref100_e_tokens_20240808_ALL\"\n",
    "\n",
    "# This is a vastly improved method of joining the two tables of proteins and tokens to give a line per token\n",
    "# This takes about 14s to process 1M proteins, find its tokens in W2V_TOKEN and output meta data for each entry to a file\n",
    "# The main difference is, by having a counter column on the protein it is a much more efficient way of moving through\n",
    "# that table in 'chunks' because you can query directly upon the 'COUNTER\" column with a > and <= query\n",
    "# BY contrast, using OFFSET and LIMIT apears to load up to the limit each time (or something) so each subsequent\n",
    "# query gets slower and slower as thee LIMIT increases.\n",
    "def extract_eukaryotic_tokens(start_pos, end_pos, iteration):\n",
    "\n",
    "    # time check\n",
    "    s = time.time()\n",
    "    \n",
    "    # output file\n",
    "    output_file = output_file_root+ str(iteration) + \".dat\"\n",
    "    \n",
    "    # create long life/expensive objects\n",
    "    of  = open(output_file, \"w\")\n",
    "    con = duckdb.connect(database=db_string)\n",
    "    \n",
    "    print(f\"iteration {iteration} querying from {start_pos} to {end_pos}.\")\n",
    "    \n",
    "    try:\n",
    "        results = con.execute(f\"SELECT W2V_PROTEIN_UREF100_E.UNIPROT_ID, W2V_PROTEIN_UREF100_E.LENGTH, W2V_TOKEN.TYPE, W2V_TOKEN.TOKEN, W2V_TOKEN.START, W2V_TOKEN.END FROM ( SELECT UNIPROT_ID, LENGTH FROM W2V_PROTEIN_UREF100_E WHERE COUNTER >= {start_pos} and COUNTER < {end_pos} ) AS W2V_PROTEIN_UREF100_E INNER JOIN W2V_TOKEN AS W2V_TOKEN ON W2V_PROTEIN_UREF100_E.UNIPROT_ID = W2V_TOKEN.UNIPROT_ID ORDER BY W2V_PROTEIN_UREF100_E.UNIPROT_ID\").fetchall()\n",
    "    except Exception as e:\n",
    "        print(f\"Error on iteration {iteration}, {e}, closing file {output_file}\")\n",
    "        of.close()\n",
    "        con.close()\n",
    "        return False\n",
    "    e1 = time.time()\n",
    "\n",
    "    #print(f\"{len(results)} results returned\")\n",
    "    if (len(results) == 0):\n",
    "        print(f\"iteration {iteration} from {start_pos} to {end_pos}.... no results returned. finished?\")\n",
    "        of.close()\n",
    "        con.close()\n",
    "        return False\n",
    "    \n",
    "    # write out the results\n",
    "    for res in results:\n",
    "        #print(res[0], res[1], res[2], res[3], res[4], res[5])\n",
    "        buffer = \"|\".join([res[0], str(res[1]), res[2], res[3], str(res[4]), str(res[5])])\n",
    "        #print(buffer)\n",
    "        of.write(buffer +'\\n')        \n",
    "    \n",
    "    # time check\n",
    "    e2 = time.time()\n",
    "    print(f\"iteration {iteration} querying from {start_pos} to {end_pos}. query took {round(e1-s,2)}s, overall took {round(e2-s,2)}s\")\n",
    "\n",
    "    of.close()\n",
    "    con.close()\n",
    "    return True\n",
    "\n",
    "\n",
    "# Macbook timings:\n",
    "# chunk size 1000   : iteration 0 querying from 0 to 1000. query took 5.04s, overall took 5.05s\n",
    "# chunk size 10000  : iteration 0 querying from 0 to 10000. query took 4.82s, overall took 4.84s\n",
    "# chunk size 100000 : iteration 0 querying from 0 to 100000. query took 6.06s, overall took 6.2s\n",
    "# chunk size 500000 : iteration 0 querying from 0 to 500000. query took 8.86s, overall took 9.74s\n",
    "# chunk size 500000 : iteration 0 querying from 0 to 1000000. query took 13.04s, overall took 14.71s\n",
    "start_pos       = 0    # start point\n",
    "chunk_size      = 10    # how many rows to return\n",
    "end_pos         = chunk_size\n",
    "keep_iterating  = True\n",
    "iteration       = 0\n",
    "\n",
    "# loop through proteins\n",
    "while keep_iterating and iteration <= 2 :\n",
    "    keep_iterating = extract_eukaryotic_tokens(start_pos, end_pos, iteration)\n",
    "    start_pos += chunk_size\n",
    "    end_pos += chunk_size\n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION - TOKEN METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Creates a new table with metadata for eukaryotic proteins - it takes 13s to load the data (29min 48s on pandas)\n",
    "# - Need to first get the metadata from the tokens file - see awk script below\n",
    "#\n",
    "# awk 'BEGIN {FS=\"|\"} {print $1}' tokens_combined/uniref100_e_tokens_20240808_ALL_COMBINED.dat > tokens_combined/metadata_uniref100_e_tokens_20240808_ALL_COMBINED_2.dat\n",
    "#\n",
    "#\n",
    "con = duckdb.connect(database=db_string)\n",
    "\n",
    "# note when yo specify the filename with a parameter within f-strings f\"...\" it can screw up the import\n",
    "file = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/tokens_combined/metadata_uniref100_e_tokens_20240808_ALL_COMBINED.dat\"\n",
    "\n",
    "# use filename directly - not as a varialbe\n",
    "con.execute(\"CREATE TABLE W2V_SENTENCE_METADATA_E AS SELECT * FROM read_csv('/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/tokens_combined/metadata_uniref100_e_tokens_20240808_ALL_COMBINED.dat', delim=':', header='false', columns={'UNIPROT_ID': 'VARCHAR', 'LENGTH': 'BIGINT', 'NUM_TOKENS': 'BIGINT', 'NUM_PF_TOKENS': 'BIGINT', 'NUM_DIS_TOKENS': 'BIGINT'})\")\n",
    "\n",
    "\n",
    "# the above isn;t working so just do it manually - but you get crap column names\n",
    "#con.execute(f\"CREATE TABLE W2V_SENTENCE_METADATA_E AS SELECT * FROM read_csv_auto('{file}', delim=':', header='false')\")\n",
    "\n",
    "# this outputs how duckdb will interpret the columns\n",
    "'''\n",
    "results = con.execute(f\"SELECT Prompt FROM sniff_csv('{file}', delim=':' )\").fetchall()\n",
    "for res in results:\n",
    "    print(res[0])\n",
    "'''\n",
    "\n",
    "con.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('UNIPROT_ID', 'VARCHAR', 'YES', None, None, None), ('LENGTH', 'BIGINT', 'YES', None, None, None), ('NUM_TOKENS', 'BIGINT', 'YES', None, None, None), ('NUM_PF_TOKENS', 'BIGINT', 'YES', None, None, None), ('NUM_DIS_TOKENS', 'BIGINT', 'YES', None, None, None)]\n",
      "Num entries: [(50249678,)]\n",
      "Num entries with >1 token: [(26463970,)]\n",
      "Num entries with >=1 pfam token: [(45909435,)]\n",
      "Num entries with >=2 pfam token: [(18645741,)]\n",
      "Num entries with >=3 pfam token: [(8451525,)]\n",
      "Num entries with >=4 pfam token: [(4631406,)]\n",
      "Num entries with >=5 pfam token: [(2915334,)]\n",
      "Num entries with >=10 pfam token: [(557233,)]\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=db_string)\n",
    "print(con.execute(\"DESCRIBE W2V_SENTENCE_METADATA_E\").fetchall())\n",
    "print('Num entries:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E\").fetchall())\n",
    "print('Num entries with >1 token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_TOKENS > 1\").fetchall())\n",
    "print('Num entries with >=1 pfam token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_PF_TOKENS >= 1\").fetchall())\n",
    "print('Num entries with >=2 pfam token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_PF_TOKENS >= 2\").fetchall())\n",
    "print('Num entries with >=3 pfam token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_PF_TOKENS >= 3\").fetchall())\n",
    "print('Num entries with >=4 pfam token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_PF_TOKENS >= 4\").fetchall())\n",
    "print('Num entries with >=5 pfam token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_PF_TOKENS >= 5\").fetchall())\n",
    "print('Num entries with >=10 pfam token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_PF_TOKENS >= 10\").fetchall())\n",
    "#print(con.execute(\"DROP TABLE W2V_SENTENCE_METADATA_E_2\").fetchall())\n",
    "#print(con.execute(\"DROP TABLE W2V_SENTENCE_METADATA_E\").fetchall())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num entries with >=1 pfam token: [(45909435,)]\n",
      "Num entries with >=5 pfam token: [(2915334,)]\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=db_string)\n",
    "print('Num entries with >=1 pfam token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_PF_TOKENS >= 1\").fetchall())\n",
    "print('Num entries with >=5 pfam token:', con.execute(\"SELECT COUNT(*) FROM W2V_SENTENCE_METADATA_E WHERE NUM_PF_TOKENS >= 5\").fetchall())\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILITIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for PFAM and PROTEIN ENTRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V_PROTEIN [('A0A010PZJ8', 1, 494)]\n",
      "W2V_PROTEIN_UREF100_E [('UniRef100', 'A0A010PZJ8', 493, 1, 494, 1, 1445577, 'Colletotrichum fioriniae PJ7')]\n"
     ]
    }
   ],
   "source": [
    "# test that W2V_TOKEN has all pfam and disorder entries\n",
    "# 1445577   : Colletotrichum fioriniae PJ7\n",
    "# 10116     : Rattus norvegicus\n",
    "con = duckdb.connect(database=db_string)\n",
    "\n",
    "# 1. Test - find a protein with pfam entries\n",
    "#    - Both of these work\n",
    "#protein_id = \"A0A009GYB3\" # this is prob not eukaryotic\n",
    "protein_id = \"A0A010PZJ8\"\n",
    "\n",
    "#tokens = con.execute(\"SELECT * FROM W2V_TOKEN WHERE UNIPROT_ID = 'A0A009GYB3'\").fetchall()\n",
    "#tokens = con.execute(\"SELECT * FROM W2V_TOKEN WHERE UNIPROT_ID = (?)\", [protein_id] ).fetchall()\n",
    "\n",
    "# 2. Find that same protein in W2V_PROTEIN\n",
    "# doesn't work - possibly because the pfam entries are from all proteins whereas W2V_PROTEIN only\n",
    "# has TrEMBL Eukaryotic proteins\n",
    "#tokens = con.execute(\"SELECT * FROM W2V_PROTEIN WHERE UNIPROT_ID = 'A0A009GYB3'\").fetchall()\n",
    "tokens = con.execute(\"SELECT * FROM W2V_PROTEIN WHERE UNIPROT_ID = (?)\", [protein_id]).fetchall()\n",
    "print('W2V_PROTEIN', tokens)\n",
    "\n",
    "# doesn't work\n",
    "#tokens = con.execute(\"SELECT * FROM W2V_TOKEN WHERE UNIPROT_ID = (?)\", ['protein_id']).fetchall()\n",
    "\n",
    "# none of these work - is the protein A0A009GYB3 in UniRef??\n",
    "# tokens = con.execute(\"SELECT * FROM W2V_PROTEIN_UNIREF_100_ALL_TAX WHERE UNIPROT_ID = 'A0A009GYB3'\").fetchall()\n",
    "tokens = con.execute(\"SELECT * FROM W2V_PROTEIN_UREF100_E WHERE UNIPROT_ID = (?)\", [protein_id]).fetchall()\n",
    "# tokens = con.execute(\"SELECT * FROM W2V_PROTEIN_UNIREF_100_ALL_TAX WHERE UNIPROT_ID = (?)\", [protein_id]).fetchall()\n",
    "# grep \"A0A009GYB3\" uniref100_tax_20240801.dat > returns nothing\n",
    "\n",
    "print('W2V_PROTEIN_UREF100_E', tokens)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('E', '710243', '1445577')]\n",
      "[('1445577', 'Colletotrichum fioriniae PJ7')]\n"
     ]
    }
   ],
   "source": [
    "# test that W2V_TOKEN has all pfam and disorder entries\n",
    "# 1445577   : Colletotrichum fioriniae PJ7\n",
    "# 10116     : Rattus norvegicus\n",
    "con = duckdb.connect(database=db_string)           \n",
    "tokens = con.execute(\"SELECT * FROM W2V_TAX_CAT WHERE ID=(?)\", ['1445577']).fetchall()\n",
    "print(tokens)\n",
    "tokens = con.execute(\"SELECT * FROM W2V_TAX_NAME WHERE TAX_ID=(?)\", ['1445577']).fetchall()\n",
    "print(tokens)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=db_string)           \n",
    "con.execute(\"DROP TABLE X\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('W2V_PFAM_E',), ('W2V_PROTEIN',), ('W2V_PROTEIN_UNIREF_100_ALL_TAX',), ('W2V_PROTEIN_UREF100_E',), ('W2V_TAX_CAT',), ('W2V_TAX_NAME',), ('W2V_TOKEN',)]\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=db_string)           \n",
    "tables = con.execute(\"SHOW TABLES\").fetchall()\n",
    "print(tables)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('uniprot_id', 'VARCHAR', 'YES', None, None, None), ('type', 'VARCHAR', 'YES', None, None, None), ('token', 'VARCHAR', 'YES', None, None, None), ('start', 'USMALLINT', 'YES', None, None, None), ('end', 'USMALLINT', 'YES', None, None, None)]\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect(database=db_string)           \n",
    "tables = con.execute(\"DESCRIBE W2V_TOKEN\").fetchall()\n",
    "print(tables)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlock database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# this doesn;t seem to work....\n",
    "'''\n",
    "def is_locked():\n",
    "    lock_file = f'{db_string}.lock'\n",
    "    return os.path.exists(lock_file)\n",
    "is_locked()\n",
    "'''\n",
    "\n",
    "# This works - execute from a command prompt then kill -9 <id if there is one list>\n",
    "fuser /Users/patrick/dev/ucl/comp0158_mscproject/database/w2v_20240731_test.db\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
