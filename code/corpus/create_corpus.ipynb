{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds overlapping regions - but assumes each token is in order\n",
    "# thus the start of token 2 will always be after the start of token 1\n",
    "def find_overlaps(tokens):\n",
    "    # List to store overlapping intervals\n",
    "    overlaps = []\n",
    "    \n",
    "    # Iterate through the sorted list and check for overlaps\n",
    "    for i in range(1, len(tokens)):\n",
    "        i1, w1, s1, e1 = tokens[i - 1]\n",
    "        i2, w2, s2, e2 = tokens[i]\n",
    "        \n",
    "        # scenario 1 : start of tokem 2 is before the end of token 1\n",
    "        if s2 <= e1:\n",
    "            overlaps.append((tokens[i - 1], tokens[i]))\n",
    "        # scenario 2 : start 1 is before the end of token 1\n",
    "        #if s2 <= e1:\n",
    "        #    overlaps.append((tokens[i - 1], tokens[i]))\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "# finds overlapping regions - and removes them, assumes the tokens\n",
    "# are in order  - thus the start of token 2 will always be after the start of token 1\n",
    "def remove_overlaps(tokens):\n",
    "    # List to store overlapping intervals\n",
    "    result = []\n",
    "    prev_start, prev_end = None, None\n",
    "    \n",
    "    for token in tokens:\n",
    "        start, end = token[2], token[3]\n",
    "        \n",
    "        # start with first token\n",
    "        if prev_start is None:\n",
    "            prev_start, prev_end = start, end\n",
    "            result.append(token)\n",
    "        # if there is overlap - don't add this item (for now)    \n",
    "        else:\n",
    "            if start <= prev_end:\n",
    "                #print('--------------> overlap : ', start , end)\n",
    "                # Overlapping interval found, skip adding this item\n",
    "                continue\n",
    "            else:\n",
    "                # No overlap, add the item to the result list\n",
    "                result.append(token)\n",
    "                prev_start, prev_end = start, end\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/kp/bqnb4b7n4ng50xtbznpbx7xh0000gn/T/ipykernel_37216/3694533337.py:34: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  col = col.rstrip(\"\\n\\s\\t\")\n"
     ]
    }
   ],
   "source": [
    "def create_corpus():\n",
    "    input_file      = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/pre_corpus_20240715_1130.dat\"\n",
    "    corpus = []\n",
    "    \n",
    "    PARSE_LIMIT  = 1000000  # number of lines to parse\n",
    "    DEBUG_LIMIT  = 100000   # number of lines after which to print a debug message\n",
    "    \n",
    "    start_time      = time.time()\n",
    "    mid_time_start  = time.time()\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    # parse all lines in the pre-corpus (each represents a protein) and build up\n",
    "    # the tokens for each - reminv overlaps\n",
    "    with open(input_file, 'r') as input:\n",
    "        for line_number, line in enumerate(input): # one line number per protein\n",
    "            \n",
    "            # protein details, pfam tokens and disordered tokens are separated by |\n",
    "            # within that are the start and end poisitions\n",
    "            # e.g. A0A010PZP8:1:633|PF00172:16:53|PF04082:216:322|DISORDER:50:103:50:109:553:598\n",
    "            # 1 protein, 2 pfam tokens, 3 disordered regions\n",
    "            \n",
    "            cols  = line.split('|')\n",
    "            token_idx = 0\n",
    "            \n",
    "            if(debug): print('\\nline >', line.strip('\\n'), '<')\n",
    "            #print(len(cols), '> entries')\n",
    "            \n",
    "            # tokens for the current line\n",
    "            tokens = []\n",
    "\n",
    "            # each col is a section - either being the uniptor part, pfam or disoreded reginos\n",
    "            for col in cols:\n",
    "                col = col.rstrip(\"\\n\\s\\t\")\n",
    "                # just in case\n",
    "                if col == None or col == \"\":\n",
    "                    continue\n",
    "                # process a PFAM token\n",
    "                if col.startswith('PF'):\n",
    "                    pf_cols = col.split(':')\n",
    "                    pf_token = pf_cols[0]\n",
    "                    for pf in range(1, len(pf_cols)-1,2):\n",
    "                        #print('PFM:', token_idx, ':', pf_token, 'start:', pf_cols[pf],'end:', pf_cols[pf + 1])\n",
    "                        tuple = (token_idx, pf_token, int(pf_cols[pf]), int(pf_cols[pf + 1]))\n",
    "                        tokens.append(tuple)\n",
    "                        token_idx += 1\n",
    "                # process a 'disordered' token\n",
    "                elif col.startswith('DIS'):\n",
    "                    dis_cols = col.split(':')\n",
    "                    for dis in range(1, len(dis_cols)-1,2):\n",
    "                        #print('DIS:', token_idx, ': start:', dis_cols[dis],'end:', dis_cols[dis + 1])\n",
    "                        tuple = (token_idx, 'DISORDER', int(dis_cols[dis]), int(dis_cols[dis+1]))\n",
    "                        tokens.append(tuple)\n",
    "                        token_idx += 1\n",
    "                # just printing out the token if needed\n",
    "                else:\n",
    "                    protein_cols = col.split(':')\n",
    "                    #print('PROT:', protein_cols[0], 'start:', protein_cols[1], 'end:', protein_cols[2])\n",
    "            #print('tokens:', tokens)\n",
    "            \n",
    "            # sort the tokens by start point (second item)\n",
    "            sorted_tokens = sorted(tokens, key=lambda x: x[2])\n",
    "            sorted_tokens_no_overlap = remove_overlaps(sorted_tokens)\n",
    "            \n",
    "            if(debug): \n",
    "                print('unsorted', tokens)\n",
    "                print('sorted:', sorted_tokens)\n",
    "                print('no overlaps',sorted_tokens_no_overlap)\n",
    "            \n",
    "            sentence = []\n",
    "            for token in sorted_tokens_no_overlap:\n",
    "                sentence.append(token[1])\n",
    "                sentence.append('GAP')\n",
    "            if(debug): print('final sentence:', sentence)\n",
    "            \n",
    "            # add to corpus\n",
    "            if(len(sentence) != 0):\n",
    "                corpus.append(sentence)\n",
    "            \n",
    "            # this just prints a progress message\n",
    "            if (line_number % DEBUG_LIMIT == 0):\n",
    "                mid_time_end = time.time()\n",
    "                exec_time = mid_time_end - mid_time_start\n",
    "                mid_time_start = mid_time_end\n",
    "                print(line_number, 'lines processed in', round(mid_time_end - start_time,2))\n",
    "            \n",
    "            # drops out if we only want to process a number of files\n",
    "            if(PARSE_LIMIT != -1):            \n",
    "                if(line_number == PARSE_LIMIT):\n",
    "                    end_time = time.time()\n",
    "                    tot_time = end_time - start_time\n",
    "                    print(PARSE_LIMIT, 'lines processed, terminating....')\n",
    "                    return corpus\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lines processed in 0.0\n",
      "100000 lines processed in 0.37\n",
      "200000 lines processed in 0.84\n",
      "300000 lines processed in 1.28\n",
      "400000 lines processed in 1.81\n",
      "500000 lines processed in 2.25\n",
      "600000 lines processed in 2.82\n",
      "700000 lines processed in 3.33\n",
      "800000 lines processed in 3.77\n",
      "900000 lines processed in 4.3\n",
      "1000000 lines processed in 4.75\n",
      "1000000 lines processed, terminating....\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "corpus = create_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n***** CORPUS *****:\\n\",corpus,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(corpus, vector_size=100, window=5, workers=4, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DISORDER\" VECTOR:\n",
      " [-0.35966924  0.02456524  0.05076128 -0.14754885 -0.0668508  -0.3754027\n",
      " -0.04968289  0.09947164 -0.03055772 -0.10764594  0.71828926  0.21175925\n",
      " -0.15042204 -0.19736947 -0.2784051   0.5424766   0.18962252 -0.43741164\n",
      " -0.18036567 -0.58696336  0.6699122   0.31809402  1.0590287  -0.11786786\n",
      "  0.34235743 -0.5437084   0.3865804   0.05919965 -0.8444295   0.0104759\n",
      "  0.10678057  0.19904487  0.885017   -0.6592032  -0.3831204  -0.19140661\n",
      "  0.05293044  0.23828791  0.21361044  0.40122643 -0.32491246  0.4420818\n",
      " -0.4419638  -0.559211   -0.16986938  0.62000066 -0.4115376   0.9462987\n",
      "  0.18533881  0.12894966  0.19906375  0.11780615 -0.35628316  0.10660051\n",
      "  0.3221957  -0.12932001  0.5308065   0.12141352 -0.5506251   0.2961019\n",
      " -0.12147589  0.15391962 -0.3452807  -0.5733853   0.35994604  0.33842188\n",
      " -0.00676064 -0.46636474 -0.00536083  0.52723366 -0.25583145 -0.1761985\n",
      " -0.33719715 -0.6141483  -0.23763172 -0.5642704   0.50907147 -0.4869721\n",
      "  0.4697093   0.47759333  0.11578534  0.45961398  0.02056129 -0.47026476\n",
      "  0.11452518 -0.33543286 -0.3379694   0.33039346  0.03558351  0.5809096\n",
      "  0.225088    0.3653071   0.20795177  0.3297249   0.5500428   0.12035164\n",
      " -0.6150775  -0.2349158  -0.21846241  0.27396172]\n",
      "\n",
      "\"PF00250\" VECTOR:\n",
      " [-0.38461676  0.17992614  0.11864605  0.24095753 -0.416792   -0.6036851\n",
      "  0.08759924  0.34615722 -0.45219874 -0.48634395  0.3854608  -0.00176551\n",
      " -0.63984436 -0.17319295 -0.5026368   0.40504643 -0.043449   -0.26772252\n",
      " -0.12548295 -0.30473363  0.14943188  0.21067098  0.34490407 -0.32679287\n",
      "  0.30095318 -0.25273508 -0.08073886  0.36855406 -0.15199815 -0.01513056\n",
      "  0.15230998 -0.05472877  0.60256976 -0.37758914 -0.47317454  0.3020181\n",
      "  0.31295314 -0.17170957  0.03725837  0.00370781 -0.00643671  0.34419748\n",
      " -0.48727277 -0.03202083  0.38048425  0.5868863  -0.22201326  0.5011793\n",
      " -0.20297799  0.2584231   0.3758504   0.53450817 -0.22207081  0.12505186\n",
      "  0.09976299 -0.43954873  0.37898308  0.18745272 -0.25626007  0.23536363\n",
      " -0.05885248  0.24640186 -0.53581244 -0.11831608  0.83834016  0.0527821\n",
      "  0.43006203  0.11013599  0.01994701  0.36378336 -0.4579728   0.22261703\n",
      "  0.03697691 -0.6180526  -0.12407731 -0.03515897  0.0849094  -0.3994386\n",
      "  0.9533653  -0.13685735 -0.01057588  0.22998743  0.14951573 -0.2449658\n",
      "  0.25642246 -0.3426414  -0.03924359 -0.27198717  0.1537949   0.3305254\n",
      "  0.27267298 -0.0385031   0.5236608  -0.33881587  0.62165296  0.29279968\n",
      " -0.7109257   0.02063636  0.08606474  0.23214124]\n"
     ]
    }
   ],
   "source": [
    "#words = list(w2v.wv.vocab)\n",
    "print('\"DISORDER\" VECTOR:\\n', w2v.wv['DISORDER'])\n",
    "print('\\n\"PF00250\" VECTOR:\\n', w2v.wv['PF00250'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
