import re
import csv
import time
import os

#
# Combines individual lines from a file of tokens (generated by extract_e_protein_tokens.py) into a single line
# per protein - eah line containing information about the tokens for that protein
# 
# Cleaned up and retested as part of code cleanup in Sept
#

''' sample input
UNIPROT LENGTH TYPE TOKEN TOKEN START TOKEN END
A0A010PZP8|632|DISORDER|Polar|50|103
A0A010PZP8|632|DISORDER|Consensus Disorder Prediction|50|109
A0A010PZP8|632|DISORDER|Consensus Disorder Prediction|553|598
A0A010PZP8|632|PFAM|PF00172|16|53
A0A010PZP8|632|PFAM|PF04082|216|322

A0A010PZK3|512|DISORDER|Consensus Disorder Prediction|414|512
A0A010PZK3|512|DISORDER|Polar|417|433
A0A010PZK3|512|DISORDER|Polar|445|462
A0A010PZK3|512|DISORDER|Polar|491|505
A0A010PZK3|512|PFAM|PF00722|58|224
A0A010PZK7|664|PFAM|PF14033|123|575
'''


''' sample output
protein_id:start:end:num tokens:numpfam tokens : num disorder tokens | disorder or pfam entries with start and end point
A0A010PZP8:1:633:5:2:3|DISORDER:50:103|DISORDER:50:109|DISORDER:553:598|PF00172:16:53|PF04082:216:322
'''


#
# CHANGE THESE VALUES ACCORDINGLY
#
input_file       = "/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/corpus_validation_sep/uniref100_e_tokens_inner_join_20240910.dat"
output_file      = "/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/corpus_validation_sep/uniref100_e_tokens_combined_20240910.dat"

#
# Combines pfam and disorder tokens into a single line per protein
#
def combine_tokens(input_file, output_file):
    
    print(f"\n --------- TOKEN COMBINATION (of disorder and pfam entries per protein) -------- \n")
    
    print(f" - Combining tokens in {input_file}, output to {output_file}\n")
    
    line_limit = -1 # set to non negative number to limit output
    s           = time.time()
    
    last_protein    = "start"
    current_protein = ""
    protein_buffer  = ""

    protein_disorder_count = 0
    protein_pfam_count = 0
    
    of      = open(output_file, "w")
    
    lines_processed = 0
    output_lines    = 0
    with open(input_file, 'r') as file:
        for line_number, line in enumerate(file):
            
            if(line_limit != -1 and lines_processed >= line_limit):
                print(f"\n - Limit reached : {lines_processed} token entries processed. {output_lines} combined protein sentences written to {output_file}\n")
                return
            
            # split the input line and extract relevant details
            cols = line.split('|')
            if(len(cols) > 1):

                protein_len     = cols[1].strip('\n')
                current_protein = cols[2].strip('\n')

                token       = cols[3].strip('\n')
                token_type  = cols[4].strip('\n')
                
                token_start = cols[5].strip('\n')
                token_end   = cols[6].strip('\n')
                
                # if the curent result line is for a new protein (ie the protein id at the start of the output has changed)
                if (last_protein == "start" or current_protein != last_protein ):

                    # if we have a new protein thats not the start, output the buffer
                    if(last_protein != "start"):
                        combined_line = protein_start_buffer + ':' + str(protein_pfam_count + protein_disorder_count) + ':' + str(protein_pfam_count) + ':' + str(protein_disorder_count) + protein_buffer
                        
                        of.write(combined_line +'\n')
                        #print(combined_line)
                        
                        output_lines += 1
                        
                        # reset
                        protein_buffer          = ""
                        protein_pfam_count      = 0
                        protein_disorder_count  = 0
                    
                    # otherwise, add to current buffer
                    last_protein            = current_protein
                    protein_start_buffer    = ':'.join([current_protein, protein_len])
                
                # for a disorder token
                if (token_type == "DISORDER"):
                    protein_buffer = protein_buffer + '|' + token_type + ':' + token_start + ':' + token_end
                    protein_disorder_count += 1
                
                # for a pfam token
                elif (token_type == "PFAM"):
                    protein_buffer = protein_buffer + '|' + token + ':' + token_start + ':' + token_end
                    protein_pfam_count += 1
            lines_processed += 1
    e = time.time()
    print(f"{lines_processed} token entries processed. {output_lines} combined protein sentences created in {round(e-s,2)}s. output to {output_file}")


# run
combine_tokens(input_file, output_file)
