{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from skbio.stats.distance import mantel\n",
    "from skbio.stats.distance import DistanceMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL CREATION\n",
    "This is a test are for model creation, the final code is in w2v_batch.py and run_w2v_00.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# parses a dicorpus file to build up sentences to create a model\n",
    "#\n",
    "def get_corpus_sentences(corpus_file):\n",
    "    # initialise\n",
    "    s = time.time()\n",
    "    sentences = []\n",
    "    counter = 0\n",
    "    num_tokens = 0\n",
    "    print(f'Parsing file for sentences: {corpus_file}')\n",
    "    with open(corpus_file, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n')\n",
    "            tokens = line.split()\n",
    "            sentences.append(tokens)\n",
    "            counter +=1\n",
    "            num_tokens += len(tokens)\n",
    "    # time check\n",
    "    e = time.time()\n",
    "    print(f\"{counter} sentences processed, {num_tokens} added in {e - s}s\" )\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create W2V model\n",
    "#\n",
    "def create_w2v(sentences, model_name, vector_size, window_size, mc):\n",
    "    current_date    = datetime.now().strftime('%Y%m%d')\n",
    "    model_name      = model_name + \".model\"\n",
    "    \n",
    "    s = time.time()\n",
    "    \n",
    "    # create model from sentences       \n",
    "    w2v = Word2Vec(sentences, vector_size=vector_size, window=window_size, workers=4, epochs=10, min_count=mc)\n",
    "    \n",
    "    # time check\n",
    "    e = time.time()\n",
    "    print(f\"{model_name} | {vector_size} | {window_size} | {mc} | {round(e-s,2)}s\")\n",
    "          \n",
    "    # save model      \n",
    "    w2v.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##corpus_file     = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/uniref100_e_corpus_20240810.txt\"\n",
    "#sentences       = get_corpus_sentences(corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "model_dir       = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/\"\n",
    "current_date    = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "# create models\n",
    "'''\n",
    "for vector_size in range(5, 10, 5):\n",
    "    for window_size in range(5,10, 5):\n",
    "        # create model\n",
    "        model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)\n",
    "        create_w2v(sentences, model_name, vector_size, window )\n",
    "'''\n",
    "\n",
    "'''\n",
    "corpus_file     = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/uniref100_e_corpus_20240810.txt\"\n",
    "sentences       = get_corpus_sentences(corpus_file)\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 5\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 5\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 5\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 5\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "\n",
    "vector_size = 20\n",
    "window_size = 5\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "\n",
    "vector_size = 20\n",
    "window_size = 5\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 10\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 10\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 20\n",
    "window_size = 10\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 10\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 10\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 20\n",
    "window_size = 10\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "'''\n",
    "\n",
    "corpus_file     = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/uniref100_e_corpus_20240810.txt\"\n",
    "sentences       = get_corpus_sentences(corpus_file)\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 5\n",
    "min_count   = 1\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 5\n",
    "min_count   = 1\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab(model_dir, model_name, vocab_dir):\n",
    "    \n",
    "    base_name_search  = re.search(\"(w2v_.*)\\.model\", model_name)\n",
    "    base_name         = base_name_search.group(1)\n",
    "\n",
    "    vocab_file  = vocab_dir+base_name+'_vocab.txt'\n",
    "    model       = Word2Vec.load(model_dir+model_name)\n",
    "    \n",
    "    print(f\"Extracting vocab for model {base_name} to {vocab_file}.\")\n",
    "\n",
    "    of = open(vocab_file, \"w\")\n",
    "    for word in model.wv.key_to_index:\n",
    "        of.write(word + '\\n')\n",
    "    of.close()\n",
    "\n",
    "# set directories\n",
    "vocab_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/\"\n",
    "\n",
    "# get vocab for a particular file\n",
    "#model_name = \"w2v_20240811_v5_w5_mc3.model\"\n",
    "#write_vocab(model_dir, model_name, vocab_dir)\n",
    "\n",
    "# find all model files\n",
    "file_list = glob.glob(os.path.join(model_dir, '*.model'))\n",
    "for file_path in file_list:\n",
    "    model_name_s  = re.search(\"(w2v_.*)\\.model\", file_path)\n",
    "    model_name         = model_name_s.group(1)\n",
    "    \n",
    "    print(f\"Getting vocab for {model_name}\")\n",
    "    #write_vocab(model_dir, model_name, vocab_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab for one model\n",
    "vocab_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/\"\n",
    "\n",
    "# get vocab for a particular file\n",
    "#model_name = \"w2v_20240811_v5_w5_mc3.model\"\n",
    "#write_vocab(model_dir, model_name, vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab for all models\n",
    "vocab_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/\"\n",
    "\n",
    "file_list = glob.glob(os.path.join(model_dir, '*.model'))\n",
    "for file_path in file_list:\n",
    "    model_name_s  = re.search(\"(w2v_.*\\.model)\", file_path)\n",
    "    model_name         = model_name_s.group(1)\n",
    "    write_vocab(model_dir, model_name, vocab_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Area for model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a pfam entry is in a model\n",
    "model = Word2Vec.load(\"/Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/mac/w2v_20240831_sg1_mc1_w3_v5_mac.model\")\n",
    "\n",
    "#pfam_id = 'PF19687'        # present in w2v_20240810_v5_w2.model\n",
    "#pfam_id = 'PF01257'        # not present in w2v_20240810_v5_w2.model - but shouldn't be\n",
    "#pfam_id = 'PF00424'        # not present in model - why not - only appears once in corpus\n",
    "pfam_id = 'PF14033'        # present in w2v_20240810_v5_w2.model\n",
    "pfam_id = 'PF00469'         # not present in model - why not - only appears once in corpus\n",
    "pfam_id = 'PF03945'\n",
    "\n",
    "try:\n",
    "    print(model.wv[pfam_id])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTANCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Symmetric Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([[0, 2, 3, 4, 5],\n",
    "                   [0, 0, 6, 7, 8],\n",
    "                   [0, 0, 0, 9, 10],\n",
    "                   [0, 0, 0, 0, 12],\n",
    "                   [0, 0, 0, 0, 0]])\n",
    "\n",
    "# Make the matrix symmetric\n",
    "symmetric_matrix = matrix + matrix.T - np.diag(matrix.diagonal())\n",
    "\n",
    "print('original matrix:\\n', matrix, '\\n')\n",
    "print('symmetric matrix:\\n', symmetric_matrix, '\\n')\n",
    "\n",
    "num_entries = matrix.shape[1]\n",
    "\n",
    "for i in range(num_entries):\n",
    "    for j in range(num_entries):\n",
    "        try:\n",
    "            assert (symmetric_matrix[i,j] == symmetric_matrix[j,i]), \"matrix not symmetric\"\n",
    "        except AssertionError as e:\n",
    "            print(f\"Assertion failed: {e} i: {i} j: {j} :: {symmetric_matrix[i,j]} != {symmetric_matrix[j,i]}\")\n",
    "\n",
    "# break something\n",
    "symmetric_matrix[1,3] = 11\n",
    "\n",
    "for i in range(num_entries):\n",
    "    for j in range(num_entries):\n",
    "        try:\n",
    "            assert (symmetric_matrix[i,j] == symmetric_matrix[j,i]), \"matrix not symmetric\"\n",
    "        except AssertionError as e:\n",
    "            print(f\"Assertion failed: {e} i: {i} j: {j} :: {symmetric_matrix[i,j]} != {symmetric_matrix[j,i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test reduce and reorder python matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ See verision in utilities sectoin which works with numpy vectors\n",
    "\n",
    "\n",
    "# Reduces the target matrix to only have entries that are common to \n",
    "# 2 vectors list the entries in 2 matrices. This routine identified the \n",
    "# common elements between these vectors and removes them from the target matrix\n",
    "# It also reorders the target matrix so that the entries are in the same order\n",
    "\n",
    "# source_vector : list of items in the source matrix\n",
    "# target_vector : list of items in the target matrix\n",
    "# target_matrix : the matrix to be reduced and reordered\n",
    "\n",
    "def reduce_python_matrix(source_vector, target_vector, target_matrix):\n",
    "\n",
    "    reorder_indices = []\n",
    "    # get index of each source vector in the target vector and add that index to a list\n",
    "    # thus this list is now in the same order as the source vector\n",
    "    for item in source_vector:\n",
    "        if (item == 'GAP' or item == 'START_GAP' or item == 'STOP_GAP' or item == 'DISORDER'):\n",
    "            continue\n",
    "        else:\n",
    "            index = np.where(target_vector == item)[0]\n",
    "            reorder_indices.append(index[0])\n",
    "    \n",
    "    reordered_vector = target_vector[reorder_indices]\n",
    "\n",
    "    print('target indices for items only in the source vector:', reorder_indices, '(should only have these rows and columns left).\\n')\n",
    "    print('target entries for items only in the source vector:', target_vector[reorder_indices], '\\n')\n",
    "\n",
    "    # take ony the rows from the target - but this will still have all columns\n",
    "    reordered_matrix = target_matrix[reorder_indices, :]\n",
    "    # now take only the columns\n",
    "    reordered_matrix = reordered_matrix[:, reorder_indices]\n",
    "\n",
    "    return reordered_vector, reordered_matrix\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "source_vector = np.array(['d', 'b'])\n",
    "target_vector = np.array(['a', 'b', 'c', 'd'])\n",
    "\n",
    "print(f\"source vector: {source_vector}\\ntarget vector: {target_vector}\\n\")\n",
    "print(f\"original target matrix: \\n{target_matrix}\\n\")\n",
    "\n",
    "target_matrix = np.array([  [10, 11, 12, 13],\n",
    "                            [20, 21, 22, 23],\n",
    "                            [30, 31, 32, 33],\n",
    "                            [40, 41, 42, 43]])\n",
    "\n",
    "\n",
    "reduced_target_vector, reduced_target_matrix = reduce_python_matrix(source_vector, target_vector, target_matrix)\n",
    "\n",
    "print('------------------- result -----------------\\n')\n",
    "print(f\"reduced target vector: \\n{reduced_target_vector}\")\n",
    "print(f\"reduced target matrix: \\n{reduced_target_matrix}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Reduces and reorders a target matrix so that it only has the items in source_v\n",
    "#\n",
    "# - DOES NOT ASSUME THAT ALL ITEMS IN SOURCE ARE IN TARGET\n",
    "#\n",
    "# source_v : list of items we want to have and in the order we need them\n",
    "# target_v : items that are currently in a target_matrix and the order they are in (assume same ordering rows and columns)\n",
    "# target_matrix : the matrix we want to remove ros/columns from and reorder to the same order as source_v\n",
    "# returns a new matrix which is the original target matrix but modified as decribed\n",
    "def reorder_matrix_2(source_v, target_v, source_matrix, target_matrix):\n",
    "\n",
    "    target_indices  = []\n",
    "    source_indices  = []\n",
    "    for i, item in enumerate (source_v):\n",
    "        if(item.startswith(\"PF\")):\n",
    "            target_index = np.where(target_v == item)[0]\n",
    "            #print(f\"target index for {item}: {target_index}\")\n",
    "            # if the source is in the target\n",
    "            if(target_index.size !=0):\n",
    "                target_indices.append(target_index[0])\n",
    "                source_indices.append(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #print('indices needed in target matrix and in new order', target_indices)\n",
    "    #print('indices needed in source vector', source_indices)\n",
    "\n",
    "    # reorder the matrix\n",
    "    reordered_t_matrix = target_matrix[target_indices, :]\n",
    "    reordered_t_matrix = reordered_t_matrix[:, target_indices]\n",
    "    \n",
    "    reordered_s_matrix = source_matrix[source_indices, :]\n",
    "    reordered_s_matrix = reordered_s_matrix[:, source_indices]\n",
    "    \n",
    "    reduced_source_v  = source_v[source_indices]\n",
    "    \n",
    "    return reordered_s_matrix, reordered_t_matrix\n",
    "\n",
    "\n",
    "\n",
    "source_vector = np.array(['d', 'b'])\n",
    "target_vector = np.array(['a', 'b', 'c', 'd'])\n",
    "\n",
    "target_matrix = np.array([  [10, 11, 12, 13],\n",
    "                            [20, 21, 22, 23],\n",
    "                            [30, 31, 32, 33],\n",
    "                            [40, 41, 42, 43]])\n",
    "\n",
    "reduced_source_matrix, reduced_target_matrix = reorder_matrix_2(source_vector, target_vector, target_matrix)\n",
    "\n",
    "print(f\"source vector: {source_vector}\\ntarget vector: {target_vector}\\n\")\n",
    "print(f\"original target matrix: \\n{target_matrix}\\n\")\n",
    "\n",
    "print(f\"reduced source matrix: \\n{reduced_source_matrix}\")\n",
    "print(f\"reduced target matrix: \\n{reduced_target_matrix}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTANCE UTILITIES - FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# get index of a pfam entry from the vector so we can look it up\n",
    "#\n",
    "def get_word_index(word, vocab_vector):\n",
    "    for i in range (len(vocab_vector)):\n",
    "        if vocab_vector[i] == word:\n",
    "            #print(f\"match for {word} at index {i}\")\n",
    "            return i\n",
    "\n",
    "# ------------------- SYMMETRIC MATRICES ----------------------------\n",
    "\n",
    "#\n",
    "# make a matrix symmetrical - my word2vec matrices are not\n",
    "#\n",
    "def make_symmetrical(matrix):\n",
    "    matrix_sym = matrix + matrix.T - np.diag(matrix.diagonal())\n",
    "    return matrix_sym\n",
    "\n",
    "#\n",
    "# test a matrix to see if its symmetric\n",
    "#\n",
    "def test_symmetric(symmetric_matrix):\n",
    "    print(symmetric_matrix.shape)\n",
    "    num_entries = symmetric_matrix.shape[1]\n",
    "    for i in range(num_entries):\n",
    "        for j in range(num_entries):\n",
    "            try:\n",
    "                assert (symmetric_matrix[i,j] == symmetric_matrix[j,i]), \"matrix not symmetric\"\n",
    "            except AssertionError as e:\n",
    "                print(f\"Assertion failed: {e} i: {i} j: {j} :: {symmetric_matrix[i,j]} != {symmetric_matrix[j,i]}\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- GET MATRICES AND VECTORS FROM NPY FILE ----------------------------\n",
    "\n",
    "#\n",
    "# extract vocab vector and matrix from npy file\n",
    "#                \n",
    "def extract_matrix_vector_files(npy_file_name):\n",
    "    npy_f              = open(npy_file_name, 'rb')\n",
    "    dist_matrix         = np.load(npy_f) #loads first array\n",
    "    vocab_vector        = np.load(npy_f)\n",
    "    \n",
    "    return vocab_vector, dist_matrix\n",
    "\n",
    "\n",
    "\n",
    "# ------------------- GET PFAM IDS FROM EVO VECTOR ----------------------------\n",
    "\n",
    "#\n",
    "# vectors in evo matrix have format K1SVA3.1/50-86|PF02829\n",
    "# need to extract these\n",
    "#\n",
    "def extract_evo_pfam_ids(evo_vector):\n",
    "    pfam_ids = []\n",
    "    for item in evo_vector:\n",
    "        #print(f\"searching in {item}\")\n",
    "        pfam_search  = re.search(\"\\|(PF.*)\", item)\n",
    "        pfam_id       = pfam_search.group(1)\n",
    "        #print(f\"found {pfam_id}\")\n",
    "        pfam_ids.append(pfam_id)\n",
    "    return pfam_ids\n",
    "\n",
    "    \n",
    "\n",
    "# ------------- REDUCE A MATRIX BASED UPON COMMON ENTRIES IN A VECTOR ----------\n",
    "\n",
    "# Reduces the target matrix to only have entries that are common to \n",
    "# 2 vectors list the entries in 2 matrices. This routine identified the \n",
    "# common elements between these vectors and removes them from the target matrix\n",
    "# It also reorders the target matrix so that the entries are in the same order\n",
    "\n",
    "# source_vector : list of items in the source matrix\n",
    "# target_vector : list of items in the target matrix\n",
    "# target_matrix : the matrix to be reduced and reordered\n",
    "\n",
    "def reduce_matrix(source_vector, target_vector, target_matrix):\n",
    "\n",
    "    reorder_indices = []\n",
    "    missing_items   = []\n",
    "    found_items     = []\n",
    "    # get index of each source vector in the target vector and add that index to a list\n",
    "    # thus this list is now in the same order as the source vector\n",
    "    for item in source_vector:\n",
    "        if (item == 'GAP' or item == 'START_GAP' or item == 'STOP_GAP' or item == 'DISORDER'):\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                find_index = np.where(target_vector == item)[0]\n",
    "                if len(find_index) > 0 :\n",
    "                    reorder_indices.append(find_index[0])\n",
    "                    found_items.append(item)\n",
    "                    #print('found item at', find_index[0])\n",
    "                else:\n",
    "                    #print(item, 'not found in target vector')\n",
    "                    missing_items.append(item)\n",
    "            except Exception as e:\n",
    "                print(f\"error looping through vector at {item}, {e}\")\n",
    "    \n",
    "    reordered_vector = target_vector[reorder_indices]\n",
    "\n",
    "    #print('target indices for items only in the source vector:', reorder_indices, '(should only have these rows and columns left).\\n')\n",
    "    #print('target entries for items only in the source vector:', target_vector[reorder_indices], '\\n')\n",
    "\n",
    "    # take ony the rows from the target - but this will still have all columns\n",
    "    reordered_matrix = target_matrix[reorder_indices, :]\n",
    "    # now take only the columns\n",
    "    reordered_matrix = reordered_matrix[:, reorder_indices]\n",
    "\n",
    "    return found_items, missing_items, reordered_vector, reordered_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Reduces and reorders a target matrix so that it only has the items in source_v\n",
    "#\n",
    "# - DOES NOT ASSUME THAT ALL ITEMS IN SOURCE ARE IN TARGET\n",
    "#\n",
    "# source_v : list of items we want to have and in the order we need them\n",
    "# target_v : items that are currently in a target_matrix and the order they are in (assume same ordering rows and columns)\n",
    "# target_matrix : the matrix we want to remove ros/columns from and reorder to the same order as source_v\n",
    "# returns a new matrix which is the original target matrix but modified as decribed\n",
    "def reorder_matrix_2(source_v, target_v, source_matrix, target_matrix):\n",
    "\n",
    "    target_indices  = []\n",
    "    source_indices  = []\n",
    "    for i, item in enumerate (source_v):\n",
    "        if(item.startswith(\"PF\")):\n",
    "            target_index = np.where(target_v == item)[0]\n",
    "            #print(f\"target index for {item}: {target_index}\")\n",
    "            # if the source is in the target\n",
    "            if(target_index.size !=0):\n",
    "                target_indices.append(target_index[0])\n",
    "                source_indices.append(i)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #print('indices needed in target matrix and in new order', target_indices)\n",
    "    #print('indices needed in source vector', source_indices)\n",
    "\n",
    "    # reorder the matrix\n",
    "    reordered_t_matrix = target_matrix[target_indices, :]\n",
    "    reordered_t_matrix = reordered_t_matrix[:, target_indices]\n",
    "    \n",
    "    reordered_s_matrix = source_matrix[source_indices, :]\n",
    "    reordered_s_matrix = reordered_s_matrix[:, source_indices]\n",
    "    \n",
    "    reduced_source_v  = source_v[source_indices]\n",
    "    \n",
    "    return reordered_s_matrix, reordered_t_matrix\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# performs mantel test on 2 matrices - in this case two matrices from word2vec\n",
    "#\n",
    "def custom_mantel_test(source_matrix, target_matrix, permutations=10):\n",
    "    \n",
    "    s = time.time()\n",
    "\n",
    "    \n",
    "    # Ensure that the matrices are square and of the same size\n",
    "    assert source_matrix.shape == target_matrix.shape, \"Matrices must have the same shape.\"\n",
    "    assert source_matrix.shape[0] == source_matrix.shape[1], \"Matrices must be square.\"\n",
    "    assert target_matrix.shape[0] == target_matrix.shape[1], \"Matrices must be square.\"\n",
    "\n",
    "    # Flatten the upper triangular parts of the matrices\n",
    "    print(\"flattening matrices......\")\n",
    "    triu_indices = np.triu_indices_from(source_matrix, k=1)\n",
    "    distances1 = source_matrix[triu_indices]\n",
    "    distances2 = target_matrix[triu_indices]\n",
    "\n",
    "    print(\"calculating pearson correlation....\")\n",
    "    # Calculate the observed Pearson correlation\n",
    "    observed_corr, _ = pearsonr(distances1, distances2)\n",
    "\n",
    "    print(\"running permutation tests....\")\n",
    "    # Permutation test\n",
    "    permuted_corrs = []\n",
    "    i = 0\n",
    "    for _ in range(permutations):\n",
    "        np.random.shuffle(distances2)\n",
    "        permuted_corr, _ = pearsonr(distances1, distances2)\n",
    "        permuted_corrs.append(permuted_corr)\n",
    "        print(f\" - permutation {i} complete\")\n",
    "        i+=1\n",
    "\n",
    "    # Calculate p-value\n",
    "    print(\"- calculating p-value....\")\n",
    "    permuted_corrs = np.array(permuted_corrs)\n",
    "    p_value = np.mean(permuted_corrs >= observed_corr)\n",
    "    \n",
    "    e = time.time()\n",
    "    \n",
    "    #print(f\"- test complete in {round(e-s,2)}s. correlation: {observed_corr}. p-value: {p_value}\")\n",
    "\n",
    "    return observed_corr, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_vector = np.array(['d', 'b', 'f'])\n",
    "target_vector = np.array(['a', 'b', 'c', 'd'])\n",
    "\n",
    "print(f\"source vector: {source_vector}\\ntarget vector: {target_vector}\\n\")\n",
    "print(f\"original target matrix: \\n{target_matrix}\\n\")\n",
    "\n",
    "target_matrix = np.array([  [10, 11, 12, 13],\n",
    "                            [20, 21, 22, 23],\n",
    "                            [30, 31, 32, 33],\n",
    "                            [40, 41, 42, 43]])\n",
    "\n",
    "#\n",
    "# -------- calll routine\n",
    "#\n",
    "# found_items, missing_items, reordered_vector, reordered_matrix\n",
    "found_items, missing_items, reduced_target_vector, reduced_target_matrix = reduce_matrix(source_vector, target_vector, target_matrix)\n",
    "\n",
    "print('------------------- result -----------------\\n')\n",
    "print(f\"missing items (in source, not in target): \\n{missing_items}\")\n",
    "print(f\"found items: \\n{found_items}\")\n",
    "print(f\"reduced target vector: \\n{reduced_target_vector}\")\n",
    "print(f\"reduced target matrix: \\n{reduced_target_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test extracting pfam ids from evo vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evo_distance_file = '/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/evo/rand_rep_distance_matrix.npy'\n",
    "\n",
    "evo_vector, evo_matrix = extract_matrix_vector_files(evo_distance_file)\n",
    "\n",
    "evo_pfam_ids = extract_evo_pfam_ids(evo_vector)\n",
    "\n",
    "print(evo_pfam_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load movel and vector of pfam words\n",
    "model = Word2Vec.load(\"/Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/mac/w2v_20240831_sg1_mc1_w3_v5_mac.model\")\n",
    "npy_distance_file = '/Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/mac/w2v_20240831_sg1_mc1_w3_v5.model_dist_normalised_mac.npy'\n",
    "\n",
    "# load the pre-calculated distance matrix and vectors\n",
    "vocab_vector, w2v_dist_matrix =  extract_matrix_vector_files(npy_distance_file)\n",
    "\n",
    "# Size of the vocabulary\n",
    "vocab_size = len(model.wv.key_to_index)\n",
    "\n",
    "# Get the vector rep of 2 pfam ids from a model\n",
    "pfam_1 = 'PF00469'\n",
    "pfam_2 = 'PF03945'\n",
    "gap = 'GAP'\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Calculate distances directly by retriveing model encoding\n",
    "# ---------------------------------------------------------------------\n",
    "print('\\ncalculating distance from scratch....')\n",
    "vector_1 = model.wv[pfam_1]\n",
    "vector_2 = model.wv[pfam_2]\n",
    "\n",
    "# calculate distance\n",
    "distance = np.linalg.norm(vector_1 - vector_2)\n",
    "\n",
    "print(f\"{pfam_1} : {vector_1} \\n{pfam_2} : {vector_2}\\nEuclidean distance: {distance}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Retrieve pre-calculated distances for the same words\n",
    "# ---------------------------------------------------------------------\n",
    "print('\\nretrieving pre-calculated distaances....')\n",
    "# find the indices in the vocab vector\n",
    "index_0 = get_word_index(gap, vocab_vector)\n",
    "index_1 = get_word_index(pfam_1, vocab_vector)\n",
    "index_2 = get_word_index(pfam_2, vocab_vector)\n",
    "\n",
    "print(vocab_vector[index_1])\n",
    "print(vocab_vector[index_2])\n",
    "\n",
    "# now check that the distance is the same\n",
    "print(f\"found {pfam_1} at {index_1} and {pfam_2} at {index_2}\")\n",
    "print(f\"distance diag: {w2v_dist_matrix[index_2, index_2]}\")\n",
    "\n",
    "# note because its diagonal..... rows are filled first, so if index 2 is greater than index 1 you need to reverse\n",
    "print(f\"normalised distance: {w2v_dist_matrix[index_1, index_2]}\")\n",
    "print(f\"normalised distance: {w2v_dist_matrix[index_2, index_1]}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Make w2v matrix symmetrical\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "#\n",
    "# Make it symmetrical\n",
    "#\n",
    "print('\\ncreating symmetric matrix....')\n",
    "s = time.time()\n",
    "w2v_dist_matrix_sym = make_symmetrical(w2v_dist_matrix)\n",
    "e = time.time()\n",
    "\n",
    "print(f\"symmetric matrix created in {round(e-s,2)}s\")\n",
    "print(f\"distance diag: {w2v_dist_matrix_sym[index_2, index_2]}\")\n",
    "\n",
    "# note because its diagonal..... rows are filled first, so if index 2 is greater than index 1 you need to reverse\n",
    "print(f\"normalised distance: {w2v_dist_matrix_sym[index_1, index_2]}\")\n",
    "print(f\"normalised distance: {w2v_dist_matrix_sym[index_2, index_1]}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Test symmetric\n",
    "# ---------------------------------------------------------------------\n",
    "'''\n",
    "print('\\ntesting that matrix is symmetric....')\n",
    "test_symmetric(w2v_dist_matrix_sym)\n",
    "print('\\ntest complete....')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARE DISTANCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract matrices and vectors and prepare for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolutionary distance matrix (matrix and vector)\n",
    "#evo_distance_file = '/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/evo/rand_rep_distance_matrix_small.npy'\n",
    "evo_distance_file = '/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/evo/rand_rep_distance_matrix.npy'\n",
    "\n",
    "# w2v distance matrix (matrix and vector)\n",
    "w2v_distance_file = '/Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/mac/w2v_20240831_sg1_mc1_w3_v5.model_dist_normalised_mac.npy'\n",
    "\n",
    "# extract files\n",
    "print('extracting matrices and vectors from .npy files')\n",
    "s = time.time()\n",
    "evo_vector, evo_matrix = extract_matrix_vector_files(evo_distance_file)\n",
    "w2v_vector, w2v_matrix = extract_matrix_vector_files(w2v_distance_file)\n",
    "e = time.time()\n",
    "print(f\"extracting files. time taken: {round(e-s,2)}s\\n\")\n",
    "\n",
    "\n",
    "# need  to make w2v matrix symmetrical and convert array into numpy array\n",
    "print('making w2v distance matrix symmetrical')\n",
    "s = time.time()\n",
    "w2v_matrix_sym = make_symmetrical(w2v_matrix)\n",
    "e = time.time()\n",
    "print(f\"make w2v symmetrical. time taken: {round(e-s,2)}s\\n\")\n",
    "\n",
    "# need to also extract pfam ids from the evo vector and convert to numpy array\n",
    "print('extracting pfam ids from evo vector into new np array')\n",
    "s = time.time()\n",
    "evo_pfam_ids  = extract_evo_pfam_ids(evo_vector)\n",
    "evo_vector_np = np.array(evo_pfam_ids)\n",
    "e = time.time()\n",
    "print(f\"extraction complete. time taken: {round(e-s,2)}s\\n\")\n",
    "\n",
    "# also convert w2v vector to numpy array\n",
    "w2v_vector_np = np.array(w2v_vector)\n",
    "\n",
    "print(f\"word2vec vector length: {len(w2v_vector)}\")\n",
    "print(f\"word2vec vector (np) length: {w2v_vector_np.size}\\n\")\n",
    "\n",
    "print(f\"word2vec dist matrix shape: {w2v_matrix.shape}\")\n",
    "print(f\"word2vec dist sym matrix shape: {w2v_matrix_sym.shape}\\n\")\n",
    "\n",
    "print(f\"evo vector (np) length: {evo_vector_np.size}\")\n",
    "print(f\"evo dist matrix shape: {evo_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorder matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reordered_s_matrix, reordered_t_matrix = reorder_matrix_2(w2v_vector_np, evo_vector, w2v_dist_matrix_sym, evo_matrix)\n",
    "\n",
    "print(f\"source matrix shape: {w2v_dist_matrix_sym.shape} vocab: {len(w2v_vector_np)}\")\n",
    "print(f\"target matrix shape: {evo_matrix.shape} vocab: {len(evo_vector)}\\n\")\n",
    "\n",
    "# reorder matrices\n",
    "#source_matrix, target_matrix = reorder_matrix_2(w2v_vector_np, evo_vector, w2v_dist_matrix, evo_matrix)\n",
    "print('reducing evo matrix....')\n",
    "s = time.time()\n",
    "#found_items, missing_items, reordered_vector, reordered_matrix\n",
    "found_items, missing_items, reduced_evo_vector, reduced_evo_matrix = reduce_matrix(w2v_vector_np, evo_vector_np, evo_matrix)\n",
    "e = time.time()\n",
    "print(f\"reduce evo matrix. time taken: {round(e-s,2)}s\\n\")\n",
    "\n",
    "print(f\"found items length: {len(found_items)}.\")\n",
    "print(f\"missing items length: {len(missing_items)}.\\n\")\n",
    "\n",
    "print(f\"reordered target vector length: {reduced_evo_vector.size}.\")\n",
    "print(f\"reordered target matrix shape: {reduced_evo_matrix.shape}.\")\n",
    "print(f\"source matrix shape: {w2v_dist_matrix_sym.shape}.\")\n",
    "\n",
    "# missing items are still in the source vector and matrix so need to remove them as well\n",
    "print('\\nreducing w2v matrix....')\n",
    "s = time.time()\n",
    "w2v_common_vector_np = np.array(found_items)\n",
    "found_items, missing_items, w2v_common_vector, w2v_common_matrix = reduce_matrix(w2v_common_vector_np, w2v_vector_np, w2v_dist_matrix_sym)\n",
    "e = time.time()\n",
    "print(f\"reduce w2v matrix. time taken: {round(e-s,2)}s\\n\")\n",
    "\n",
    "print(f\"found items length: {len(found_items)}.\")\n",
    "print(f\"missing items length: {len(missing_items)}.\\n\")\n",
    "\n",
    "print(f\"reordered w2v vector length: {w2v_common_vector.size}.\")\n",
    "print(f\"reordered w2v matrix shape: {w2v_common_matrix.shape}.\")\n",
    "\n",
    "\n",
    "# Ensure that the matrices are square and of the same size\n",
    "assert w2v_common_matrix.shape == reduced_evo_matrix.shape, \"Both matrices must have the same shape.\"\n",
    "assert w2v_common_matrix.shape[0] == w2v_common_matrix.shape[1], \"Word2Vec matrix must be square.\"\n",
    "assert reduced_evo_matrix.shape[0] == reduced_evo_matrix.shape[1], \"Evo matrix must be square.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mantel test - custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# run a mantel test\n",
    "#\n",
    "s = time.time()\n",
    "num_permutations = 50\n",
    "observed_corr, p_value = custom_mantel_test(w2v_common_matrix, reduced_evo_matrix, num_permutations )\n",
    "e = time.time()\n",
    "print(f\"Mantel test with {num_permutations} complete in {round(e-s,2)}s corr : {observed_corr} p_val : {p_value}.\")\n",
    "\n",
    "s = time.time()\n",
    "num_permutations = 100\n",
    "observed_corr, p_value = custom_mantel_test(w2v_common_matrix, reduced_evo_matrix, num_permutations )\n",
    "e = time.time()\n",
    "print(f\"Mantel test with {num_permutations} complete in {round(e-s,2)}s corr : {observed_corr} p_val : {p_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mantel test - scikitbio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_permutations = [25,50,100]\n",
    "\n",
    "for n in num_permutations:\n",
    "    print(f\"Starting Mantel test with {n} permutations\")\n",
    "    s= time.time()\n",
    "    corr_coeff, p_value, num = mantel(w2v_common_matrix, reduced_evo_matrix, permutations=n)\n",
    "    print (corr_coeff, p_value, num)\n",
    "    e = time.time()\n",
    "    print(f\"Mantel test with {n} permutations complete in {round(e-s,2)}s corr : {corr_coeff} p_val : {p_value} num: {num}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dist_matrix = DistanceMatrix(w2v_common_matrix, ids=w2v_common_vector)\n",
    "evo_dist_matrix = DistanceMatrix(reduced_evo_matrix, ids=reduced_evo_vector)\n",
    "\n",
    "num_permutations = [25,50,100]\n",
    "\n",
    "for n in num_permutations:\n",
    "    print(f\"Starting Mantel test with {n} permutations\")\n",
    "    s= time.time()\n",
    "    corr_coeff, p_value, num = mantel(w2v_dist_matrix, evo_dist_matrix, permutations=n)\n",
    "    e = time.time()\n",
    "    print(f\"Mantel test with {n} permutations complete in {round(e-s,2)}s corr : {corr_coeff} p_val : {p_value} num: {num}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COSINE DISTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "#\n",
    "# gets the vocab for a model - returns a python list and a numpy array\n",
    "#\n",
    "def get_model_vocab(model_dir, model_name):\n",
    "    pfam_ids = []\n",
    "\n",
    "    model = Word2Vec.load(model_dir+model_name+'.model')\n",
    "    vocab = model.wv.key_to_index\n",
    "    \n",
    "    print(f\"Model {model_name} has a vocab of {len(vocab)}\")\n",
    "    for word in vocab:\n",
    "        pfam_ids.append(word)\n",
    "    return pfam_ids\n",
    "\n",
    "# creates a distance matrix for a model\n",
    "# this assumes a certain naming convention for models and vocab files\n",
    "# For example\n",
    "#   If the model name is : w2v_20240811_v5_w5_mc3, the code will expect\n",
    "#   - a model called w2v_20240811_v5_w5_mc3.model\n",
    "#   - a vocab file  called w2v_20240811_v5_w5_mc3.txt\n",
    "#   And will output a distance matrix called:\n",
    "#   - w2v_20240811_v5_w5_mc3_dist.npy\n",
    "#   \n",
    "def create_cosine_distance_matrix(model_base_name, model_name):\n",
    "    print(f\"Creating distance matrix for model {model_base_name}\")\n",
    "    print(f\"Loading model {model_name}\")\n",
    "    # get pfam ids from the vocab file corresponding to the model\n",
    "    model       = Word2Vec.load(model_name)\n",
    "    \n",
    "    #print('model_vocab:', model.wv.key_to_index)\n",
    "    pfam_ids = []\n",
    "    vocab = model.wv.key_to_index\n",
    "    \n",
    "    print(f\"Model {model_base_name} has a vocab of {len(vocab)}\")\n",
    "    \n",
    "    word_vectors    = model.wv\n",
    "\n",
    "    print('word vectors shape:', word_vectors.vectors.shape)\n",
    "    #print(word_vectors[0].shape)\n",
    "    \n",
    "    '''\n",
    "    for word in vocab:\n",
    "        pfam_ids.append(word)\n",
    "    num_entries     = len(pfam_ids)\n",
    "    \n",
    "    error_count = 0\n",
    "    success_count = 0\n",
    "    s = time.time()\n",
    "    vectors = np.empty(num_entries)\n",
    "    \n",
    "    # loop through each entry and calulate its distance\n",
    "    for i in range(num_entries):\n",
    "        pfam_1 = pfam_ids[i]\n",
    "        try:\n",
    "            v1 = model.wv[pfam_1]\n",
    "            vectors[i] = v1\n",
    "            success_count +=1\n",
    "        except Exception as e: # a bit convoluted, but want to print out the missing pfam\n",
    "            print('Exception', e)\n",
    "            continue\n",
    "    '''\n",
    "    s = time.time()\n",
    "    print('calculating cosine distances')\n",
    "    cosine_distance_matrix = cosine_distances(word_vectors.vectors, word_vectors.vectors)\n",
    "    \n",
    "    print('calculating euclidean distances')\n",
    "    euclidean_distance_matrix = euclidean_distances(word_vectors.vectors, word_vectors.vectors)\n",
    "    \n",
    "    #normalized_dist_matrix = (distance_matrix - min_dist) / (max_dist - min_dist)\n",
    "    # save distance matrix and pfam ids\n",
    "    #output_name = model_name+\"_cosine_dist_normalised.npy\"\n",
    "    #with open(output_name, \"wb\") as f:\n",
    "    #    np.save(f, cosine_distance_matrix)\n",
    "    #    np.save(f, pfam_ids)\n",
    "\n",
    "    e = time.time()\n",
    "    print(f\"cosine distance matrix computed for model: {model_base_name}. num words: {num_entries}. time: {round(e-s,2)}s.\")\n",
    "    \n",
    "    return euclidean_distance_matrix, cosine_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model /Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/skip_mc1/w2v_20240831_sg1_mc1_w3_v5_mac.model.\n",
      "creating distance matrix\n",
      "Creating distance matrix for model w2v_20240831_sg1_mc1_w3_v5_mac.model\n",
      "Loading model /Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/skip_mc1/w2v_20240831_sg1_mc1_w3_v5_mac.model\n",
      "Model w2v_20240831_sg1_mc1_w3_v5_mac.model has a vocab of 15473\n",
      "word vectors shape: (15473, 5)\n",
      "calculating cosine distances\n",
      "calculating euclidean distances\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_entries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m s\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating distance matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m euclidean_distance_matrix, cosine_distance_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_cosine_distance_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_base_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m e \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m matrix_time_taken\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(e\u001b[38;5;241m-\u001b[39ms, \u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 80\u001b[0m, in \u001b[0;36mcreate_cosine_distance_matrix\u001b[0;34m(model_base_name, model_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#normalized_dist_matrix = (distance_matrix - min_dist) / (max_dist - min_dist)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# save distance matrix and pfam ids\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#output_name = model_name+\"_cosine_dist_normalised.npy\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#with open(output_name, \"wb\") as f:\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#    np.save(f, cosine_distance_matrix)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#    np.save(f, pfam_ids)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m e \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine distance matrix computed for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_base_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. num words: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnum_entries\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(e\u001b[38;5;241m-\u001b[39ms,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m euclidean_distance_matrix, cosine_distance_matrix\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_entries' is not defined"
     ]
    }
   ],
   "source": [
    "output_dir = '/Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/skip_mc1/'\n",
    "model_base_name = 'w2v_20240831_sg1_mc1_w3_v5_mac.model'\n",
    "model_name      = output_dir+model_base_name\n",
    "\n",
    "print(f\"loading model {model_name}.\")\n",
    "        \n",
    "#\n",
    "#--------------------------- create dist -----------------------\n",
    "#\n",
    "s=time.time()\n",
    "print(\"creating distance matrix\")\n",
    "euclidean_distance_matrix, cosine_distance_matrix = create_cosine_distance_matrix(model_base_name, model_name)\n",
    "e = time.time()\n",
    "matrix_time_taken=str(round(e-s, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# load model and vocab\n",
    "model   = Word2Vec.load(model_name)\n",
    "vocab   = model.wv.key_to_index\n",
    "\n",
    "# list all items in vocab\n",
    "\n",
    "for i, word_1 in enumerate(vocab):\n",
    "    for j, word_2 in enumerate(vocab):\n",
    "        \n",
    "        if(word_1 == 'GAP' or word_1 == 'DISORDER' or word_1 == 'START_GAP' or word_1 == 'STOP_GAP'):\n",
    "            continue\n",
    "        if(word_2 == 'GAP' or word_2 == 'DISORDER' or word_2 == 'START_GAP' or word_2 == 'STOP_GAP'):\n",
    "            continue\n",
    "        if(word_1 == word_2):\n",
    "            continue\n",
    "        \n",
    "        #print(i, word)\n",
    "        v1 = model.wv[word_1]\n",
    "        v2 = model.wv[word_2]\n",
    "\n",
    "        euc_dist = np.linalg.norm(v1 - v2)\n",
    "        d2       = euclidean_distance_matrix[i,j]\n",
    "        d3       = euclidean_distance_matrix[j,i]\n",
    "        \n",
    "        # work out cosine myself\n",
    "        cosine_similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        cos_dist = 1 - cosine_similarity\n",
    "        \n",
    "        # get from library\n",
    "        cos_dist_1 = cosine(v1, v2)\n",
    "        \n",
    "        # get from matrix\n",
    "        d4       = cosine_distance_matrix[i,j]\n",
    "        d5       = cosine_distance_matrix[j,i]\n",
    "        \n",
    "        print(f\"EUC: {word_1} to {word_2} : \\t| {euc_dist} \\t| {d2}  \\t| {d3}\")\n",
    "        print(f\"COS: {word_1} to {word_2} : \\t| {cos_dist_1} \\t| {d4}  \\t| {d5}\\n\")\n",
    "\n",
    "            \n",
    "        assert np.round(euc_dist, 4)     == np.round(d2, 4), \"Euclidean distance not the same - d2\"\n",
    "        assert np.round(euc_dist, 4)     == np.round(d3, 4), \"Euclidean distance not the same - d3\"\n",
    "        assert np.round(cos_dist_1, 2)   == np.round(d4, 2), f\"Cos distance not the same - {cos_dist_1} v {d4}.\"\n",
    "        assert np.round(cos_dist_1, 2)   == np.round(d5, 2), \"Cos distance not the same - d5.\"\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
