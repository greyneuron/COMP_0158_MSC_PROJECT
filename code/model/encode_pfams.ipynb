{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import itertools\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import duckdb\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_string = \"/Users/patrick/dev/ucl/comp0158_mscproject/database/w2v_20240731_test.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distances for pfam entries in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(model_dir, model_name, vocab_dir):\n",
    "    vocab_file = vocab_dir+model_name+'_vocab.txt'\n",
    "    \n",
    "    # get pfam ids from the models vocab\n",
    "    #print(f\"Encoding vocab file: {vocab_file}\")\n",
    "    model = Word2Vec.load(model_dir+model_name+'.model')\n",
    "    pfam_ids = []\n",
    "    with open(vocab_file, 'r') as vf:\n",
    "        for line in vf:\n",
    "            line = line.rstrip()\n",
    "            line =  line.lstrip()\n",
    "            if line.startswith('PF'):\n",
    "                pfam_ids.append(line)\n",
    "                #encoding = model.wv[line]\n",
    "                #print(f\"Encoding :{line}: {encoding}\")\n",
    "    #print(pfam_ids)\n",
    "    vf.close()\n",
    "    \n",
    "    # calculate matrix size and initialise\n",
    "    num_entries = len(pfam_ids)\n",
    "    distance_matrix = np.zeros((num_entries, num_entries))\n",
    "    \n",
    "    # create empty distance matrix\n",
    "    #print(f\"Calculating distances for {num_entries} pfam ids under model {model_name}\")\n",
    "    error_count = 0\n",
    "    success_count = 0\n",
    "    s = time.time()\n",
    "    for i in range(num_entries):\n",
    "        for j in range(i+1, num_entries):\n",
    "            pfam_1 = pfam_ids[i]\n",
    "            pfam_2 = pfam_ids[j]\n",
    "            try:\n",
    "                v1 = model.wv[pfam_1]\n",
    "                v2 = model.wv[pfam_2]\n",
    "                distance = np.linalg.norm(v1 - v2)\n",
    "                distance_matrix[i][j] = distance\n",
    "                success_count +=1\n",
    "            except Exception as e: # a bit convoluted, but want to print out the missing pfam\n",
    "                #print(f\"Exception calculating {pfam_1} to {pfam_2} : {e.args[0]}\")\n",
    "                missing = re.search(\"Key '(.*)' not\", e.args[0] )\n",
    "                print(missing.group(1))\n",
    "                #of.write(missing.group(1) + '\\n')\n",
    "                error_count +=1\n",
    "                continue\n",
    "    # close the output file\n",
    "    #of.close()\n",
    "    \n",
    "    output_name = vocab_dir+model_name+\"_dist\"\n",
    "    np.save(output_name, distance_matrix)\n",
    "    e = time.time()\n",
    "    print(f\"distance matrix computed for model: {model_name}. num words: {num_entries}. time: {round(e-s,2)}s. success: {success_count} fail: {error_count} output: {output_name}.npy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distances for one model\n",
    "vocab_dir=\"/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/\"\n",
    "\n",
    "# get vocab for a particular file\n",
    "model_name = \"w2v_20240811_v5_w5_mc3\"\n",
    "#calculate_distances(model_dir, model_name, vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/kp/bqnb4b7n4ng50xtbznpbx7xh0000gn/T/ipykernel_15954/63249940.py:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  model_name_s  = re.search(\"(w2v_.*)\\.model\", file_path)\n"
     ]
    }
   ],
   "source": [
    "# get distances for all models\n",
    "vocab_dir=\"/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/\"\n",
    "\n",
    "file_list = glob.glob(os.path.join(model_dir, '*.model'))\n",
    "for file_path in file_list:\n",
    "    model_name_s  = re.search(\"(w2v_.*)\\.model\", file_path)\n",
    "    model_name         = model_name_s.group(1)\n",
    "    #calculate_distances(model_dir, model_name, vocab_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get vocab for a model for pfam words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vocab(vocab_dir, model_name):\n",
    "    # extracts pfam ids from the vecotr\n",
    "\n",
    "    pfam_ids = []\n",
    "    vocab_file = vocab_dir+model_name+'_vocab.txt'\n",
    "    \n",
    "    with open(vocab_file, 'r') as vf:\n",
    "        for line in vf:\n",
    "            line = line.rstrip()\n",
    "            line =  line.lstrip()\n",
    "            if line.startswith('PF'):\n",
    "                pfam_ids.append(line)\n",
    "    vf.close()\n",
    "    \n",
    "    np_pfam_ids = np.array(pfam_ids)\n",
    "    \n",
    "    return pfam_ids, np_pfam_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE ROWS/COLS FROM TARGET IF NOT IN SOUCRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model w2v_20240811_v5_w5_mc5 vocab size: 12802.\n",
      "Model w2v_20240811_v5_w5_mc3 vocab size: 13529.\n",
      "Shared model words: 12802.\n",
      "Loading target dist matrix /Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/vocab/w2v_20240811_v5_w5_mc3_dist.npy\n",
      "Target dist matrix reduced from (13529, 13529) to (12802, 12802)\n"
     ]
    }
   ],
   "source": [
    "vocab_dir=\"/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/word2vec/COMP_0158_MSC_PROJECT/data/models/\"\n",
    "\n",
    "# models to compare - ulitmately we want a new target model with only entries that are in the soure as well\n",
    "source_model_name = \"w2v_20240811_v5_w5_mc5\"\n",
    "target_model_name = \"w2v_20240811_v5_w5_mc3\"\n",
    "\n",
    "# get te vocab list for both models we wish to compare\n",
    "source_vocab, np_source_vocab = get_model_vocab(vocab_dir, source_model_name)\n",
    "target_vocab, np_target_vocab = get_model_vocab(vocab_dir, target_model_name)\n",
    "\n",
    "print(f\"Model {source_model_name} vocab size: {len(source_vocab)}.\")\n",
    "print(f\"Model {target_model_name} vocab size: {len(target_vocab)}.\")\n",
    "\n",
    "# create a True/False mask with True where the item from the source array is in the target array\n",
    "mask = np.isin(np_target_vocab, np_source_vocab)\n",
    "print(f\"Shared model words: {mask.sum()}.\")\n",
    "\n",
    "# load the target matrix\n",
    "distance_matrix_name = vocab_dir+target_model_name+'_dist.npy'\n",
    "print(f\"Loading target dist matrix {distance_matrix_name}\")\n",
    "dist_matrix = np.load(distance_matrix_name)\n",
    "\n",
    "\n",
    "# remove non-common rows/columns from the target matrix\n",
    "\n",
    "mask                = np.isin(np_target_vocab, np_source_vocab)\n",
    "dist_matrix_subset  = dist_matrix[np.ix_(mask, mask)]\n",
    "\n",
    "print(f\"Target dist matrix reduced from {dist_matrix.shape} to {dist_matrix_subset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False False False  True False]\n",
      "[[11 12 16]\n",
      " [21 22 26]\n",
      " [61 62 66]]\n"
     ]
    }
   ],
   "source": [
    "# sample code to test the use of masks\n",
    "# compare two vectors and find the indices in the second for common entries\n",
    "source_v = np.array(['a', 'b', 'f'])\n",
    "target_v = np.array(['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "\n",
    "mask = np.isin(target_v, source_v)\n",
    "print(mask)\n",
    "\n",
    "\n",
    "# apply this mask to a target matrix - the mask should be applied to ros and columns\n",
    "target_matrix = np.array([  [11, 12, 13, 14, 15, 16], \\\n",
    "                            [21, 22, 23, 24, 25, 26], \\\n",
    "                            [31, 32, 33, 34, 35, 36], \\\n",
    "                            [41, 42, 43, 44, 45, 46], \\\n",
    "                            [51, 52, 53, 54, 55, 56], \\\n",
    "                            [61, 62, 63, 64, 55, 66]])\n",
    "\n",
    "# new matrix has the dimensions corresponding to the number of shared items in the two original vectors\n",
    "# the entries in the matrix are the row/column entries that had 'True' in the mask\n",
    "target_subset = target_matrix[np.ix_(mask, mask)]\n",
    "print(target_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
