{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# parses a dicorpus file to build up sentences to create a model\n",
    "#\n",
    "def get_corpus_sentences(corpus_file):\n",
    "    # initialise\n",
    "    s = time.time()\n",
    "    sentences = []\n",
    "    counter = 0\n",
    "    num_tokens = 0\n",
    "    print(f'Parsing file for sentences: {corpus_file}')\n",
    "    with open(corpus_file, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n')\n",
    "            tokens = line.split()\n",
    "            sentences.append(tokens)\n",
    "            counter +=1\n",
    "            num_tokens += len(tokens)\n",
    "    # time check\n",
    "    e = time.time()\n",
    "    print(f\"{counter} sentences processed, {num_tokens} added in {e - s}s\" )\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create W2V model\n",
    "#\n",
    "def create_w2v(sentences, model_name, vector_size, window_size, mc):\n",
    "    current_date    = datetime.now().strftime('%Y%m%d')\n",
    "    model_name      = model_name + \".model\"\n",
    "    \n",
    "    s = time.time()\n",
    "    \n",
    "    # create model from sentences       \n",
    "    w2v = Word2Vec(sentences, vector_size=vector_size, window=window_size, workers=4, epochs=10, min_count=mc)\n",
    "    \n",
    "    # time check\n",
    "    e = time.time()\n",
    "    print(f\"{model_name} | {vector_size} | {window_size} | {mc} | {round(e-s,2)}s\")\n",
    "          \n",
    "    # save model      \n",
    "    w2v.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##corpus_file     = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/uniref100_e_corpus_20240810.txt\"\n",
    "#sentences       = get_corpus_sentences(corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file for sentences: /Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/uniref100_e_corpus_20240810.txt\n",
      "45909435 sentences processed, 255674676 added in 558.3428671360016s\n",
      "/Users/patrick/dev/ucl/comp0158_mscproject/data/models/w2v_20240814_v5_w5_mc1.model | 5 | 5 | 1 | 1474.09s\n",
      "/Users/patrick/dev/ucl/comp0158_mscproject/data/models/w2v_20240814_v10_w5_mc1.model | 10 | 5 | 1 | 1362.54s\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "model_dir       = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/\"\n",
    "current_date    = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "# create models\n",
    "'''\n",
    "for vector_size in range(5, 10, 5):\n",
    "    for window_size in range(5,10, 5):\n",
    "        # create model\n",
    "        model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)\n",
    "        create_w2v(sentences, model_name, vector_size, window )\n",
    "'''\n",
    "\n",
    "'''\n",
    "corpus_file     = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/uniref100_e_corpus_20240810.txt\"\n",
    "sentences       = get_corpus_sentences(corpus_file)\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 5\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 5\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 5\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 5\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "\n",
    "vector_size = 20\n",
    "window_size = 5\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "\n",
    "vector_size = 20\n",
    "window_size = 5\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 10\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 10\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 20\n",
    "window_size = 10\n",
    "min_count   = 3\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 10\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 10\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 20\n",
    "window_size = 10\n",
    "min_count   = 5\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "'''\n",
    "\n",
    "corpus_file     = \"/Users/patrick/dev/ucl/comp0158_mscproject/data/corpus/uniref100_e_corpus_20240810.txt\"\n",
    "sentences       = get_corpus_sentences(corpus_file)\n",
    "\n",
    "vector_size = 5\n",
    "window_size = 5\n",
    "min_count   = 1\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 5\n",
    "min_count   = 1\n",
    "model_name = model_dir+\"w2v_\"+current_date + \"_v\"+str(vector_size)+\"_w\"+str(window_size)+\"_mc\"+str(min_count)\n",
    "create_w2v(sentences, model_name, vector_size, window_size, min_count )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vocab for w2v_20240811_v5_w10_mc5\n",
      "Getting vocab for w2v_20240811_v5_w10_mc3\n",
      "Getting vocab for w2v_20240811_v10_w5_mc3\n",
      "Getting vocab for w2v_20240811_v10_w10_mc5\n",
      "Getting vocab for w2v_20240811_v5_w5_mc5\n",
      "Getting vocab for w2v_20240811_v20_w5_mc5\n",
      "Getting vocab for w2v_20240811_v5_w5_mc3\n",
      "Getting vocab for w2v_20240811_v10_w10_mc3\n",
      "Getting vocab for w2v_20240811_v10_w5_mc5\n",
      "Getting vocab for w2v_20240811_v20_w5_mc3\n",
      "Getting vocab for w2v_20240811_v20_w10_mc5\n",
      "Getting vocab for w2v_20240811_v20_w10_mc3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/kp/bqnb4b7n4ng50xtbznpbx7xh0000gn/T/ipykernel_90630/1526553224.py:3: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  base_name_search  = re.search(\"(w2v_.*)\\.model\", model_name)\n",
      "/var/folders/kp/bqnb4b7n4ng50xtbznpbx7xh0000gn/T/ipykernel_90630/1526553224.py:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  model_name_s  = re.search(\"(w2v_.*)\\.model\", file_path)\n"
     ]
    }
   ],
   "source": [
    "def write_vocab(model_dir, model_name, vocab_dir):\n",
    "    \n",
    "    base_name_search  = re.search(\"(w2v_.*)\\.model\", model_name)\n",
    "    base_name         = base_name_search.group(1)\n",
    "\n",
    "    vocab_file  = vocab_dir+base_name+'_vocab.txt'\n",
    "    model       = Word2Vec.load(model_dir+model_name)\n",
    "    \n",
    "    print(f\"Extracting vocab for model {base_name} to {vocab_file}.\")\n",
    "\n",
    "    of = open(vocab_file, \"w\")\n",
    "    for word in model.wv.key_to_index:\n",
    "        of.write(word + '\\n')\n",
    "    of.close()\n",
    "\n",
    "# set directories\n",
    "vocab_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/\"\n",
    "\n",
    "# get vocab for a particular file\n",
    "#model_name = \"w2v_20240811_v5_w5_mc3.model\"\n",
    "#write_vocab(model_dir, model_name, vocab_dir)\n",
    "\n",
    "# find all model files\n",
    "file_list = glob.glob(os.path.join(model_dir, '*.model'))\n",
    "for file_path in file_list:\n",
    "    model_name_s  = re.search(\"(w2v_.*)\\.model\", file_path)\n",
    "    model_name         = model_name_s.group(1)\n",
    "    \n",
    "    print(f\"Getting vocab for {model_name}\")\n",
    "    #write_vocab(model_dir, model_name, vocab_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab for one model\n",
    "vocab_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/\"\n",
    "\n",
    "# get vocab for a particular file\n",
    "#model_name = \"w2v_20240811_v5_w5_mc3.model\"\n",
    "#write_vocab(model_dir, model_name, vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/kp/bqnb4b7n4ng50xtbznpbx7xh0000gn/T/ipykernel_90630/122697594.py:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  model_name_s  = re.search(\"(w2v_.*\\.model)\", file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting vocab for model w2v_20240811_v5_w10_mc5 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v5_w10_mc5_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v5_w10_mc3 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v5_w10_mc3_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v10_w5_mc3 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v10_w5_mc3_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v10_w10_mc5 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v10_w10_mc5_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v5_w5_mc5 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v5_w5_mc5_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v20_w5_mc5 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v20_w5_mc5_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v5_w5_mc3 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v5_w5_mc3_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v10_w10_mc3 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v10_w10_mc3_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v10_w5_mc5 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v10_w5_mc5_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v20_w5_mc3 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v20_w5_mc3_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v20_w10_mc5 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v20_w10_mc5_vocab.txt.\n",
      "Extracting vocab for model w2v_20240811_v20_w10_mc3 to /Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/w2v_20240811_v20_w10_mc3_vocab.txt.\n"
     ]
    }
   ],
   "source": [
    "# get vocab for all models\n",
    "vocab_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/vocab/\"\n",
    "model_dir=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/\"\n",
    "\n",
    "file_list = glob.glob(os.path.join(model_dir, '*.model'))\n",
    "for file_path in file_list:\n",
    "    model_name_s  = re.search(\"(w2v_.*\\.model)\", file_path)\n",
    "    model_name         = model_name_s.group(1)\n",
    "    write_vocab(model_dir, model_name, vocab_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.6016934   0.20834175  0.9652066   0.9793175  -0.38478503 -1.2593813\n",
      "  0.30134046  0.23477417  0.829169    0.50237304]\n"
     ]
    }
   ],
   "source": [
    "# check if a pfam entry is in a model\n",
    "#model = Word2Vec.load(\"/Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/w2v_20240831_sg1_mc1_w8_v100.model\")\n",
    "model = Word2Vec.load(\"/Users/patrick/dev/ucl/word2vec/comp_0158_msc_project/data/models/hpc/w2v_20240901_sg1_mc3_w21_v10.model\")\n",
    "\n",
    "#pfam_id = 'PF19687'        # present in w2v_20240810_v5_w2.model\n",
    "#pfam_id = 'PF01257'        # not present in w2v_20240810_v5_w2.model - but shouldn't be\n",
    "#pfam_id = 'PF00424'        # not present in model - why not - only appears once in corpus\n",
    "#pfam_id = 'PF14033'        # present in w2v_20240810_v5_w2.model\n",
    "#pfam_id = 'PF00469'         # not present in model - why not - only appears once in corpus\n",
    "pfam_id = 'PF03945'\n",
    "\n",
    "try:\n",
    "    print(model.wv[pfam_id])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12806\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already trained your Word2Vec model\n",
    "model = Word2Vec.load(\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/w2v_20240810_v5_w2.model\")\n",
    "\n",
    "# Size of the vocabulary\n",
    "vocab_size = len(model.wv.key_to_index)\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"/Users/patrick/dev/ucl/comp0158_mscproject/data/models/w2v_20240810_v5_w2.model\")\n",
    "dictionary_file=\"/Users/patrick/dev/ucl/comp0158_mscproject/data/analysis/w2v_20240810_v5_w2_included_pfams.txt\"\n",
    "\n",
    "of = open(dictionary_file, \"w\")\n",
    "for word in model.wv.key_to_index:\n",
    "    of.write(word + '\\n')\n",
    "of.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_ucl_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
